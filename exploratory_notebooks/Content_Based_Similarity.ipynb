{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "979811e9",
   "metadata": {},
   "source": [
    "# Notebook Objective and Setup\n",
    "\n",
    "BGG05 is the building of a content-based item filter. Using category weights, I use my domain expertise to tune an item similarity matrix for all game IDs in the games file.\n",
    "\n",
    "This content-based filter could be used as-is to find similar games to a user's catalog and predict ratings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4e48291",
   "metadata": {},
   "source": [
    "## Notebook Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e76df09b",
   "metadata": {},
   "source": [
    "### Package Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "57e943a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import regex as re\n",
    "import time\n",
    "import os\n",
    "import gc\n",
    "import copy\n",
    "import json\n",
    "\n",
    "# ignore warnings (gets rid of Pandas copy warnings)\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "pd.options.display.max_columns = None\n",
    "\n",
    "pd.set_option(\"display.max_columns\", 30)\n",
    "pd.set_option(\"display.max_rows\", 30)\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "# # NLP tools\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "# import re\n",
    "# import nltk\n",
    "# import fasttext\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "# from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "# from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "# from tensorflow.python.keras.preprocessing import sequence, text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "275f08a9",
   "metadata": {},
   "source": [
    "### Notebook Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e95d7216",
   "metadata": {},
   "source": [
    "##### Processing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a78e8abb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    # remove numbers\n",
    "    text = re.sub(r\"\\d+\", \"\", text)\n",
    "    # remove punctuation except periods\n",
    "    text = re.sub(r\"[^\\w\\s\\.]\", \"\", text)\n",
    "    text = text.lower()\n",
    "    text = text.strip()\n",
    "    return text\n",
    "\n",
    "def filter_stopwords(text):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    word_tokens = word_tokenize(text)\n",
    "    filtered_sentence = [w for w in word_tokens if not w.lower() in stop_words]\n",
    "    return \" \".join(filtered_sentence)\n",
    "\n",
    "def evaluate_quality_words_over_thresh(text):\n",
    "    word_tokens = word_tokenize(text)\n",
    "    return len(word_tokens) > 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "64427375",
   "metadata": {},
   "outputs": [],
   "source": [
    "def processing_pipeline_games(weight_groups, df):\n",
    "    \"\"\"\n",
    "    !!!Hard-coded processor!!!\n",
    "    Takes in weight tuples and a dataframe\n",
    "    Scales specific dataframe columns to tuples\n",
    "\n",
    "    Inputs:\n",
    "    weight_groups: list of weight tuples (x, y)\n",
    "    df: df to be scaled\n",
    "\n",
    "    Returns:\n",
    "    Processed Dataframe\"\"\"\n",
    "\n",
    "    # Whole pipeline with continuous then categorical transformers\n",
    "    total_pipeline = ColumnTransformer(\n",
    "        [\n",
    "            (\n",
    "                \"games_weight_weight\",\n",
    "                MinMaxScaler(feature_range=weight_groups[0]),\n",
    "                [\"GameWeight\"],\n",
    "            ),\n",
    "            (\"avgrating\", MinMaxScaler(feature_range=weight_groups[1]), [\"AvgRating\"]),\n",
    "            (\n",
    "                \"bayes_weight\",\n",
    "                MinMaxScaler(feature_range=weight_groups[2]),\n",
    "                [\"BayesAvgRating\"],\n",
    "            ),\n",
    "            (\n",
    "                \"players_weight\",\n",
    "                MinMaxScaler(feature_range=weight_groups[3]),\n",
    "                [\"BestPlayers\"],\n",
    "            ),\n",
    "            (\n",
    "                \"playtime_weight\",\n",
    "                MinMaxScaler(feature_range=weight_groups[4]),\n",
    "                [\"Playtime\"],\n",
    "            ),\n",
    "            (\n",
    "                \"remainder_weight\",\n",
    "                MinMaxScaler(feature_range=weight_groups[5]),\n",
    "                [\"Cat:Thematic\",\n",
    "    \"Cat:Strategy\",\n",
    "    \"Cat:Family\",\n",
    "    \"Cat:War\",\n",
    "    \"Cat:CGS\",\n",
    "    \"Cat:Abstract\",\n",
    "    \"Cat:Party\",\n",
    "    \"Cat:Childrens\",],\n",
    "            ),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Fit and tranform the pipeline on x_train, then transform x_test\n",
    "    processed = total_pipeline.fit_transform(df)\n",
    "\n",
    "    return processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5d0e6662",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_dataset(dataset, weights, tfidf=False):\n",
    "    \"\"\"\n",
    "    Set each item either according to Term Frequency or TF-IDF\n",
    "    Then scale dataset according to provided weights\n",
    "\n",
    "    tfidf flag:\n",
    "    Use TRUE when RARE entries are more important than FREQUENT entries\n",
    "    Use FALSE when COMMON entries are more important than RARE entries\n",
    "\n",
    "    Inputs:\n",
    "    dataset: dataset to scale\n",
    "    weights: tuple (x, y) to weight dataset\n",
    "    tfidf: set flag to True for TF-IDF\n",
    "\n",
    "    Outputs:\n",
    "    Scaled data\n",
    "    \"\"\"\n",
    "\n",
    "    # get list of titles to reapply to DF after transformation\n",
    "    titles = list(dataset.columns)\n",
    "    indices = list(dataset.index)\n",
    "\n",
    "    # set up weighted scaler\n",
    "    scaler = MinMaxScaler(feature_range=weights)\n",
    "\n",
    "    # get total number of entries\n",
    "    total_entries = sum(dataset.sum())\n",
    "\n",
    "    if tfidf:\n",
    "\n",
    "        # instantiate tfidf transformer\n",
    "        tfidf = TfidfTransformer()\n",
    "\n",
    "        # convert matrix to tfidf\n",
    "        tfidf_dataset = pd.DataFrame(\n",
    "            tfidf.fit_transform(dataset).toarray(), columns=titles\n",
    "        )\n",
    "\n",
    "        # run scaler on transpose (scale by row not column)\n",
    "        transpose_scaled = scaler.fit_transform(tfidf_dataset.T)\n",
    "\n",
    "    else:\n",
    "        # for each column,\n",
    "        for item in list(dataset.columns):\n",
    "            # set item as its term frequency\n",
    "            dataset.loc[dataset[item] > 0, item] = (\n",
    "                dataset[item].sum() / total_entries\n",
    "            )\n",
    "\n",
    "        # run scaler on transpose (scale by row not column)\n",
    "        transpose_scaled = scaler.fit_transform(dataset.T)\n",
    "\n",
    "    # rebuild dataframe\n",
    "    scaled_dataset = pd.DataFrame(transpose_scaled.T, columns=titles, index=indices)\n",
    "\n",
    "    return scaled_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ee8e79f",
   "metadata": {},
   "source": [
    "# Content Based Filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbfc4e58",
   "metadata": {},
   "source": [
    "## Set Weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81252846",
   "metadata": {},
   "source": [
    "These are the scales for each of these categories. All entries in the category will be scaled to this tuple range by the MinMaxScaler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ab1cf563",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### DO NOT TOUCH THESE ARE THE PRODUCTION WEIGHTS!!!!!\n",
    "\n",
    "games_weight_weight = (-1.5, 1.5)  # game weight. Is a range, so (-, )\n",
    "rating_weight = (-0.3, 0.3)\n",
    "bayes_weight = (-0.5, 0.5)  # game weighted rating. Is a range, so (-, )\n",
    "players_weight = (0, 1)  # best players. Is a problematic range due to outliers\n",
    "playtime_weight = (0, 2)  # playtime. Is a range so (-, ). Has high outliers\n",
    "\n",
    "designers_weight = (0, 0.25)  # designer, binary\n",
    "mechanics_weight = (0, 0.75)  # mechanics, binary\n",
    "subcategories_weight = (0, 0.75)  # other mechanics like card game, print&play. binary\n",
    "family_weights = (0, 1)  # game families like pandemic, century. binary\n",
    "categories_weight = (0, 1)  # the five large overarching categories, binary\n",
    "themes_weight = (0, 0.5)  # themes like space, western. binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1ea3382c",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = {\"games_weight\": games_weight_weight,\n",
    "           \"rating\": rating_weight,\n",
    "           \"bayes\": bayes_weight,\n",
    "           \"players\": players_weight,\n",
    "           \"playtime\": playtime_weight,\n",
    "           \"designers\": designers_weight,\n",
    "           \"mechanics\": mechanics_weight,\n",
    "           \"subcategories\": subcategories_weight,\n",
    "           \"family\": family_weights,\n",
    "           \"categories\": categories_weight,\n",
    "           \"themes\": themes_weight}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c122b8b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"games_weight_weight = (-1, 1) # game weight. Is a range, so (-, )\n",
    "rating_weight = (-.5, .5)\n",
    "bayes_weight = (-.001, .001) # game weighted rating. Is a range, so (-, )\n",
    "players_weight = (0, 1) # best players. Is a problematic range due to outliers\n",
    "playtime_weight = (0, 2) # playtime. Is a range so (-, ). Has high outliers\n",
    "families_weight = (0, 0.5) # families, binary\n",
    "mechanics_weight = (0, .75) # mechanics, binary\n",
    "subcategories_weight = (0, .75) # other mechanics like card game, print&play. binary\n",
    "family_weights = (0, 0.5) # game families like pandemic, century. binary\n",
    "categories_weight = (0, 1) # the five large overarching categories, binary\n",
    "themes_weight = (-.15, 0.15) # themes like space, western. binary\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c29560c",
   "metadata": {},
   "source": [
    "## Load and Prep Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57837896",
   "metadata": {},
   "outputs": [],
   "source": [
    "how_many_games = 1000\n",
    "\n",
    "game_data_dir = \"../data/prod/games/game_dfs_clean\"\n",
    "\n",
    "# Load games\n",
    "games = pd.read_pickle(f\"{game_data_dir}/games_clean.pkl\")\n",
    "\n",
    "# I don't want to deal with every game ever to be honest, so let's reduce.\n",
    "# Let's just take the top 5000 games by average rating\n",
    "games = games.sort_values(\"BayesAvgRating\", ascending=False).head(how_many_games).reset_index(drop=True)\n",
    "\n",
    "games = games.sort_values(\"BGGId\").reset_index(drop=True)\n",
    "\n",
    "bgg_ids = games[\"BGGId\"].tolist()\n",
    "bgg_names = games[\"Name\"].tolist()\n",
    "game_lookup = {value.lower():key for key, value in zip(bgg_ids, bgg_names)}\n",
    "\n",
    "games['AvgRating'] = games['AvgRating'].round(2)\n",
    "games['BayesAvgRating'] = games['BayesAvgRating'].round(2)\n",
    "games['GameWeight'] = games['GameWeight'].round(2)\n",
    "\n",
    "# determine playtime for each game according to community\n",
    "games[\"Playtime\"] = 0\n",
    "games[\"Playtime\"] = games.apply(\n",
    "    lambda x: np.mean(x[\"ComMinPlaytime\"] + x[\"ComMaxPlaytime\"]), axis=1\n",
    ")\n",
    "\n",
    "# set upper cap on playtime\n",
    "over_6_hours = list(games.loc[games[\"Playtime\"] > 480].index)\n",
    "games.loc[over_6_hours, \"Playtime\"] = 480\n",
    "games.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dc912fa",
   "metadata": {},
   "source": [
    "### Weight Scale Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1333a6e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def refine_scaled_subset(filename, column, weight_type, thresh):\n",
    "    df = pd.read_pickle(f\"{game_data_dir}/{filename}.pkl\")\n",
    "    df = pd.get_dummies(df, columns=[column], prefix=\"\", prefix_sep=\"\").groupby(\"BGGId\").sum().reset_index()\n",
    "    \n",
    "    # get floor of mechanics presence in catalog (.03% of games)\n",
    "    df_floor = round(df.shape[0] * thresh)\n",
    "    # make a list of mechanics more than the floor\n",
    "    sums = pd.DataFrame(df.sum() >= df_floor)\n",
    "\n",
    "    # get indices for the mechanics keeping\n",
    "    keep_df = sums.loc[sums[0] == True].index\n",
    "\n",
    "    df = df[keep_df]\n",
    "\n",
    "    df = df[df['BGGId'].isin(bgg_ids)].set_index(\"BGGId\")\n",
    "\n",
    "    # scaled mechanics\n",
    "    scaled_df = scale_dataset(df, weights[weight_type], tfidf=True)\n",
    "\n",
    "    return scaled_df.reset_index(names=\"BGGId\").drop(columns=[\"BGGId\"])\n",
    "\n",
    "def refine_binary_subset(filename, column, weight_type, thresh):\n",
    "    df = pd.read_pickle(f\"{game_data_dir}/{filename}.pkl\")\n",
    "    df = pd.get_dummies(df, columns=[column], prefix=\"\", prefix_sep=\"\").groupby(\"BGGId\").sum().reset_index()\n",
    "\n",
    "    # get floor of mechanics presence in catalog (.03% of games)\n",
    "    df_floor = round(df.shape[0] * thresh)\n",
    "    # make a list of mechanics more than the floor\n",
    "    sums = pd.DataFrame(df.sum() >= df_floor)\n",
    "\n",
    "    # get indices for the mechanics keeping\n",
    "    keep_df = sums.loc[sums[0] == True].index\n",
    "\n",
    "    df = df[keep_df]\n",
    "\n",
    "    df = df[df['BGGId'].isin(bgg_ids)].set_index(\"BGGId\")\n",
    "\n",
    "    df = df.replace(1, weights[weight_type][1])\n",
    "\n",
    "    return df.reset_index(names=\"BGGId\").drop(columns=[\"BGGId\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4e52593",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_mechanics = refine_binary_subset(filename=\"mechanics_clean\",\n",
    "                                        column=\"mechanic\",\n",
    "                                        weight_type=\"mechanics\",\n",
    "                                        thresh=0.003).astype('int8')\n",
    "scaled_mechanics.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78105108",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_themes = refine_binary_subset(filename=\"themes_clean\",\n",
    "                                        column=\"Theme\",\n",
    "                                        weight_type=\"themes\",\n",
    "                                        thresh=0.003).astype('int8')\n",
    "scaled_themes.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3ff57f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_subcategories = refine_binary_subset(filename=\"subcategories_clean\",\n",
    "                                        column=\"boardgamecategory\",\n",
    "                                        weight_type=\"subcategories\",\n",
    "                                        thresh=0.003).astype('int8')\n",
    "scaled_subcategories.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2689820",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_designers = refine_binary_subset(filename=\"designers_clean\",\n",
    "                                        column=\"boardgamedesigner\",\n",
    "                                        weight_type=\"designers\",\n",
    "                                        thresh=0.003).astype('int8')\n",
    "scaled_designers.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "763e66cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scaled game families\n",
    "families = pd.get_dummies(games[\"Family\"]).astype(int)\n",
    "\n",
    "# get floor of mechanics presence in catalog (.01% of games)\n",
    "families_floor = round(families.shape[0] * 0.001)\n",
    "\n",
    "# make a list of themes more than the floor\n",
    "sums = pd.DataFrame(families.sum() >= families_floor)\n",
    "\n",
    "# get indices for the mechanics keeping\n",
    "keep_families = sums.loc[sums[0] == True].index\n",
    "\n",
    "families = families[keep_families]\n",
    "\n",
    "scaled_families = families.replace(1, weights[\"family\"][1]).astype('int8')\n",
    "\n",
    "scaled_families.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8984f9f7",
   "metadata": {},
   "source": [
    "### Master CBF Frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d60ca69a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# # instantiate MissForest imputer and fill all nans in scaled_games\n",
    "# imputer = KNNImputer(n_neighbors=5)\n",
    "# scaled_games = pd.DataFrame(\n",
    "#     imputer.fit_transform(scaled_games), columns=games_included_columns\n",
    "# )\n",
    "\n",
    "# include these columns for comparison\n",
    "games_included_columns = [\n",
    "    \"GameWeight\",\n",
    "    \"AvgRating\",\n",
    "    \"BayesAvgRating\",\n",
    "    \"BestPlayers\",\n",
    "    \"Playtime\",\n",
    "    \"Cat:Thematic\",\n",
    "    \"Cat:Strategy\",\n",
    "    \"Cat:Family\",\n",
    "    \"Cat:War\",\n",
    "    \"Cat:CGS\",\n",
    "    \"Cat:Abstract\",\n",
    "    \"Cat:Party\",\n",
    "    \"Cat:Childrens\",\n",
    "]\n",
    "\n",
    "games_reduced = games[games_included_columns]\n",
    "\n",
    "games_reduced.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70adaad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up weight groups for hard coded pipeline\n",
    "weight_groups = [\n",
    "    games_weight_weight,\n",
    "    rating_weight,\n",
    "    bayes_weight,\n",
    "    players_weight,\n",
    "    playtime_weight,\n",
    "    categories_weight,\n",
    "]\n",
    "\n",
    "# process scaled_games with pipeline\n",
    "scaled_games = pd.DataFrame(\n",
    "    processing_pipeline_games(weight_groups, games_reduced), columns=games_included_columns\n",
    ")\n",
    "\n",
    "scaled_games.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be35687a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# put together master dataframe with other already processed dataframes\n",
    "master_games = pd.concat(\n",
    "    (\n",
    "        scaled_games,\n",
    "        scaled_themes,\n",
    "        scaled_mechanics,\n",
    "        scaled_families,\n",
    "        scaled_designers,\n",
    "        scaled_subcategories\n",
    "    ),\n",
    "    axis=1,\n",
    ")  # , description_vectors, word_vectors\n",
    "\n",
    "# put game id on master_games DF\n",
    "master_games[\"BGGId\"] = bgg_ids\n",
    "\n",
    "# set index to id\n",
    "master_games = master_games.set_index(\"BGGId\")\n",
    "\n",
    "# fill nans with 0\n",
    "master_games = master_games.fillna(0)\n",
    "\n",
    "master_games.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2067432c",
   "metadata": {},
   "outputs": [],
   "source": [
    "gh_keys = list({x:y for x, y in game_lookup.items() if \"haven\" in x}.values())\n",
    "gh_keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ea38345",
   "metadata": {},
   "outputs": [],
   "source": [
    "games[games['BGGId'].isin(gh_keys)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6830b8e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "master_games[master_games.index.isin(gh_keys)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4da929b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "master_games.to_pickle(\"master_games_scaled.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ba19e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean up\n",
    "\n",
    "del scaled_mechanics\n",
    "del scaled_families\n",
    "del scaled_designers\n",
    "del scaled_games\n",
    "del scaled_subcategories\n",
    "del scaled_themes\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd859967",
   "metadata": {},
   "source": [
    "## Item Similarity via Cosine Distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1da5607",
   "metadata": {},
   "outputs": [],
   "source": [
    "{x:y for x, y in game_lookup.items() if \"haven\" in x}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "784c375c",
   "metadata": {},
   "outputs": [],
   "source": [
    "master_games = pd.read_pickle(\"master_games_scaled.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6f9b1ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get cosine similarities!\n",
    "cosine_sims = cosine_similarity(master_games)\n",
    "\n",
    "# do similarities by game id\n",
    "sims_byid = pd.DataFrame(cosine_sims, columns=bgg_ids)\n",
    "sims_byid[\"Game_Id\"] = bgg_ids\n",
    "sims_byid = sims_byid.set_index(\"Game_Id\", drop=True)\n",
    "\n",
    "sims_byid = sims_byid.round(2)\n",
    "\n",
    "sims_byid = sims_byid.replace(1.00, 0)\n",
    "sims_byid.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "26825089",
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in np.arange(0, len(bgg_names), 1):\n",
    "    bgg_names[item] = re.sub(\"[^A-Za-z0-9\\s]+\", \"\", bgg_names[item])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd666ec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "sims_byname = sims_byid.copy()\n",
    "\n",
    "lowercase_bgg_names = [x.lower() for x in bgg_names]\n",
    "sims_byname = sims_byname.set_axis(lowercase_bgg_names, axis=1).set_axis(lowercase_bgg_names, axis=0)\n",
    "sims_byname.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7a52ca28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save to pickles, we really only need the id one\n",
    "sims_byid.to_pickle(f\"{game_data_dir}/game_cosine_similarity_byid.pkl\")\n",
    "sims_byname.to_pickle(f\"{game_data_dir}/game_cosine_similarity_byname.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58e03992",
   "metadata": {},
   "outputs": [],
   "source": [
    "del sims_byname\n",
    "del sims_byid\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdda5478",
   "metadata": {},
   "source": [
    "### CHECK GAME HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4edae9cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save to pickles, we really only need the id one\n",
    "# sims_byid = pd.read_pickle(\"{game_data_dir}/game_cosine_similarity_byid.pkl\")\n",
    "sims_byname = pd.read_pickle(\n",
    "    f\"{game_data_dir}/game_cosine_similarity_byname.pkl\"\n",
    ")\n",
    "\n",
    "# make all the fields lowercase\n",
    "\n",
    "sims_byname.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9d1f2de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # find any entries with a particular string in the index or column name\n",
    "# def find_string_in_index_or_column(df, string):\n",
    "#     df =  df[df.index.str.contains(string, case=False, na=False)]\n",
    "#     columns=list(df.columns[df.columns.str.contains(\"gloomhaven\",case=False, na=False)])\n",
    "#     return df[columns]\n",
    "\n",
    "# sims_byname = find_string_in_index_or_column(sims_byname, \"gloomhaven\")\n",
    "# sims_byname.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c0bb401",
   "metadata": {},
   "source": [
    "This is why we made the name one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7669caea",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# test my specific game set here\n",
    "\n",
    "test_dict = {\n",
    "    \"Dominion\": list(sims_byname[\"dominion\"].sort_values(ascending=False)[:15].index),\n",
    "    \"D_Sim\": list(sims_byname[\"dominion\"].sort_values(ascending=False)[:15]),\n",
    "    \"Gloomhaven\": list(\n",
    "        sims_byname[\"gloomhaven\"].sort_values(ascending=False)[:15].index\n",
    "    ),\n",
    "    \"G_Sim\": list(sims_byname[\"gloomhaven\"].sort_values(ascending=False)[:15]),\n",
    "    \"Pandemic\": list(sims_byname[\"pandemic\"].sort_values(ascending=False)[:15].index),\n",
    "    \"Pa_Sim\": list(sims_byname[\"pandemic\"].sort_values(ascending=False)[:15]),\n",
    "    \"Splendor\": list(sims_byname[\"splendor\"].sort_values(ascending=False)[:15].index),\n",
    "    \"Sp_Sim\": list(sims_byname[\"splendor\"].sort_values(ascending=False)[:15]),\n",
    "    \"Viticulture Essential Edition\": list(\n",
    "        sims_byname[\"viticulture essential edition\"]\n",
    "        .sort_values(ascending=False)[:15]\n",
    "        .index\n",
    "    ),\n",
    "    \"V_Sim\": list(\n",
    "        sims_byname[\"viticulture essential edition\"].sort_values(ascending=False)[:15]\n",
    "    ),\n",
    "    \"Agricola\": list(sims_byname[\"agricola\"].sort_values(ascending=False)[:15].index),\n",
    "    \"Ag_Sim\": list(sims_byname[\"agricola\"].sort_values(ascending=False)[:15]),\n",
    "    \"Space Base\": list(\n",
    "        sims_byname[\"space base\"].sort_values(ascending=False)[:15].index\n",
    "    ),\n",
    "    \"Spa_Sim\": list(sims_byname[\"space base\"].sort_values(ascending=False)[:15]),\n",
    "    \"Terraforming Mars\": list(\n",
    "        sims_byname[\"terraforming mars\"].sort_values(ascending=False)[:15].index\n",
    "    ),\n",
    "    \"Te_Sim\": list(sims_byname[\"terraforming mars\"].sort_values(ascending=False)[:15]),\n",
    "    \"Chess\": list(sims_byname[\"chess\"].sort_values(ascending=False)[:15].index),\n",
    "    \"Ch_Sim\": list(sims_byname[\"chess\"].sort_values(ascending=False)[:15]),\n",
    "    # 'Sagrada':list(sims_byname['sagrada'].sort_values(ascending=False)[:15].index), \n",
    "    # 'Sa_Sim':list(sims_byname['sagrada'].sort_values(ascending=False)[:15]),\n",
    "    \"Azul\": list(sims_byname[\"azul\"].sort_values(ascending=False)[:15].index),\n",
    "    \"Az_Sim\": list(sims_byname[\"azul\"].sort_values(ascending=False)[:15]),\n",
    "    \"Codenames\": list(sims_byname[\"codenames\"].sort_values(ascending=False)[:15].index),\n",
    "    \"Co_Sim\": list(sims_byname[\"codenames\"].sort_values(ascending=False)[:15]),\n",
    "    \"Lords of Waterdeep\": list(\n",
    "        sims_byname[\"lords of waterdeep\"].sort_values(ascending=False)[:15].index\n",
    "    ),\n",
    "    \"L_Sim\": list(sims_byname[\"lords of waterdeep\"].sort_values(ascending=False)[:15]),\n",
    "    \"Century: Spice Road\": list(\n",
    "        sims_byname[\"century spice road\"].sort_values(ascending=False)[:15].index\n",
    "    ),\n",
    "    \"Ce_Sim\": list(sims_byname[\"century spice road\"].sort_values(ascending=False)[:15]),\n",
    "    \"Power Grid\": list(\n",
    "        sims_byname[\"power grid\"].sort_values(ascending=False)[:15].index\n",
    "    ),\n",
    "    \"P_Grid\": list(sims_byname[\"power grid\"].sort_values(ascending=False)[:15]),\n",
    "}\n",
    "\n",
    "pd.DataFrame(test_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ff219c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3551ad4f",
   "metadata": {},
   "source": [
    "# Appendix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a026443b",
   "metadata": {},
   "source": [
    "## Glove Word Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bb088ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# amount of vocabulary to use, will pick the top 10000 words seen in the corpus\n",
    "features = 5000\n",
    "\n",
    "# max text sequence length, must match tokens in transfer file, we are using glove 300d so it is 300\n",
    "max_words = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c496a814",
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate our word tokenizer\n",
    "tokenizer = Tokenizer(num_words=features)\n",
    "\n",
    "# Create vocabulary with training texts\n",
    "tokenizer.fit_on_texts(\n",
    "    games[\"Description\"]\n",
    ")  # nltk method which creates a vocab index based on the word frequency, every word gets own integer value\n",
    "\n",
    "# pad the train text to 300, or cut off if over\n",
    "tokenized_train = tokenizer.texts_to_sequences(\n",
    "    games[\"Description\"]\n",
    ")  # transforms each text to a sequence of integers\n",
    "tokenized_train = sequence.pad_sequences(\n",
    "    tokenized_train, maxlen=max_words, truncating=\"post\", padding=\"post\"\n",
    ")  # truncates or pads the vector to the max_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "367c5e24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# identify the embedding filename; we are using the Glove 42B 300d embeddings\n",
    "glove_file = \"glove.42B.300d.txt\"\n",
    "\n",
    "# create the embeddings index dictionary\n",
    "embeddings_index = {}  # create a lookup dictionary to store words and their vectors\n",
    "f = open(glove_file, errors=\"ignore\")  # open our embedding file\n",
    "for line in f:  # for each line in the file\n",
    "    values = line.split(\n",
    "        \" \"\n",
    "    )  # split the line on spaces between the word and its vectors\n",
    "    word = values[0]  # the word is the first entry\n",
    "    if (\n",
    "        word in tokenizer.word_index.keys()\n",
    "    ):  # we check if the word is in our tokenizer word index\n",
    "        coefs = np.asarray(values[1:], dtype=\"float32\")  # if so, get the word's vectors\n",
    "        embeddings_index[word] = (\n",
    "            coefs  # add the word and its vectors to the embeddings_index dictionary\n",
    "        )\n",
    "f.close()\n",
    "\n",
    "print(\n",
    "    \"Found %s word vectors.\" % len(embeddings_index)\n",
    ")  # report how many words in our corpus were found in the GloVe words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb5a091c",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_tokens = (\n",
    "    len(tokenizer.word_index) + 1\n",
    ")  # for num tokens we always do the length of our word index +1 for a pad token\n",
    "hits = 0\n",
    "misses = 0\n",
    "\n",
    "embedding_matrix = np.zeros(\n",
    "    (num_tokens, max_words)\n",
    ")  # setting up an array for our tokens with a row per token and 300 columns\n",
    "for (\n",
    "    word,\n",
    "    i,\n",
    ") in tokenizer.word_index.items():  # for each word in the tokenizer word index\n",
    "    embedding_vector = embeddings_index.get(\n",
    "        word\n",
    "    )  # get the vector from the embeddings index dictionary\n",
    "    if embedding_vector is not None:  # if the vector isn't None,\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = (\n",
    "            embedding_vector  # store the embedding vector in the matrix at that index\n",
    "        )\n",
    "        hits += 1\n",
    "    else:\n",
    "        misses += 1\n",
    "\n",
    "print(\"Converted %d words (%d misses)\" % (hits, misses))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d5aa2ec",
   "metadata": {},
   "source": [
    "### Single Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5c77cc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = games[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e33d6f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_storage = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a5b5650",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp.apply(lambda x: vectorize(x), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14112607",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(vector_storage).T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c21d61bf",
   "metadata": {},
   "source": [
    "### Real Word Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fe55dfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize(x):\n",
    "\n",
    "    description = x[\"Description\"]\n",
    "\n",
    "    tokens = nlp(description)\n",
    "\n",
    "    vector_storage[x[\"BGGId\"]] = {}\n",
    "\n",
    "    for token in tokens:\n",
    "\n",
    "        word = str(token)\n",
    "\n",
    "        try:\n",
    "            if np.all(embedding_matrix[tokenizer.word_index[word]]):\n",
    "                vector_storage[x[\"BGGId\"]][word] = np.mean(\n",
    "                    embedding_matrix[tokenizer.word_index[word]]\n",
    "                )\n",
    "            else:\n",
    "                continue\n",
    "        except:\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c1c3e06",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_storage = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f81370fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "descriptions = games[[\"BGGId\", \"Description\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60bdde95",
   "metadata": {},
   "outputs": [],
   "source": [
    "descriptions.apply(lambda x: vectorize(x), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe35f66f",
   "metadata": {},
   "outputs": [],
   "source": [
    "description_vectors = pd.DataFrame(vector_storage).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a16d6cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "description_vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb3383c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get only vectors that show up more than once!\n",
    "refined_vectors = description_vectors.loc[\n",
    "    :, (description_vectors.isnull().sum(axis=0) <= 21923)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0933469f",
   "metadata": {},
   "outputs": [],
   "source": [
    "refined_vectors.fillna(0, inplace=True)\n",
    "refined_vectors.reset_index(inplace=True)\n",
    "refined_vectors.drop(\"level_0\", axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b6f47e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "refined_vectors.to_pickle(\"{game_data_dir}/description_vectors.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fc6cc19",
   "metadata": {},
   "outputs": [],
   "source": [
    "del vector_storage\n",
    "del refined_vectors\n",
    "del embedding_matrix\n",
    "del tokenizer\n",
    "del descriptions\n",
    "del embedding_vector\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55b29d63",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "24bf4988",
   "metadata": {},
   "source": [
    "### TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21a19690",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load spaCy with English language processor\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e931b873",
   "metadata": {},
   "outputs": [],
   "source": [
    "descriptions = games[[\"Description\"]].astype(str)\n",
    "descriptions['Description'] = descriptions['Description'].apply(lambda x: clean_text(x))\n",
    "descriptions['Description'] = descriptions['Description'].apply(lambda x: filter_stopwords(x))\n",
    "descriptions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19a1c4b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare the vectorizer with the chosen parameters\n",
    "tfid_proc = TfidfVectorizer(\n",
    "    sublinear_tf=True,\n",
    "    use_idf=True,\n",
    "    max_df=0.8,\n",
    "    min_df=0.005,\n",
    "    ngram_range=(1, 3),\n",
    "    max_features=5000,\n",
    ")\n",
    "\n",
    "# fit the vectorizer to the descriptions\n",
    "word_vectors = tfid_proc.fit_transform(descriptions[\"Description\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2764543a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cast the vector array to a data frame with columns named by the features selected by the vectorizer\n",
    "word_vectors_df = pd.DataFrame(\n",
    "    word_vectors.toarray(), columns=tfid_proc.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "325354e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vectors_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "361f3124",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vectors_df.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "boardgamegeek-ZH0FNRKg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "292.865px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
