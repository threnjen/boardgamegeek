{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from config import CONFIGS\n",
    "import os\n",
    "import re\n",
    "import os\n",
    "import json\n",
    "from typing import Tuple\n",
    "\n",
    "# from utils.processing_functions import load_file_local_first, save_file_local_first\n",
    "# from nltk.corpus import stopwords\n",
    "# from nltk.tokenize import word_tokenize\n",
    "# from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "\n",
    "ENVIRONMENT = os.environ.get(\"ENVIRONMENT\", \"dev\")\n",
    "S3_SCRAPER_BUCKET = CONFIGS[\"s3_scraper_bucket\"]\n",
    "GAME_CONFIGS = CONFIGS[\"games\"]\n",
    "RATINGS_CONFIGS = CONFIGS[\"ratings\"]\n",
    "IS_LOCAL = True if os.environ.get(\"IS_LOCAL\", \"False\").lower() == \"true\" else False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_major_and_all_components(component_type:str, df:pd.DataFrame) -> Tuple[list, list]:\n",
    "\n",
    "    # Extract unique components from Positive_Components\n",
    "    unique_positive_components = set()\n",
    "    for components in df[f'{component_type}_Components']:\n",
    "        unique_positive_components.update(components)\n",
    "    major_components = [x for x in list(unique_positive_components) if x != ''] \n",
    "    # sort major components by number of words in the entry, highest to lowest\n",
    "    major_components = sorted(major_components, key=lambda x: len(x.split()), reverse=True)\n",
    "\n",
    "    # Extract unique elements from both Positive_Components and Positive_Sentences\n",
    "    unique_sentence_components = set()  # Start with Positive_Components\n",
    "    for sentences in df[f'{component_type}_Sentences']:\n",
    "        unique_sentence_components.update(sentences)\n",
    "    sentence_components = [x for x in list(unique_sentence_components) if x != ''] \n",
    "    sentence_components = sorted(sentence_components, key=lambda x: len(x.split()), reverse=True)\n",
    "\n",
    "    # Extract unique elements from both Positive_Components and Positive_Sentences\n",
    "    unique_all_components = set(unique_positive_components)  # Start with Positive_Components\n",
    "    for sentences in df[f'{component_type}_Sentences']:\n",
    "        unique_all_components.update(sentences)\n",
    "    all_components = [x for x in list(unique_all_components) if x != ''] \n",
    "    all_components = sorted(all_components, key=lambda x: len(x.split()), reverse=True)\n",
    "\n",
    "\n",
    "    return all_components, major_components, sentence_components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean up positives and negatives into vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle(\"top_1000_cleaned_rag.pkl\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_positives, major_positives, sentence_positives = get_major_and_all_components(\"Positive\", df)\n",
    "all_negatives, major_negatives, sentence_negatives = get_major_and_all_components(\"Negative\", df)\n",
    "\n",
    "len(all_positives), len(major_positives), len(sentence_positives), len(all_negatives), len(major_negatives), len(sentence_negatives)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_attributes_store = json.loads(open(\"positive_matches.json\").read())\n",
    "# negative_attributes_store = json.loads(open(\"negative_matches.json\").read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(keywords_to_iterate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_as_dict = df.to_dict(orient='records')\n",
    "\n",
    "for game_entry in df_as_dict[:1]:\n",
    "    print(f\"Preparing BGG ID: {game_entry['BGGId']}\")\n",
    "\n",
    "    tester_initial_positive_components = game_entry['Positive_Components']\n",
    "    tester_initial_positive_sentences = game_entry['Positive_Sentences']\n",
    "    \n",
    "    for positive_component in tester_initial_positive_components:\n",
    "        game_entry[f\"positive {positive_component}\"] = 1\n",
    "\n",
    "    for evaluation_item in all_positives[:1]:\n",
    "\n",
    "        if evaluation_item in tester_initial_positive_sentences:\n",
    "            print(f\"Skipping {evaluation_item}\")\n",
    "            continue\n",
    "        if evaluation_item in tester_initial_positive_components:\n",
    "            print(f\"Skipping {evaluation_item}\")\n",
    "            continue\n",
    "        else:\n",
    "            print(f\"\\nEval item or phrase: {evaluation_item}\")\n",
    "            current_major_key = [x for x in major_positives if evaluation_item.startswith(x)][0]\n",
    "            \n",
    "            # Create matches based on sentence similarities\n",
    "            this_item_sentence_matches = positive_attributes_store[evaluation_item]\n",
    "            sentence_matches_excluding_current_major_key = {x:y for x,y in this_item_sentence_matches.items() if not x.startswith(current_major_key)}\n",
    "            \n",
    "            keywords_to_iterate = list(sentence_matches_excluding_current_major_key.keys())\n",
    "\n",
    "            while(len(keywords_to_iterate)):\n",
    "                key_or_phrase = keywords_to_iterate.pop(0)\n",
    "                this_item_major_key = [x for x in major_positives if key_or_phrase.startswith(x)][0]\n",
    "                if f\"positive {this_item_major_key}\" not in game_entry:\n",
    "                    game_entry[f\"positive {this_item_major_key}\"] = 1 - sentence_matches_excluding_current_major_key[key_or_phrase]\n",
    "                keys_to_delete = [x for x in all_positives if x.startswith(this_item_major_key)]\n",
    "                keywords_to_iterate = [x for x in keywords_to_iterate if x not in keys_to_delete]\n",
    "\n",
    "            if evaluation_item not in major_positives:\n",
    "                continue\n",
    "\n",
    "            # Create matches based on keyword similarities\n",
    "            this_item_keyword_matches = positive_attributes_store[current_major_key]\n",
    "\n",
    "            for this_item_major_key in this_item_keyword_matches.keys():\n",
    "                if f\"positive {this_item_major_key}\" not in game_entry:\n",
    "                    game_entry[f\"positive {this_item_major_key}\"] = 1 - this_item_keyword_matches[this_item_major_key]\n",
    "                if f\"positive {this_item_major_key}\" in game_entry:\n",
    "                    game_entry[f\"positive {this_item_major_key}\"] = max(game_entry[f\"positive {this_item_major_key}\"], 1 - this_item_keyword_matches[this_item_major_key])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key_or_phrase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "this_item_major_key = [x for x in major_positives if key_or_phrase.startswith(x)]\n",
    "this_item_major_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[x for x in major_positives if \"collect\" in x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_as_dict[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Weaviate create attribute embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import weaviate\n",
    "import weaviate.classes as wvc\n",
    "from pydantic import BaseModel, ConfigDict\n",
    "from weaviate.classes.config import Configure\n",
    "from weaviate.classes.query import Filter\n",
    "from weaviate.classes.query import MetadataQuery\n",
    "from weaviate.util import generate_uuid5\n",
    "\n",
    "class WeaviateClient(BaseModel):\n",
    "    model_config = ConfigDict(arbitrary_types_allowed=True)\n",
    "    weaviate_client: weaviate.client = None\n",
    "    collection: weaviate.collections.Collection = None\n",
    "\n",
    "    def model_post_init(self, __context):\n",
    "        self.weaviate_client = self.connect_weaviate_client_docker()\n",
    "\n",
    "    def connect_weaviate_client_docker(self) -> weaviate.client:\n",
    "        if not IS_LOCAL:\n",
    "            client = weaviate.connect_to_local(\n",
    "                host=\"127.0.0.1\",\n",
    "                port=8081,\n",
    "                grpc_port=50051,\n",
    "                headers={\n",
    "                    \"X-OpenAI-Api-Key\": os.environ[\"OPENAI_API_KEY\"],\n",
    "                },\n",
    "            )\n",
    "            return client\n",
    "\n",
    "        return weaviate.connect_to_local(\n",
    "            port=8081,\n",
    "            headers={\n",
    "                \"X-OpenAI-Api-Key\": os.environ[\"OPENAI_API_KEY\"],\n",
    "            },\n",
    "        )\n",
    "    \n",
    "    def find_near_objects(self, collection_name, uuid, limit:int =20):\n",
    "        self.collection = self.weaviate_client.collections.get(collection_name)\n",
    "        response = self.collection.query.near_object(\n",
    "            near_object=uuid,\n",
    "            limit=limit,\n",
    "            return_metadata=MetadataQuery(distance=True),\n",
    "        )\n",
    "        return response.objects\n",
    "    \n",
    "    def close_client(self):\n",
    "        self.weaviate_client.close()\n",
    "    \n",
    "    def add_similarity_collection_item(self, item:pd.Series, collection_name: str = \"similarity_collection\") -> None:\n",
    "\n",
    "        self.collection = self.weaviate_client.collections.get(collection_name)\n",
    "\n",
    "        print(f\"Adding data for game {item[\"BGGId\"]}\")\n",
    "\n",
    "        game_object = {\n",
    "            \"bggid\": str(item[\"BGGId\"]),\n",
    "            \"name\": str(item[\"Name\"]).lower(),\n",
    "            # \"description\": str(item[\"Description\"]).lower(),\n",
    "            \"about\": str(item[\"About\"]).lower(),\n",
    "            \"positive\": str(item[\"Positive\"]).lower(),\n",
    "            \"negative\": str(item[\"Negative\"]).lower(),\n",
    "        }\n",
    "        \n",
    "        uuid = generate_uuid5(game_object)\n",
    "\n",
    "        if self.collection.data.exists(uuid):\n",
    "            return\n",
    "        else:\n",
    "            self.collection.data.insert(properties=game_object, uuid=uuid)\n",
    "\n",
    "        return uuid\n",
    "\n",
    "    def create_similarity_collection(self, collection_name: str = \"similarity_collection\") -> None:\n",
    "\n",
    "        if self.weaviate_client.collections.exists(collection_name):\n",
    "            print(\"Collection already exists for this block\")\n",
    "            self.weaviate_client.collections.delete(collection_name)\n",
    "            print(\"Deleted and recreating collection\")\n",
    "            return\n",
    "\n",
    "        self.weaviate_client.collections.create(\n",
    "            name=collection_name,\n",
    "            vectorizer_config=[\n",
    "                Configure.NamedVectors.text2vec_transformers(\n",
    "                    name=\"title_vector\",\n",
    "                    source_properties=[\"title\"],\n",
    "                )\n",
    "            ],\n",
    "            properties=[\n",
    "                wvc.config.Property(\n",
    "                    name=\"bggid\",\n",
    "                    data_type=wvc.config.DataType.TEXT,\n",
    "                    skip_vectorization=True,\n",
    "                    vectorize_property_name=False,\n",
    "                ),\n",
    "                wvc.config.Property(\n",
    "                    name=\"name\",\n",
    "                    data_type=wvc.config.DataType.TEXT,\n",
    "                    skip_vectorization=True,\n",
    "                    vectorize_property_name=False,\n",
    "                ),\n",
    "                # wvc.config.Property(\n",
    "                #     name=\"description\", data_type=wvc.config.DataType.TEXT\n",
    "                # ),\n",
    "                wvc.config.Property(name=\"about\", data_type=wvc.config.DataType.TEXT),\n",
    "                wvc.config.Property(\n",
    "                    name=\"positive\", data_type=wvc.config.DataType.TEXT\n",
    "                ),\n",
    "                wvc.config.Property(\n",
    "                    name=\"negative\", data_type=wvc.config.DataType.TEXT\n",
    "                ),\n",
    "            ],\n",
    "        )\n",
    "\n",
    "    def add_attributes_collection_item(self, attribute:str, collection_name: str = \"game_attributes\") -> None:\n",
    "\n",
    "        self.collection = self.weaviate_client.collections.get(collection_name)\n",
    "\n",
    "        print(f\"Adding data for attribute {attribute}\")\n",
    "\n",
    "        attribute_object = {\n",
    "            \"attribute_name\": attribute,\n",
    "            \"attribute\": attribute,\n",
    "        }\n",
    "        \n",
    "        uuid = generate_uuid5(attribute_object)\n",
    "\n",
    "        if self.collection.data.exists(uuid):\n",
    "            return uuid\n",
    "        else:\n",
    "            self.collection.data.insert(properties=attribute_object, uuid=uuid)\n",
    "\n",
    "        return uuid\n",
    "    \n",
    "    def create_attributes_collection(self, collection_name: str = \"game_attributes\") -> None:\n",
    "\n",
    "        if self.weaviate_client.collections.exists(collection_name):\n",
    "            print(\"Collection already exists for this block\")\n",
    "            self.weaviate_client.collections.delete(collection_name)\n",
    "            print(\"Deleted and recreating collection\")\n",
    "            return\n",
    "\n",
    "        self.weaviate_client.collections.create(\n",
    "            name=collection_name,\n",
    "            vectorizer_config=[\n",
    "                Configure.NamedVectors.text2vec_transformers(\n",
    "                    name=\"title_vector\",\n",
    "                    source_properties=[\"title\"],\n",
    "                )\n",
    "            ],\n",
    "            properties=[\n",
    "                wvc.config.Property(\n",
    "                    name=\"attribute_name\",\n",
    "                    data_type=wvc.config.DataType.TEXT,\n",
    "                    skip_vectorization=True,\n",
    "                    vectorize_property_name=False,\n",
    "                ),\n",
    "                wvc.config.Property(\n",
    "                    name=\"attribute\", data_type=wvc.config.DataType.TEXT, vectorize_property_name=False\n",
    "                ),\n",
    "            ],\n",
    "        )\n",
    "\n",
    "    def close_client(self):\n",
    "        self.weaviate_client.close()\n",
    "\n",
    "\n",
    "weaviate_client = WeaviateClient()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle(\"top_1000_cleaned_rag.pkl\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Positives Storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_positives, major_positives, sentence_positives = get_major_and_all_components(\"Positive\", df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weaviate_client.create_attributes_collection(collection_name=\"positives\")\n",
    "\n",
    "positive_attributes_store = {}\n",
    "\n",
    "for item in all_positives:\n",
    "    uuid = weaviate_client.add_attributes_collection_item(item, collection_name=\"positives\")\n",
    "    positive_attributes_store[item] = uuid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "match_storage = {}\n",
    "total_entries = len(all_positives)\n",
    "entries_completed = 0\n",
    "\n",
    "\n",
    "for major_component in all_positives:\n",
    "    uuid = positive_attributes_store[major_component]\n",
    "    pos_similars = weaviate_client.find_near_objects(collection_name=\"positives\", uuid=uuid, limit=2000)\n",
    "    matches_without_major_component = {x.properties['attribute_name']:x.metadata.distance for x in pos_similars if x.metadata.distance <= .40 and not x.properties['attribute_name'].startswith(major_component)}\n",
    "    match_storage[major_component] = matches_without_major_component\n",
    "    # report back every 100 entries completed\n",
    "    entries_completed += 1\n",
    "    if entries_completed % 100 == 0:\n",
    "        print(f\"Completed {entries_completed} of {total_entries}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('positive_matches.json', 'w') as f:\n",
    "    json.dump(match_storage, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Negatives Storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_negatives, major_negatives, sentence_negatives = get_major_and_all_components(\"Negative\", df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weaviate_client.create_attributes_collection(collection_name=\"negatives\")\n",
    "\n",
    "negative_attributes_store = {}\n",
    "\n",
    "for item in all_negatives:\n",
    "    uuid = weaviate_client.add_attributes_collection_item(item, collection_name=\"negatives\")\n",
    "    negative_attributes_store[item] = uuid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "match_storage = {}\n",
    "total_entries = len(all_negatives)\n",
    "entries_completed = 0\n",
    "\n",
    "for major_component in all_negatives:\n",
    "    uuid = negative_attributes_store[major_component]\n",
    "    pos_similars = weaviate_client.find_near_objects(collection_name=\"negatives\", uuid=uuid, limit=2000)\n",
    "    matches_without_major_component = {x.properties['attribute_name']:x.metadata.distance for x in pos_similars if x.metadata.distance <= .40 and not x.properties['attribute_name'].startswith(major_component)}\n",
    "    match_storage[major_component] = matches_without_major_component\n",
    "    entries_completed += 1\n",
    "    if entries_completed % 100 == 0:\n",
    "        print(f\"Completed {entries_completed} of {total_entries}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('negative_matches.json', 'w') as f:\n",
    "    json.dump(match_storage, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Content Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_1000 = load_file_local_first(\n",
    "            path=GAME_CONFIGS[\"clean_dfs_directory\"], file_name=\"top_1000_with_attached_rag.pkl\"\n",
    "        )\n",
    "top_1000 = top_1000[['BGGId','Name']]\n",
    "df = load_file_local_first(\n",
    "            path=GAME_CONFIGS[\"clean_dfs_directory\"], file_name=\"top_1000_cleaned_rag.pkl\"\n",
    "        )\n",
    "df = df.merge(top_1000, on='BGGId', how='left')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "weaviate_client.connect_weaviate_client_docker()\n",
    "weaviate_client.create_similarity_collection()\n",
    "def add_objects_to_db(row):\n",
    "    uuid = weaviate_client.add_similarity_collection_item(row)\n",
    "    return uuid\n",
    "df['UUID'] = df.apply(add_objects_to_db, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_pickle('temp_top_1000_with_uuid.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get gloomhaven uuid\n",
    "uuid = df.iloc[2]['UUID']\n",
    "uuid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similars = weaviate_client.find_near_objects(collection_name=\"similarity_collection\", uuid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "picks = []\n",
    "\n",
    "for item in similars:\n",
    "    picks.append(str(item.uuid))\n",
    "    \n",
    "df[df['UUID'].isin(picks)]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "boardgamegeek-ZH0FNRKg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
