{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a3bcc119",
   "metadata": {},
   "source": [
    "# Notebook Objective and Setup\n",
    "\n",
    "BGG06 is where synthetic ratings are produced for each user, using the content-based item filter from BGG05."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60b8b86c",
   "metadata": {},
   "source": [
    "## Package Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "492d8e1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import regex as re\n",
    "import time\n",
    "import os\n",
    "import gc\n",
    "import copy\n",
    "import json\n",
    "from statistics import mean\n",
    "\n",
    "# ignore warnings (gets rid of Pandas copy warnings)\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.options.display.max_columns = None\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "\n",
    "#from scipy import sparse\n",
    "#from scipy.sparse import csr_matrix\n",
    "#from scipy import spatial\n",
    "\n",
    "#from sklearn.metrics.pairwise import cosine_similarity\n",
    "#import sklearn.preprocessing as pp\n",
    "from sklearn.preprocessing import MinMaxScaler#, OneHotEncoder, StandardScaler, PolynomialFeatures, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5db3972",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.compat.v1.losses import cosine_distance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ef316c6",
   "metadata": {},
   "source": [
    "## Notebook Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ba3f68b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def produce_synthetic_ratings_all(user, num_ratings_create, game_ids):\n",
    "    '''\n",
    "    Takes in a dictionary of user's ratings and the number of ratings to synthesize\n",
    "    Synthesizes ratings and creates a dictionary of all synthesized ratings for the user\n",
    "    Returns synthesized ratings\n",
    "    \n",
    "    Inputs:\n",
    "    user: the user id to create ratings for\n",
    "    temp_users_dictionary: dictionary of specific user's real ratings\n",
    "    num_ratings_create : simple number. # Ratings to make in the run.\n",
    "    \n",
    "    Outputs:\n",
    "    user_comps_dict : dictionary of synthesized ratings specifically for user\n",
    "    '''\n",
    "    \n",
    "    start = time.time()\n",
    "    \n",
    "    #print(\"Producing items for user\")\n",
    "    \n",
    "    user_items = user_ratings[user]\n",
    "    user_mean = users_means[user]\n",
    "    \n",
    "    temp_users_dictionary = {}\n",
    "    \n",
    "    # copy the current user dictionary to a temp storage dictionary that we can manipulate\n",
    "\n",
    "    for item in user_ratings[user]:\n",
    "        this_rating = round((user_ratings[user][item]-user_mean), 1)\n",
    "        temp_users_dictionary[int(item)] = this_rating\n",
    "        synthetic_users_dictionary[user][int(item)] = int(this_rating*10)\n",
    "        \n",
    "    \n",
    "    # get the original number of ratings by this user\n",
    "    original_num_ratings = len(temp_users_dictionary)\n",
    "    \n",
    "    # start at iteration 0\n",
    "    iteration = 0\n",
    "    \n",
    "    # set up dict to store all specific comps for this user\n",
    "    users_comp_dict = {}\n",
    "\n",
    "    # populate the comps with the user's baseline items\n",
    "    for item in temp_users_dictionary:  \n",
    "        users_comp_dict[item] = [1, 1, item, 0, 0, temp_users_dictionary[item]]\n",
    "        #overall confidence, this item similarity, item, iteration, degrees away, item name\n",
    "       \n",
    "    # while the list of items that the user rated is < the number of ratings needed:\n",
    "    while len(temp_users_dictionary.keys()) < num_ratings_create:\n",
    "        \n",
    "        start_set_length = len(temp_users_dictionary.keys())\n",
    "        \n",
    "        users_rated_items = list(temp_users_dictionary.keys())\n",
    "        #print(len(users_rated_items))\n",
    "        \n",
    "        iteration += 1 # advance the iteration\n",
    "        \n",
    "        #print(\"Starting iteration \"+str(iteration))\n",
    "        \n",
    "        new_items = [] # make a list to hold the items for this iteration        \n",
    "        \n",
    "        # for each rated item:\n",
    "        for rated in users_rated_items:\n",
    "            \n",
    "            #print(\"Current item: \"+str(rated))\n",
    "            \n",
    "            # get rating for current item\n",
    "            rated_rating = temp_users_dictionary[rated]\n",
    "        \n",
    "            # get current best comp:\n",
    "            current_position = 0\n",
    "            current_comp = game_comps_byid_lookup[rated][0][current_position]\n",
    "            \n",
    "            while current_comp in new_items:\n",
    "                \n",
    "                # increment position\n",
    "                current_position+=1 \n",
    "                \n",
    "                if current_position >= 10000:\n",
    "                    #print(current_position)\n",
    "                    break\n",
    "                                                        \n",
    "                else:\n",
    "                    # reset current comp to new position new_items\n",
    "                    current_comp = game_comps_byid_lookup[rated][0][current_position]\n",
    "\n",
    "                    # continue back to check\n",
    "                    continue\n",
    "            \n",
    "            # any time the current comp is in users_rated_items already:\n",
    "            while current_comp in temp_users_dictionary.keys():\n",
    "                \n",
    "                # increment position\n",
    "                current_position+=1 \n",
    "                \n",
    "                if current_position >= 10000:\n",
    "                    #print(current_position)\n",
    "                    break\n",
    "                                    \n",
    "                else:\n",
    "                \n",
    "                    # reset current comp to new position users_comp_dict\n",
    "                    current_comp = game_comps_byid_lookup[rated][0][current_position]\n",
    "\n",
    "                    # continue back to check\n",
    "                    continue\n",
    "            \n",
    "            # The next section activates once the current comp is not already in the user's rated items\n",
    "            \n",
    "            if current_position >= 10000:\n",
    "                #print(current_position)\n",
    "                break\n",
    "                            \n",
    "            else:\n",
    "            \n",
    "                # getting similarity of the current comp\n",
    "                comp_similarity = game_comps_byid_lookup[rated][1][current_position]\n",
    "                \n",
    "              \n",
    "                # get the synthetic rating for the item by taking the rating of the base item * similarity\n",
    "                synthetic_rating = round((rated_rating * comp_similarity), 1)\n",
    "        \n",
    "                # get the overall confidence of this rating \n",
    "                # confidence = confidence of prior item * similarity of current item\n",
    "                confidence = users_comp_dict[rated][0] * comp_similarity\n",
    "                degrees = users_comp_dict[rated][4] + 1\n",
    "\n",
    "                # add this item to the list of new items we are adding to the ratings this round\n",
    "                new_items.append(current_comp)\n",
    "            \n",
    "                # make the user's comp dict\n",
    "                users_comp_dict[current_comp] = [confidence, comp_similarity, rated, iteration, degrees, synthetic_rating]\n",
    "            \n",
    "                # update the temporary dictionary with the synthetic rating for the item\n",
    "                temp_users_dictionary[current_comp] = synthetic_rating\n",
    "                \n",
    "                # add to synthetic users\n",
    "                synthetic_users_dictionary[user][current_comp] = int(synthetic_rating*10)\n",
    "               \n",
    "        end_set_length = len(temp_users_dictionary.keys())\n",
    "            \n",
    "        if start_set_length == end_set_length:\n",
    "            \n",
    "            break\n",
    "        \n",
    "        continue\n",
    "       \n",
    "    end = time.time()\n",
    "    print(str(end-start)+' seconds for user.\\n')\n",
    "    \n",
    "    return users_comp_dict, temp_users_dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c83968d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_synthetic_ratings(user, synthetic_users_dictionary, user_comps_dict, original_num_ratings, desired_ratings):\n",
    "    '''\n",
    "    Takes the user's synthesized comps dict, the original number of ratings the user made, \n",
    "    and the desired number of ratings the user needs.\n",
    "    Creates a df sorting the synthesized ratings by confidence level, \n",
    "    keeping the highest confidence if an item was recommended more than once.\n",
    "    Evaluates number of ratings needed to reach 500 and keeps only that many ratings with the highest confidence.\n",
    "    For each item kept, logs the synthetic rating to the user;s dictionary\n",
    "    \n",
    "    Inputs:\n",
    "    user: specific user to sort\n",
    "    synthetic_users_dictionary: reference to the dictionary of synthesized items\n",
    "    user_comps_dict: dictionary of synthesized ratings specifically for user\n",
    "    original_num_ratings: The number of ratings the user actually rated\n",
    "    desired_ratings: the number of ratings needed by the user\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    # Use this one when you want only exactly x ratings and don't want to necessarily keep everything produced\n",
    "    \n",
    "    # showing synthetic ratings only\n",
    "    user_comps_df = pd.DataFrame(user_comps_dict.values(), index=user_comps_dict.keys(), columns=['OverallConfidence', 'SimtoLast', 'RecFrom', 'DegreesAway', 'SyntheticRating']).sort_values('OverallConfidence', ascending=False).drop_duplicates(keep='first')\n",
    "    \n",
    "    # get a list of the ratings to keep (past the real ratings)\n",
    "    keep_items = list(user_comps_df[original_num_ratings:desired_ratings].index)\n",
    "\n",
    "    # for each item that we keep,\n",
    "    for item in keep_items:\n",
    "    \n",
    "        # add the rating to the real storage dictionary\n",
    "        synthetic_users_dictionary[user][item] = user_comps_df.loc[item]['SyntheticRating']\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c38f9b8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def populate_all_ratings(user, synthetic_users_dictionary, temp_users_dictionary):\n",
    "    '''\n",
    "    Takes the user's synthesized comps dict, the original number of ratings the user made, \n",
    "    and the desired number of ratings the user needs.\n",
    "    Creates a df sorting the synthesized ratings by confidence level, \n",
    "    keeping the highest confidence if an item was recommended more than once.\n",
    "    Evaluates number of ratings needed to reach 500 and keeps only that many ratings with the highest confidence.\n",
    "    For each item kept, logs the synthetic rating to the user;s dictionary\n",
    "    \n",
    "    Inputs:\n",
    "    user: specific user to sort\n",
    "    synthetic_users_dictionary: reference to the dictionary of synthesized items\n",
    "    user_comps_dict: dictionary of synthesized ratings specifically for user\n",
    "    original_num_ratings: The number of ratings the user actually rated\n",
    "    desired_ratings: the number of ratings needed by the user\n",
    "    \n",
    "    '''   \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    not_rated = list(set(game_ids) - set(temp_users_dictionary.keys()))\n",
    "    print(str(len(not_rated))+\" games were not rated\")\n",
    "            \n",
    "    for item in not_rated:\n",
    "        temp_users_dictionary[item] = 0\n",
    "        users_comp_dict[item] = [0, 0, 0, iteration, 0, 0]\n",
    "    \n",
    "    print(\"End length of rated items is \"+str(len(temp_users_dictionary)))\n",
    "    \n",
    "    # get a list of the ratings to keep (past the real ratings)\n",
    "    keep_items = sorted(list(temp_users_dictionary.keys()))\n",
    "\n",
    "    # for each item that we keep,\n",
    "    for item in keep_items:\n",
    "    \n",
    "        # add the rating to the real storage dictionary\n",
    "        synthetic_users_dictionary[user][item] = temp_users_dictionary[item]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e55353f",
   "metadata": {},
   "source": [
    "## Required Data Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3803aba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read games for game_ids\n",
    "games = pd.read_pickle('data_cleaned_new_scraper/games.pkl')\n",
    "game_ids = list(games['BGGId'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e56ce5c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read cosine similarity pickle\n",
    "sims_byid = pd.read_pickle('data_cleaned_new_scraper/game_cosine_similarity_byid.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "787d2be7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Opening JSON file\n",
    "with open('data_cleaned_new_scraper/user_means.json') as json_file:\n",
    "    users_means = json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dacb3fe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Opening JSON file\n",
    "with open('data_cleaned_new_scraper/user_ratings.json') as json_file:\n",
    "    user_ratings = json.load(json_file)\n",
    "\n",
    "all_users = list(user_ratings.keys())\n",
    "\n",
    "user_block_1 = all_users[:40000]\n",
    "user_block_2 = all_users[40000:80000]\n",
    "user_block_3 = all_users[80000:120000]\n",
    "user_block_4 = all_users[120000:160000]\n",
    "user_block_5 = all_users[160000:200000]\n",
    "user_block_6 = all_users[200000:240000]\n",
    "user_block_7 = all_users[240000:]\n",
    "\n",
    "user_blocks = [user_block_1, user_block_2, user_block_3, user_block_4, user_block_5, user_block_6, user_block_7]\n",
    "\n",
    "del user_ratings\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f63e1e71",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# dictionary of game IDs-Names\n",
    "\n",
    "# Load games\n",
    "games = pd.read_pickle('data_cleaned_new_scraper/games.pkl')\n",
    "\n",
    "# lists of game ids and game names\n",
    "game_ids = list(games['BGGId'])\n",
    "game_names = list(games['Name'])\n",
    "\n",
    "# make lookup dictionary\n",
    "game_id_lookup = {}\n",
    "\n",
    "# store ids and names in lookup dictionary\n",
    "for key, item in zip(game_ids, game_names):\n",
    "    game_id_lookup[key] = item\n",
    "\n",
    "    \n",
    "del games\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02a44e82",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(game_id_lookup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feb1ec29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get top 1000 most similar games for each game and store in dictionary\n",
    "\n",
    "game_comps_byid_lookup = {}\n",
    "\n",
    "for item in sims_byid.columns:\n",
    "    results = pd.DataFrame(data={'Similarity': sims_byid[item].sort_values(ascending=False)[1:]})\n",
    "    current_cap = results['Similarity'].max()\n",
    "    comps_index = list(results[:5000].index.astype('int32'))\n",
    "    comps_similarity = list(results[:5000]['Similarity'])\n",
    "    game_comps_byid_lookup[item] = [comps_index, comps_similarity]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97992357",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "del sims_byid\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "000edb67",
   "metadata": {},
   "outputs": [],
   "source": [
    "#with open('game_comps_byid_lookup.json', 'w') as convert_file:\n",
    "#     convert_file.write(json.dumps(game_comps_byid_lookup))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a616634c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Opening JSON file\n",
    "#with open('game_comps_byid_lookup.json') as json_file:\n",
    "#    game_comps_byid_lookup = json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae074fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of synthetic ratings to produce\n",
    "#num_ratings_create = 200\n",
    "\n",
    "# number of ratings we will end up using\n",
    "#desired_ratings = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec41415f",
   "metadata": {},
   "source": [
    "# Produce Synthetic Ratings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfd782f8",
   "metadata": {},
   "source": [
    "## Test One User"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20179faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Opening JSON file\n",
    "with open('data_cleaned_new_scraper/user_ratings.json') as json_file:\n",
    "    user_ratings = json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09ccad62",
   "metadata": {},
   "outputs": [],
   "source": [
    "user = 'Monika1234'\n",
    "user_mean = users_means[user]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "788a07af",
   "metadata": {},
   "outputs": [],
   "source": [
    "user = 'Torsten'\n",
    "user_mean = users_means[user]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e63783c6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "this_user = pd.DataFrame(user_ratings[user].values(), index=user_ratings[user].keys())\n",
    "this_user.reset_index(inplace=True)\n",
    "this_user.rename(columns={0:'Rating', 'index':'BGGId'}, inplace=True)\n",
    "this_user['Game'] = this_user['BGGId'].astype('int32').map(game_id_lookup)\n",
    "this_user.sort_values('Game', ascending=True).head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b28bc6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(game_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "970b232f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of synthetic ratings to produce\n",
    "num_ratings_create = 2500\n",
    "\n",
    "# number of ratings we will end up using\n",
    "desired_ratings = 2500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6131591",
   "metadata": {},
   "outputs": [],
   "source": [
    "del synthetic_users_dictionary\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaeca88c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up a synthetic ratings dictionary to store the users and ratings\n",
    "synthetic_users_dictionary = {}\n",
    "synthetic_users_dictionary[user] = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7b6a183",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"Starting user \"+user)\n",
    "\n",
    "# call function to produce synthetic ratings\n",
    "user_comps_dict, temp_users_dictionary  = produce_synthetic_ratings_all(user, num_ratings_create, game_ids) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce71fd39",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp2 = pd.DataFrame(synthetic_users_dictionary[user].values(), index=synthetic_users_dictionary[user].keys())\n",
    "temp2['Game'] = temp2.index.map(game_id_lookup)\n",
    "temp2['Rating'] = (temp2[0]/10)+user_mean\n",
    "temp2.reset_index(inplace=True)\n",
    "temp2.drop(['index', 0], axis=1, inplace=True)\n",
    "temp2.sort_values('Rating', ascending=False).head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88affa0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_comps_df = pd.DataFrame(user_comps_dict.values(), index=user_comps_dict.keys(), columns=['OverallConfidence', 'SimtoLast', 'RecFrom', 'Iteration', 'DegreesAway', 'SyntheticRating']).sort_values('OverallConfidence', ascending=False).drop_duplicates(keep='first')\n",
    "\n",
    "user_comps_df['SyntheticRating'] = user_comps_df['SyntheticRating']+user_mean\n",
    "user_comps_df['RecommendedItem'] = user_comps_df.index.map(game_id_lookup)\n",
    "user_comps_df['Seed'] = user_comps_df['RecFrom'].map(game_id_lookup)\n",
    "user_comps_df.sort_values('SyntheticRating', ascending=False).head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02496dc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_comps_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09aaa46a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(20,10))\n",
    "\n",
    "sns.set(font_scale = 1.5) # set our font scale bigger for this vis\n",
    "\n",
    "# scatter our data\n",
    "sns.set_style('darkgrid')\n",
    "scatter2 = sns.scatterplot(x=\"DegreesAway\", y='SyntheticRating', data=user_comps_df, \n",
    "                           hue='DegreesAway', palette='viridis', s=100)\n",
    "ax.axhline(user_mean)\n",
    "ax.text(x=.5, y=(user_mean+.2), s='User Mean '+str(user_mean), alpha=0.7, color='black')\n",
    "\n",
    "ax.get_legend().remove()\n",
    "\n",
    "plt.title(str(desired_ratings)+\" Synthetic Ratings for a 10-Rating User\", fontsize=30)\n",
    "plt.xlabel(\"Steps Away from True Rating\", fontsize=20)\n",
    "plt.ylabel(\"Rating\", fontsize=20)\n",
    "\n",
    "\n",
    "plt.tight_layout\n",
    "#plt.savefig('images/synthetic_from10.png')\n",
    "plt.show()\n",
    ";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ae59614",
   "metadata": {},
   "outputs": [],
   "source": [
    "del synthetic_users_dictionary\n",
    "del user_comps_df\n",
    "del temp_users_dictionary\n",
    "del this_user\n",
    "del user_ratings\n",
    "del user_comps_dict\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfe8d70a",
   "metadata": {},
   "source": [
    "## Process ALL Users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9977ae24",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(game_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "275bbfcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of synthetic ratings to produce\n",
    "num_ratings_create = 250\n",
    "\n",
    "# number of ratings we will end up using\n",
    "desired_ratings = 250"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a271dba8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "block_marker = 0\n",
    "\n",
    "for block in user_blocks:\n",
    "\n",
    "    block_marker +=1\n",
    "    \n",
    "    # Opening JSON file\n",
    "    with open('data_cleaned_new_scraper/user_ratings_block_'+str(block_marker)+'.json') as json_file:\n",
    "        user_ratings = json.load(json_file)\n",
    "    \n",
    "    # set up a synthetic ratings dictionary to store the users and ratings\n",
    "    synthetic_users_dictionary = {}\n",
    "    \n",
    "    user_count = 0\n",
    "    \n",
    "    for user in block:\n",
    "        print(user)\n",
    "        user_count+=1\n",
    "        \n",
    "        synthetic_users_dictionary[user] = {}\n",
    "   \n",
    "        print(\"Starting user \"+str(user_count))\n",
    "               \n",
    "        # call function to produce synthetic ratings\n",
    "        user_comps_dict, temp_users_dictionary = produce_synthetic_ratings_all(user, num_ratings_create, game_ids) \n",
    "    \n",
    "        #sort_synthetic_ratings(user, synthetic_users_dictionary, temp_users_dictionary)\n",
    "    \n",
    "        del user_comps_dict\n",
    "        del temp_users_dictionary\n",
    "        #gc.collect()\n",
    "\n",
    "    # save dictionary\n",
    "    with open('synthetic_ratings/users_synthetic_250_'+str(block_marker)+'.json', 'w') as convert_file:\n",
    "        convert_file.write(json.dumps(synthetic_users_dictionary))\n",
    "    \n",
    "    del synthetic_users_dictionary\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbfd1db9",
   "metadata": {},
   "outputs": [],
   "source": [
    "del user_ratings\n",
    "del game_comps_byid_lookup\n",
    "del user_blocks\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5bc6cd0",
   "metadata": {},
   "source": [
    "# Produce Matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33ea7eae",
   "metadata": {},
   "outputs": [],
   "source": [
    "larger_matrix = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "475b0e89",
   "metadata": {},
   "outputs": [],
   "source": [
    "for append in range(1, 8):\n",
    "    \n",
    "    print(\"Opening file \"+str(append))\n",
    "    with open('synthetic_ratings/users_synthetic_250_'+str(append)+'.json') as json_file:\n",
    "        set_of_ratings = json.load(json_file)\n",
    "        \n",
    "    print(\"Converting file to DF\")\n",
    "    matrix = pd.DataFrame(set_of_ratings).T\n",
    "\n",
    "    print(\"Clearing memory\")\n",
    "    del set_of_ratings\n",
    "    gc.collect()\n",
    "\n",
    "    print(\"Filling NaN\")\n",
    "    matrix.fillna(0, inplace=True)\n",
    "    \n",
    "    print(\"Converting to Int8\")\n",
    "    matrix = matrix.astype('int8') \n",
    "    \n",
    "    #print(\"Converting to sparse\")\n",
    "    #matrix_sparsed = matrix.astype(pd.SparseDtype(\"float32\"))\n",
    "    \n",
    "    print(\"Adding to larger DF\")\n",
    "    larger_matrix = larger_matrix.append(matrix)\n",
    "           \n",
    "    print(larger_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bd98942",
   "metadata": {},
   "outputs": [],
   "source": [
    "larger_matrix.fillna(0, inplace=True)\n",
    "larger_matrix = larger_matrix.astype('int8')\n",
    "#larger_matrix = larger_matrix.astype(pd.SparseDtype(\"float32\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40be93a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "larger_matrix.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ae69a2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "larger_matrix.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88b99671",
   "metadata": {},
   "outputs": [],
   "source": [
    "larger_matrix.to_pickle('synthetic_ratings/users_synthetic_250_fullmatrix.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cd1edc4",
   "metadata": {},
   "source": [
    "# Similarity Calculations - Tensorflow (GPU only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b3be941d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import regex as re\n",
    "import time\n",
    "import os\n",
    "import gc\n",
    "import copy\n",
    "import json\n",
    "from statistics import mean\n",
    "\n",
    "# ignore warnings (gets rid of Pandas copy warnings)\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.options.display.max_columns = None\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "\n",
    "from scipy import sparse\n",
    "from scipy.sparse import csr_matrix\n",
    "from scipy import spatial\n",
    "\n",
    "#from sklearn.metrics.pairwise import cosine_similarity\n",
    "#import sklearn.preprocessing as pp\n",
    "#from sklearn.preprocessing import StandardScaler, PolynomialFeatures, MinMaxScaler, OneHotEncoder\n",
    "from sklearn.preprocessing import MinMaxScaler, normalize\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.compat.v1.losses import cosine_distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "722f1152",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the basic file required for this work - the full matrix\n",
    "\n",
    "larger_matrix = pd.read_pickle('synthetic_ratings/users_synthetic_1k_fullmatrix.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a0421a3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert full matrix to numpy and delete matrix\n",
    "\n",
    "matrix_array = larger_matrix.to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b484aac",
   "metadata": {},
   "source": [
    "## Preparing the user blocks and user storage dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d110c1cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Lookup dictionary complete\n"
     ]
    }
   ],
   "source": [
    "users_list = list(larger_matrix.index)\n",
    "\n",
    "user_blocks_lookup = {}\n",
    "\n",
    "chunk_size = int(np.ceil(matrix_array.shape[0]/20))\n",
    "\n",
    "start = 0\n",
    "incrementer = 0\n",
    "\n",
    "while start < matrix_array.shape[0]:\n",
    "    \n",
    "    end = start + chunk_size\n",
    "    incrementer += 1\n",
    "    \n",
    "    user_blocks_lookup[incrementer] = users_list[start:end]\n",
    "\n",
    "    start += chunk_size\n",
    "\n",
    "print(\"\\nLookup dictionary complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f42d4c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_id_lookup = {}\n",
    "\n",
    "increment=0\n",
    "for user in users_list:\n",
    "    \n",
    "    increment+=1\n",
    "    user_id_lookup[increment] = user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d741fca7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Lookup dictionary complete\n"
     ]
    }
   ],
   "source": [
    "block_indices_lookup = {}\n",
    "\n",
    "start = 0\n",
    "incrementer = 0\n",
    "\n",
    "while start < matrix_array.shape[0]:\n",
    "    \n",
    "    end = start + chunk_size\n",
    "    incrementer += 1\n",
    "    \n",
    "    block_indices_lookup[incrementer] = {}\n",
    "    block_indices_lookup[incrementer]['Start'] = start\n",
    "    block_indices_lookup[incrementer]['End'] = end\n",
    "    \n",
    "    start += chunk_size\n",
    "\n",
    "print(\"\\nLookup dictionary complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "269f2e69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "85"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del larger_matrix\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0dee479",
   "metadata": {},
   "source": [
    "ONLY RUN THIS AGAIN IF THE USER LIST CHANGES !!!!!  THIS WILL RESET ALL STORAGE DICTIONARIES ON DISK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cb86f0e0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "'''max_range = len(user_blocks_lookup)+1\n",
    "\n",
    "for item in np.arange(1,max_range,1):\n",
    "    \n",
    "    storage_dict = {}\n",
    "    \n",
    "    for user in user_blocks_lookup[item]:\n",
    "        storage_dict[user] = {}\n",
    "    \n",
    "    # save dictionary\n",
    "    with open('user_similarities/similarity_storage_1k_'+str(item)+'.json', 'w') as convert_file:\n",
    "        convert_file.write(json.dumps(storage_dict))\n",
    "    \n",
    "    del storage_dict'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cb59f98",
   "metadata": {},
   "source": [
    "##### Files:\n",
    "\n",
    "- user_blocks_lookup  dict in format  dict[file_append]:[list of users in block]\n",
    "- user_id_lookup  dict in format dict[user_id] = username\n",
    "- block_indices_lookup  dict in format dict[file_append]: {'Start': start index, 'End': end index}\n",
    "- storage dictionaries located at 'user_similarities/similarity_storage'+str(file_append)+'.json'\n",
    "- matrix_array  numpy array which must be numerically indexed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1a402ad",
   "metadata": {},
   "source": [
    "#### Calculation Steps\n",
    "\n",
    "- Set up matrix blocks a and b\n",
    "    - b should already be transposed\n",
    "- Load matrices into tensors a and b\n",
    "- Normalize each tensor on axis 1 and del variables as they are used\n",
    "- Do matmul an normed a and b\n",
    "- Profit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b5fa99a8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Lookup dictionary complete\n"
     ]
    }
   ],
   "source": [
    "# TEMP STUFF\n",
    "\n",
    "block_indices_lookup = {}\n",
    "\n",
    "start = 0\n",
    "incrementer = 0\n",
    "\n",
    "\n",
    "end = start + chunk_size\n",
    "    \n",
    "block_indices_lookup[1] = {}\n",
    "block_indices_lookup[1]['Start'] = start\n",
    "block_indices_lookup[1]['End'] = end\n",
    "\n",
    "print(\"\\nLookup dictionary complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cc1ab727",
   "metadata": {},
   "outputs": [],
   "source": [
    "precompute_matrix = matrix_array[:134000].T\n",
    "precompute_matrix2 = matrix_array[134000:].T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c6d5dcc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_user_block(array_chunk_a, matrix, indices):\n",
    "    \n",
    "    this_start = time.time()\n",
    "    # make additional array parts to compare user against. Memory limitations here.\n",
    "    #array_chunk_b = ((matrix[indices, :]).astype('float32'))\n",
    "    array_chunk_b = (matrix[indices, :])#.astype('float32'))\n",
    "    checkpoint = time.time()\n",
    "    #print(str(checkpoint-this_start)+\" Made compacted array\")\n",
    "    \n",
    "    #normalize_b = normalize(array_chunk_b, axis=0)\n",
    "    \n",
    "    a = tf.constant(array_chunk_a, dtype=tf.float32)\n",
    "\n",
    "    #b = tf.constant(array_chunk_b)\n",
    "    #b = tf.constant(normalize_b)\n",
    "    \n",
    "    b = tf.constant(array_chunk_b, dtype=tf.float32)\n",
    "    \n",
    "    #b_norm = np.linalg.norm(array_chunk_b, axis=0)\n",
    "    #b = b/b_norm\n",
    "    \n",
    "    checkpoint1 = time.time()      \n",
    "    #print(str(checkpoint1-checkpoint)+\" Loaded into Tensors\")\n",
    "\n",
    "    a = tf.nn.l2_normalize(a, 1)\n",
    "    b = tf.nn.l2_normalize(b, 0)\n",
    "    \n",
    "    checkpoint2 = time.time()      \n",
    "    #print(str(checkpoint2-checkpoint1)+\" normalized\")    \n",
    "    \n",
    "    similarities = tf.matmul(a, b)\n",
    "    checkpoint3 = time.time()\n",
    "    #print(str(checkpoint3-checkpoint2)+\" Got Similarity Scores\")\n",
    "           \n",
    "    user_similarities = similarities.numpy().reshape(-1,1)\n",
    "        \n",
    "    return user_similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2ddc9d35",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting block 1\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "181\n",
      "182\n",
      "183\n",
      "184\n",
      "185\n",
      "186\n",
      "187\n",
      "188\n",
      "189\n",
      "190\n",
      "191\n",
      "192\n",
      "193\n",
      "194\n",
      "195\n",
      "196\n",
      "197\n",
      "198\n",
      "199\n",
      "200\n",
      "201\n",
      "202\n",
      "203\n",
      "204\n",
      "205\n",
      "206\n",
      "207\n",
      "208\n",
      "209\n",
      "210\n",
      "211\n",
      "212\n",
      "213\n",
      "214\n",
      "215\n",
      "216\n",
      "217\n",
      "218\n",
      "219\n",
      "220\n",
      "221\n",
      "222\n",
      "223\n",
      "224\n",
      "225\n",
      "226\n",
      "227\n",
      "228\n",
      "229\n",
      "230\n",
      "231\n",
      "232\n",
      "233\n",
      "234\n",
      "235\n",
      "236\n",
      "237\n",
      "238\n",
      "239\n",
      "240\n",
      "241\n",
      "242\n",
      "243\n",
      "244\n",
      "245\n",
      "246\n",
      "247\n",
      "248\n",
      "249\n",
      "250\n",
      "251\n",
      "252\n",
      "253\n",
      "254\n",
      "255\n",
      "256\n",
      "257\n",
      "258\n",
      "259\n",
      "260\n",
      "261\n",
      "262\n",
      "263\n",
      "264\n",
      "265\n",
      "266\n",
      "267\n",
      "268\n",
      "269\n",
      "270\n",
      "271\n",
      "272\n",
      "273\n",
      "274\n",
      "275\n",
      "276\n",
      "277\n",
      "278\n",
      "279\n",
      "280\n",
      "281\n",
      "282\n",
      "283\n",
      "284\n",
      "285\n",
      "286\n",
      "287\n",
      "288\n",
      "289\n",
      "290\n",
      "291\n",
      "292\n",
      "293\n",
      "294\n",
      "295\n",
      "296\n",
      "297\n",
      "298\n",
      "299\n",
      "300\n",
      "301\n",
      "302\n",
      "303\n",
      "304\n",
      "305\n",
      "306\n",
      "307\n",
      "308\n",
      "309\n",
      "310\n",
      "311\n",
      "312\n",
      "313\n",
      "314\n",
      "315\n",
      "316\n",
      "317\n",
      "318\n",
      "319\n",
      "320\n",
      "321\n",
      "322\n",
      "323\n",
      "324\n",
      "325\n",
      "326\n",
      "327\n",
      "328\n",
      "329\n",
      "330\n",
      "331\n",
      "332\n",
      "333\n",
      "334\n",
      "335\n",
      "336\n",
      "337\n",
      "338\n",
      "339\n",
      "340\n",
      "341\n",
      "342\n",
      "343\n",
      "344\n",
      "345\n",
      "346\n",
      "347\n",
      "348\n",
      "349\n",
      "350\n",
      "351\n",
      "352\n",
      "353\n",
      "354\n",
      "355\n",
      "356\n",
      "357\n",
      "358\n",
      "359\n",
      "360\n",
      "361\n",
      "362\n",
      "363\n",
      "364\n",
      "365\n",
      "366\n",
      "367\n",
      "368\n",
      "369\n",
      "370\n",
      "371\n",
      "372\n",
      "373\n",
      "374\n",
      "375\n",
      "376\n",
      "377\n",
      "378\n",
      "379\n",
      "380\n",
      "381\n",
      "382\n",
      "383\n",
      "384\n",
      "385\n",
      "386\n",
      "387\n",
      "388\n",
      "389\n",
      "390\n",
      "391\n",
      "392\n",
      "393\n",
      "394\n",
      "395\n",
      "396\n",
      "397\n",
      "398\n",
      "399\n",
      "400\n",
      "401\n",
      "402\n",
      "403\n",
      "404\n",
      "405\n",
      "406\n",
      "407\n",
      "408\n",
      "409\n",
      "410\n",
      "411\n",
      "412\n",
      "413\n",
      "414\n",
      "415\n",
      "416\n",
      "417\n",
      "418\n",
      "419\n",
      "420\n",
      "421\n",
      "422\n",
      "423\n",
      "424\n",
      "425\n",
      "426\n",
      "427\n",
      "428\n",
      "429\n",
      "430\n",
      "431\n",
      "432\n",
      "433\n",
      "434\n",
      "435\n",
      "436\n",
      "437\n",
      "438\n",
      "439\n",
      "440\n",
      "441\n",
      "442\n",
      "443\n",
      "444\n",
      "445\n",
      "446\n",
      "447\n",
      "448\n",
      "449\n",
      "450\n",
      "451\n",
      "452\n",
      "453\n",
      "454\n",
      "455\n",
      "456\n",
      "457\n",
      "458\n",
      "459\n",
      "460\n",
      "461\n",
      "462\n",
      "463\n",
      "464\n",
      "465\n",
      "466\n",
      "467\n",
      "468\n",
      "469\n",
      "470\n",
      "471\n",
      "472\n",
      "473\n",
      "474\n",
      "475\n",
      "476\n",
      "477\n",
      "478\n",
      "479\n",
      "480\n",
      "481\n",
      "482\n",
      "483\n",
      "484\n",
      "485\n",
      "486\n",
      "487\n",
      "488\n",
      "489\n",
      "490\n",
      "491\n",
      "492\n",
      "493\n",
      "494\n",
      "495\n",
      "496\n",
      "497\n",
      "498\n",
      "499\n",
      "500\n",
      "501\n",
      "502\n",
      "503\n",
      "504\n",
      "505\n",
      "506\n",
      "507\n",
      "508\n",
      "509\n",
      "510\n",
      "511\n",
      "512\n",
      "513\n",
      "514\n",
      "515\n",
      "516\n",
      "517\n",
      "518\n",
      "519\n",
      "520\n",
      "521\n",
      "522\n",
      "523\n",
      "524\n",
      "525\n",
      "526\n",
      "527\n",
      "528\n",
      "529\n",
      "530\n",
      "531\n",
      "532\n",
      "533\n",
      "534\n",
      "535\n",
      "536\n",
      "537\n",
      "538\n",
      "539\n",
      "540\n",
      "541\n",
      "542\n",
      "543\n",
      "544\n",
      "545\n",
      "546\n",
      "547\n",
      "548\n",
      "549\n",
      "550\n",
      "551\n",
      "552\n",
      "553\n",
      "554\n",
      "555\n",
      "556\n",
      "557\n",
      "558\n",
      "559\n",
      "560\n",
      "561\n",
      "562\n",
      "563\n",
      "564\n",
      "565\n",
      "566\n",
      "567\n",
      "568\n",
      "569\n",
      "570\n",
      "571\n",
      "572\n",
      "573\n",
      "574\n",
      "575\n",
      "576\n",
      "577\n",
      "578\n",
      "579\n",
      "580\n",
      "581\n",
      "582\n",
      "583\n",
      "584\n",
      "585\n",
      "586\n",
      "587\n",
      "588\n",
      "589\n",
      "590\n",
      "591\n",
      "592\n",
      "593\n",
      "594\n",
      "595\n",
      "596\n",
      "597\n",
      "598\n",
      "599\n",
      "600\n",
      "601\n",
      "602\n",
      "603\n",
      "604\n",
      "605\n",
      "606\n",
      "607\n",
      "608\n",
      "609\n",
      "610\n",
      "611\n",
      "612\n",
      "613\n",
      "614\n",
      "615\n",
      "616\n",
      "617\n",
      "618\n",
      "619\n",
      "620\n",
      "621\n",
      "622\n",
      "623\n",
      "624\n",
      "625\n",
      "626\n",
      "627\n",
      "628\n",
      "629\n",
      "630\n",
      "631\n",
      "632\n",
      "633\n",
      "634\n",
      "635\n",
      "636\n",
      "637\n",
      "638\n",
      "639\n",
      "640\n",
      "641\n",
      "642\n",
      "643\n",
      "644\n",
      "645\n",
      "646\n",
      "647\n",
      "648\n",
      "649\n",
      "650\n",
      "651\n",
      "652\n",
      "653\n",
      "654\n",
      "655\n",
      "656\n",
      "657\n",
      "658\n",
      "659\n",
      "660\n",
      "661\n",
      "662\n",
      "663\n",
      "664\n",
      "665\n",
      "666\n",
      "667\n",
      "668\n",
      "669\n",
      "670\n",
      "671\n",
      "672\n",
      "673\n",
      "674\n",
      "675\n",
      "676\n",
      "677\n",
      "678\n",
      "679\n",
      "680\n",
      "681\n",
      "682\n",
      "683\n",
      "684\n",
      "685\n",
      "686\n",
      "687\n",
      "688\n",
      "689\n",
      "690\n",
      "691\n",
      "692\n",
      "693\n",
      "694\n",
      "695\n",
      "696\n",
      "697\n",
      "698\n",
      "699\n",
      "700\n",
      "701\n",
      "702\n",
      "703\n",
      "704\n",
      "705\n",
      "706\n",
      "707\n",
      "708\n",
      "709\n",
      "710\n",
      "711\n",
      "712\n",
      "713\n",
      "714\n",
      "715\n",
      "716\n",
      "717\n",
      "718\n",
      "719\n",
      "720\n",
      "721\n",
      "722\n",
      "723\n",
      "724\n",
      "725\n",
      "726\n",
      "727\n",
      "728\n",
      "729\n",
      "730\n",
      "731\n",
      "732\n",
      "733\n",
      "734\n",
      "735\n",
      "736\n",
      "737\n",
      "738\n",
      "739\n",
      "740\n",
      "741\n",
      "742\n",
      "743\n",
      "744\n",
      "745\n",
      "746\n",
      "747\n",
      "748\n",
      "749\n",
      "750\n",
      "751\n",
      "752\n",
      "753\n",
      "754\n",
      "755\n",
      "756\n",
      "757\n",
      "758\n",
      "759\n",
      "760\n",
      "761\n",
      "762\n",
      "763\n",
      "764\n",
      "765\n",
      "766\n",
      "767\n",
      "768\n",
      "769\n",
      "770\n",
      "771\n",
      "772\n",
      "773\n",
      "774\n",
      "775\n",
      "776\n",
      "777\n",
      "778\n",
      "779\n",
      "780\n",
      "781\n",
      "782\n",
      "783\n",
      "784\n",
      "785\n",
      "786\n",
      "787\n",
      "788\n",
      "789\n",
      "790\n",
      "791\n",
      "792\n",
      "793\n",
      "794\n",
      "795\n",
      "796\n",
      "797\n",
      "798\n",
      "799\n",
      "800\n",
      "801\n",
      "802\n",
      "803\n",
      "804\n",
      "805\n",
      "806\n",
      "807\n",
      "808\n",
      "809\n",
      "810\n",
      "811\n",
      "812\n",
      "813\n",
      "814\n",
      "815\n",
      "816\n",
      "817\n",
      "818\n",
      "819\n",
      "820\n",
      "821\n",
      "822\n",
      "823\n",
      "824\n",
      "825\n",
      "826\n",
      "827\n",
      "828\n",
      "829\n",
      "830\n",
      "831\n",
      "832\n",
      "833\n",
      "834\n",
      "835\n",
      "836\n",
      "837\n",
      "838\n",
      "839\n",
      "840\n",
      "841\n",
      "842\n",
      "843\n",
      "844\n",
      "845\n",
      "846\n",
      "847\n",
      "848\n",
      "849\n",
      "850\n",
      "851\n",
      "852\n",
      "853\n",
      "854\n",
      "855\n",
      "856\n",
      "857\n",
      "858\n",
      "859\n",
      "860\n",
      "861\n",
      "862\n",
      "863\n",
      "864\n",
      "865\n",
      "866\n",
      "867\n",
      "868\n",
      "869\n",
      "870\n",
      "871\n",
      "872\n",
      "873\n",
      "874\n",
      "875\n",
      "876\n",
      "877\n",
      "878\n",
      "879\n",
      "880\n",
      "881\n",
      "882\n",
      "883\n",
      "884\n",
      "885\n",
      "886\n",
      "887\n",
      "888\n",
      "889\n",
      "890\n",
      "891\n",
      "892\n",
      "893\n",
      "894\n",
      "895\n",
      "896\n",
      "897\n",
      "898\n",
      "899\n",
      "900\n",
      "901\n",
      "902\n",
      "903\n",
      "904\n",
      "905\n",
      "906\n",
      "907\n",
      "908\n",
      "909\n",
      "910\n",
      "911\n",
      "912\n",
      "913\n",
      "914\n",
      "915\n",
      "916\n",
      "917\n",
      "918\n",
      "919\n",
      "920\n",
      "921\n",
      "922\n",
      "923\n",
      "924\n",
      "925\n",
      "926\n",
      "927\n",
      "928\n",
      "929\n",
      "930\n",
      "931\n",
      "932\n",
      "933\n",
      "934\n",
      "935\n",
      "936\n",
      "937\n",
      "938\n",
      "939\n",
      "940\n",
      "941\n",
      "942\n",
      "943\n",
      "944\n",
      "945\n",
      "946\n",
      "947\n",
      "948\n",
      "949\n",
      "950\n",
      "951\n",
      "952\n",
      "953\n",
      "954\n",
      "955\n",
      "956\n",
      "957\n",
      "958\n",
      "959\n",
      "960\n",
      "961\n",
      "962\n",
      "963\n",
      "964\n",
      "965\n",
      "966\n",
      "967\n",
      "968\n",
      "969\n",
      "970\n",
      "971\n",
      "972\n",
      "973\n",
      "974\n",
      "975\n",
      "976\n",
      "977\n",
      "978\n",
      "979\n",
      "980\n",
      "981\n",
      "982\n",
      "983\n",
      "984\n",
      "985\n",
      "986\n",
      "987\n",
      "988\n",
      "989\n",
      "990\n",
      "991\n",
      "992\n",
      "993\n",
      "994\n",
      "995\n",
      "996\n",
      "997\n",
      "998\n",
      "999\n",
      "1000\n",
      "Saving dictionary for this set of users\n",
      "Average time per user: 0.8517002968878655\n"
     ]
    }
   ],
   "source": [
    "time_test = []\n",
    "\n",
    "# for each user block in the block_indices_lookup. The user blocks are integers from 1-20\n",
    "for user_block in block_indices_lookup:\n",
    "       \n",
    "    print(\"Starting block \"+str(user_block))\n",
    "    \n",
    "    # Get the start and end indexes for the block\n",
    "    starting_block_indexes = block_indices_lookup[user_block]\n",
    "    base_start = starting_block_indexes['Start'] # starting user\n",
    "    base_end = starting_block_indexes['End'] # ending user\n",
    "    \n",
    "    # Load the storage dictionary for this block\n",
    "    with open('user_similarities/similarity_storage_1k_'+str(user_block)+'.json') as json_file:\n",
    "        base_users_storage = json.load(json_file) \n",
    "    \n",
    "    # only do the user ids in this block, then save to the fils\n",
    "    for user_id in np.arange(base_start, 1001, 1):#base_end, 1):\n",
    "        print(user_id)\n",
    "        \n",
    "        user_name = user_id_lookup[user_id+1]\n",
    "        #print(user_name)\n",
    "    \n",
    "        # log start time\n",
    "        #print(\"Making matrices\")\n",
    "        start = time.time()\n",
    "               \n",
    "        # make the single user matrix for the one user\n",
    "        single_user = matrix_array[user_id].reshape(1,-1)\n",
    "        # get the indices where the user is nonzero\n",
    "        indices = list(np.nonzero(single_user)[1])\n",
    "        # make the user with only the nonzero indices\n",
    "        array_chunk_a = (single_user[:, indices])#.astype('float32')\n",
    "        #normalize_a = normalize(array_chunk_a, axis=1)\n",
    "        checkpoint = time.time()\n",
    "        #print(str(checkpoint-start)+\" Processed single user\")\n",
    "        \n",
    "        #a = tf.constant(array_chunk_a)\n",
    "        #a = tf.nn.l2_normalize(a, 1)\n",
    "        \n",
    "        #process_user_block(a, precompute_matrix, indices)\n",
    "        user_similarities_1 = process_user_block(array_chunk_a, precompute_matrix, indices)\n",
    "        user_similarities_2 = process_user_block(array_chunk_a, precompute_matrix2, indices)\n",
    "        #user_similarities = process_user_block(array_chunk_a, precompute_matrix, indices)\n",
    "        \n",
    "        \n",
    "        checkpoint3 = time.time()\n",
    "        user_similarities = np.append(user_similarities_1, user_similarities_2)   \n",
    "        max_spot = np.argmax(user_similarities)\n",
    "        mean_spot = np.median(user_similarities)\n",
    "        user_similarities[max_spot] = mean_spot\n",
    "        scaler = MinMaxScaler(feature_range=(-1,1))\n",
    "        user_similarities = scaler.fit_transform(user_similarities.reshape(-1,1)).ravel()\n",
    "        #user_similarities = list(np.round(user_similarities, 2).ravel())\n",
    "        \n",
    "        checkpoint4 = time.time()\n",
    "        #print(str(checkpoint4-checkpoint3)+\" Processed/Scaled Similarity scores\") \n",
    "        \n",
    "        over75 = list((user_similarities >= .6).nonzero()[0])\n",
    "        under75 = list((user_similarities <= -.6).nonzero()[0])\n",
    "        all_comps = over75 + under75\n",
    "        \n",
    "        for item in all_comps:\n",
    "            item = int(item)\n",
    "            base_users_storage[user_name][item] = float(user_similarities[item])\n",
    "          \n",
    "        checkpoint5 = time.time()\n",
    "        #print(str(checkpoint5-checkpoint4)+\" Stored scores in dictionary\\n\")\n",
    "        \n",
    "        end = time.time()\n",
    "        elapsed = end-start\n",
    "        #print(str(elapsed)+' seconds elapsed for this user\\n\\n')\n",
    "        time_test.append(elapsed)\n",
    "    \n",
    "    print(\"Saving dictionary for this set of users\")\n",
    "    # save dictionary\n",
    "    with open('user_similarities/similarity_storage_1k_'+str(user_block)+'.json', 'w') as convert_file:\n",
    "        convert_file.write(json.dumps(base_users_storage))\n",
    "    \n",
    "    avg_time = mean(time_test)\n",
    "    print(\"Average time per user: \"+str(avg_time))\n",
    "    \n",
    "    del base_users_storage\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d038f44",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_users_storage['cfarrell']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc913a32",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(base_users_storage['Torsten'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d19bc147",
   "metadata": {},
   "source": [
    "# Make User Means Dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5b3245ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Opening JSON file\n",
    "with open('data_cleaned/user_ratings.json') as json_file:\n",
    "    user_ratings = json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "430a9274",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "263503"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(user_ratings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "21e01517",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_means = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f135aa20",
   "metadata": {},
   "outputs": [],
   "source": [
    "for person in user_ratings:\n",
    "    user_items = []\n",
    "    for item in user_ratings[person]:\n",
    "        user_items.append(user_ratings[person][item])\n",
    "    user_mean = round((mean(user_items)), 1)\n",
    "    user_means[person] = user_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "54330a35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.1"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_means['Threnody']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "63312378",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_means['moosh21']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7d998083",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.7"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_means['Shade92008']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "17ed8d87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.8"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_means['Torsten']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c304226b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save dictionary\n",
    "with open('data_cleaned/user_means.json', 'w') as convert_file:\n",
    "    convert_file.write(json.dumps(user_means))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbf3bab8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a743904",
   "metadata": {},
   "outputs": [],
   "source": [
    "del user_means\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "94a06425",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Opening JSON file\n",
    "with open('data_cleaned/user_means.json') as json_file:\n",
    "    user_means_dict = json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0ceeac09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Mean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Torsten</th>\n",
       "      <td>6.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mitnachtKAUBO-I</th>\n",
       "      <td>6.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>avlawn</th>\n",
       "      <td>6.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mike Mayer</th>\n",
       "      <td>6.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mease19</th>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Mean\n",
       "Torsten           6.8\n",
       "mitnachtKAUBO-I   6.6\n",
       "avlawn            6.4\n",
       "Mike Mayer        6.7\n",
       "Mease19           7.0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_means = pd.DataFrame.from_dict(user_means_dict, orient='index')\n",
    "user_means.rename(columns={0:'Mean'}, inplace=True)\n",
    "user_means.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3bd769a",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_means.to_pickle('data_cleaned/user)mea')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f14e0ae0",
   "metadata": {},
   "source": [
    "# Make Ratings Block Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c526947c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Opening JSON file\n",
    "with open('data_cleaned/user_ratings.json') as json_file:\n",
    "    user_ratings = json.load(json_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66a1ddfb",
   "metadata": {},
   "source": [
    "## Make scaled ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "09347736",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_ratings_scaled = {}\n",
    "\n",
    "for person in user_ratings:\n",
    "    user_ratings_scaled[person] = {}\n",
    "    user_mean = mean(user_ratings[person].values())\n",
    "    for item in user_ratings[person]:\n",
    "        new_value = round((user_ratings[person][item] - user_mean), 2)\n",
    "        user_ratings_scaled[person][item] = new_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "90983ab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save dictionary\n",
    "with open('real_ratings/real_user_ratings_dictionary_scaled.json', 'w') as convert_file:\n",
    "    convert_file.write(json.dumps(user_ratings_scaled))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68819867",
   "metadata": {},
   "source": [
    "## Make smaller ratings blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8a4fd98e",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_users = list(user_ratings.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1eac2a65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "263503"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "420130aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_block_1 = all_users[:40000]\n",
    "user_block_2 = all_users[40000:80000]\n",
    "user_block_3 = all_users[80000:120000]\n",
    "user_block_4 = all_users[120000:160000]\n",
    "user_block_5 = all_users[160000:200000]\n",
    "user_block_6 = all_users[200000:240000]\n",
    "user_block_7 = all_users[240000:]\n",
    "\n",
    "user_blocks = [user_block_1, user_block_2, user_block_3, user_block_4, user_block_5, user_block_6, user_block_7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7e284a26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting block 1\n",
      "Starting block 2\n",
      "Starting block 3\n",
      "Starting block 4\n",
      "Starting block 5\n",
      "Starting block 6\n",
      "Starting block 7\n"
     ]
    }
   ],
   "source": [
    "iteration = 0\n",
    "\n",
    "for block in user_blocks:\n",
    "    \n",
    "    iteration += 1\n",
    "    \n",
    "    print(\"Starting block \"+str(iteration))\n",
    "    \n",
    "    block_of_users = {key: value for key, value in user_ratings.items() if key in block}\n",
    "    \n",
    "    #for scaled only:\n",
    "    for person in block_of_users:\n",
    "        user_mean = mean(block_of_users[person].values())\n",
    "        for item in block_of_users[person]:\n",
    "            new_value = round((block_of_users[person][item] - user_mean), 2)\n",
    "            block_of_users[person][item] = new_value\n",
    "    \n",
    "    # save dictionary\n",
    "    with open('data_cleaned/user_ratings_block_scaled_'+str(iteration)+'.json', 'w') as convert_file:\n",
    "        convert_file.write(json.dumps(block_of_users))\n",
    "        \n",
    "    del block_of_users\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "76180a55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del user_blocks\n",
    "del user_ratings\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d28d9172",
   "metadata": {},
   "source": [
    "# Deprecated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ee8c631",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make dataframe from synthetic sort and melt to longform\n",
    "synthetic_user_ratings = pd.DataFrame.from_dict(synthetic_users_dictionary)\n",
    "synthetic_user_ratings.reset_index(inplace=True)\n",
    "synthetic_user_ratings.rename(columns={'index':'BGGId', user:'Rating'}, inplace=True)\n",
    "synthetic_user_ratings['Rating'] = synthetic_user_ratings['Rating']+user_mean\n",
    "    \n",
    "    \n",
    "synthetic_user_ratings = pd.DataFrame.from_dict(synthetic_users_dictionary).T\n",
    "synthetic_user_ratings.reset_index(inplace=True)\n",
    "synthetic_user_ratings.rename(columns={'index':'UserID'}, inplace=True)\n",
    "synthetic_user_ratings_long = synthetic_user_ratings.melt(id_vars='UserID', var_name='BGGId', value_name='Rating').dropna()\n",
    "synthetic_user_ratings_long.sort_values('UserID', inplace=True)\n",
    "synthetic_user_ratings_long\n",
    "    \n",
    "# save longform\n",
    "synthetic_user_ratings_long.to_pickle('synthetic_ratings_new_scraper/synthetic_ratings_'+path+'_'+number+'.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45d17159",
   "metadata": {},
   "outputs": [],
   "source": [
    "def produce_synthetic_ratings(user, temp_users_dictionary, num_ratings_create):\n",
    "    '''\n",
    "    Takes in a dictionary of user's ratings and the number of ratings to synthesize\n",
    "    Synthesizes ratings and creates a dictionary of all synthesized ratings for the user\n",
    "    Returns synthesized ratings\n",
    "    \n",
    "    Inputs:\n",
    "    user: the user id to create ratings for\n",
    "    temp_users_dictionary: dictionary of specific user's real ratings\n",
    "    num_ratings_create : simple number. # Ratings to make in the run.\n",
    "    \n",
    "    Outputs:\n",
    "    user_comps_dict : dictionary of synthesized ratings specifically for user\n",
    "    '''\n",
    "    \n",
    "    print(\"Producing items for user\")\n",
    "    \n",
    "    # start at iteration 0\n",
    "    iteration = 0\n",
    "    \n",
    "    # set up dict to store all specific comps for this user\n",
    "    users_comp_dict = {}\n",
    "\n",
    "    # populate the comps with the user's baseline items\n",
    "    for item in temp_users_dictionary:  \n",
    "        users_comp_dict[item] = [1, 1, item, 0, 0, temp_users_dictionary[item]]\n",
    "        #overall confidence, this item similarity, item, iteration, degrees away, item name\n",
    "       \n",
    "    # while the list of items that the user rated is < the number of ratings needed:\n",
    "    while len(users_comp_dict.keys()) < num_ratings_create:\n",
    "        \n",
    "        users_rated_items = list(temp_users_dictionary.keys())\n",
    "        \n",
    "        iteration += 1 # advance the iteration\n",
    "        \n",
    "        new_items = [] # make a list to hold the items for this iteration        \n",
    "        \n",
    "        # for each rated item:\n",
    "        for rated in users_rated_items:\n",
    "            \n",
    "            print(\"\\nCurrent item: \"+str(rated))\n",
    "            # get rating for current item\n",
    "            rated_rating = temp_users_dictionary[rated]\n",
    "            print(rated_rating)\n",
    "        \n",
    "            # get current best comp:\n",
    "            current_position = 0\n",
    "            current_comp = game_comps_byid_lookup[rated][0][current_position]\n",
    "            \n",
    "            while current_comp in new_items:\n",
    "                \n",
    "                # increment position\n",
    "                current_position+=1 \n",
    "                \n",
    "                if current_position >= 21923:\n",
    "                    #print(current_position)\n",
    "                    break\n",
    "                                                        \n",
    "                else:\n",
    "                    # reset current comp to new position new_items\n",
    "                    current_comp = game_comps_byid_lookup[rated][0][current_position]\n",
    "\n",
    "                    # continue back to check\n",
    "                    continue\n",
    "            \n",
    "            # any time the current comp is in users_rated_items already:\n",
    "            while current_comp in users_comp_dict.keys():\n",
    "                \n",
    "                # increment position\n",
    "                current_position+=1 \n",
    "                \n",
    "                if current_position >= 21923:\n",
    "                    #print(current_position)\n",
    "                    break\n",
    "                                    \n",
    "                else:\n",
    "                \n",
    "                    # reset current comp to new position users_comp_dict\n",
    "                    current_comp = game_comps_byid_lookup[rated][0][current_position]\n",
    "\n",
    "                    # continue back to check\n",
    "                    continue\n",
    "            \n",
    "            # The next section activates once the current comp is not already in the user's rated items\n",
    "            \n",
    "            if current_position >= 21923:\n",
    "                #print(current_position)\n",
    "                break\n",
    "                            \n",
    "            else:\n",
    "            \n",
    "            \n",
    "                # getting similarity of the current comp\n",
    "                comp_similarity = game_comps_byid_lookup[rated][1][current_position]\n",
    "                print(current_position)\n",
    "                print(comp_similarity)\n",
    "              \n",
    "                # get the synthetic rating for the item by taking the rating of the base item * similarity\n",
    "                synthetic_rating = rated_rating * comp_similarity\n",
    "                print(synthetic_rating)\n",
    "                \n",
    "                # get the overall confidence of this rating \n",
    "                # confidence = confidence of prior item * similarity of current item\n",
    "                confidence = users_comp_dict[rated][0] * comp_similarity\n",
    "                degrees = users_comp_dict[rated][4] + 1\n",
    "\n",
    "                # add this item to the list of new items we are adding to the ratings this round\n",
    "                new_items.append(current_comp)\n",
    "            \n",
    "                # make the user's comp dict\n",
    "                users_comp_dict[current_comp] = [confidence, comp_similarity, rated, iteration, degrees, synthetic_rating]\n",
    "            \n",
    "                # update the temporary dictionary with the synthetic rating for the item\n",
    "                temp_users_dictionary[current_comp] = synthetic_rating\n",
    "        \n",
    "        continue\n",
    "\n",
    "    print(\"End length of rated items is \"+str(len(users_comp_dict))+'\\n')\n",
    "\n",
    "    return users_comp_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10cf1052",
   "metadata": {},
   "outputs": [],
   "source": [
    "#user_matrix = pd.read_pickle('data_cleaned/ratings_matrix_cleaned_03.pkl')\n",
    "#user_matrix = user_matrix.T\n",
    "#user_matrix.index = user_matrix.index.astype('int32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5554cf0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# run the data synthesizer for each of the 6 ratings matrix files\n",
    "process_to_synthetic(item, num_ratings_create, desired_ratings, game_ids, '250')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb2bfc33",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_user(user_items, user, game_ids):\n",
    "    '''\n",
    "    Takes in user's rated items, a the username, and a list of game_ids\n",
    "    Get the mean for the user\n",
    "    Builds a list of user's rated items and subtracts user mean from all ratings\n",
    "    Builds a corresponding list of game ids for the rated games\n",
    "    Gets intersection of user's rated ids with the overall game_ids\n",
    "    Stores user game_id:rating in user ratings dictionary \n",
    "    Returns the user dictionary\n",
    "    \n",
    "    Inputs: \n",
    "    user_items: dataframe column of user's rated items\n",
    "    user: user to retrieve\n",
    "    game_ids: the game_ids we are using in our recommender\n",
    "    \n",
    "    Outputs:\n",
    "    overall_user: user dictionary with user's ratings\n",
    "    '''\n",
    "    \n",
    "    # get the mean rating for that user\n",
    "    user_mean = user_items.mean()\n",
    "    \n",
    "    # normalize the ratings for that user by subtracting their mean from all ratings, store in list\n",
    "    game_ratings_normed =  list(user_items - user_mean)\n",
    "    \n",
    "    # Get a list of all of the game IDs that the user rated\n",
    "    users_game_ids = list(user_items.index)\n",
    "    \n",
    "    # get the set of usable game ids\n",
    "    game_ids_set = set(game_ids).intersection(set(users_game_ids))\n",
    "    \n",
    "    # make user storage dictionary\n",
    "    user_ratings = {}\n",
    "    \n",
    "    # for the key/value pairs of game_ids and normalized ratings\n",
    "    for key, value in zip(users_game_ids, game_ratings_normed):\n",
    "        user_ratings[key] = value\n",
    "    \n",
    "    # make a dictionary to store the intersected ratings\n",
    "    set_dictionary = {}\n",
    "    \n",
    "    # for each matching key, value in game_ids and game_ratings for the user\n",
    "    for item in game_ids_set:\n",
    "        set_dictionary[item] = user_ratings[item]\n",
    "\n",
    "    # store the user's ratings\n",
    "    overall_user = set_dictionary\n",
    "    \n",
    "    return overall_user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "168fd9a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_matrix_to_synthetic(path, num_ratings_create, desired_ratings, game_ids, number):\n",
    "    '''\n",
    "    Process a user matrix and create synthetic data for each user in the matrix\n",
    "    \n",
    "    Inputs:\n",
    "    Path: path appendation for file\n",
    "    num_ratings_create: The total number of minimum ratings per user\n",
    "    desired_ratings: the needed number of ratings per user\n",
    "    '''\n",
    "    \n",
    "    # load and transpose data frame\n",
    "    user_matrix = pd.read_pickle('data_cleaned/ratings_matrix_cleaned_'+path+'.pkl')\n",
    "    user_matrix.drop_duplicates(keep='first', inplace=True)\n",
    "    user_matrix = user_matrix.T\n",
    "    user_matrix.index = user_matrix.index.astype('int32')\n",
    "    \n",
    "    # set up a synthetic ratings dictionary to store the users and ratings\n",
    "    synthetic_users_dictionary = {}\n",
    "\n",
    "    # for each user in the test matrix:\n",
    "    for user in user_matrix.columns:\n",
    "   \n",
    "        print(\"Starting user \"+user)\n",
    "        \n",
    "        user_items = user_matrix[user].dropna(axis=0)\n",
    "        \n",
    "        # copy the current user dictionary to a temp storage dictionary that we can manipulate\n",
    "        synthetic_users_dictionary[user] = get_user(user_items, user, game_ids)\n",
    "        temp_users_dictionary = copy.deepcopy(synthetic_users_dictionary[user])\n",
    "    \n",
    "        # get the original number of ratings by this user\n",
    "        original_num_ratings = len(temp_users_dictionary)\n",
    "        print(\"User starts with \"+str(original_num_ratings)+\" ratings\")\n",
    "    \n",
    "        # call function to produce synthetic ratings\n",
    "        user_comps_dict = produce_synthetic_ratings(user, temp_users_dictionary, num_ratings_create)\n",
    "        # call sort function for top synthetic ratings\n",
    "        sort_synthetic_ratings(user, synthetic_users_dictionary, user_comps_dict, original_num_ratings, desired_ratings)\n",
    "    \n",
    "    # make dataframe from synthetic sort and melt to longform\n",
    "    synthetic_user_ratings = pd.DataFrame.from_dict(synthetic_users_dictionary).T\n",
    "    synthetic_user_ratings.reset_index(inplace=True)\n",
    "    synthetic_user_ratings.rename(columns={'index':'UserID'}, inplace=True)\n",
    "    synthetic_user_ratings_long = synthetic_user_ratings.melt(id_vars='UserID', var_name='BGGId', value_name='Rating').dropna()\n",
    "    synthetic_user_ratings_long.sort_values('UserID', inplace=True)\n",
    "    synthetic_user_ratings_long\n",
    "    \n",
    "    # save longform\n",
    "    synthetic_user_ratings_long.to_pickle('synthetic_ratings_new_scraper/synthetic_ratings_'+path+'_'+number+'.pkl')\n",
    "    \n",
    "    # save dictionary\n",
    "    with open('synthetic_ratings_new_scraper/users_dump_syntheticratings'+path+'_'+number+'.json', 'w') as convert_file:\n",
    "        convert_file.write(json.dumps(synthetic_users_dictionary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c86f2ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_synthetic_ratings(user, synthetic_users_dictionary, user_comps_dict, original_num_ratings, desired_ratings):\n",
    "    '''\n",
    "    Takes the user's synthesized comps dict, the original number of ratings the user made, \n",
    "    and the desired number of ratings the user needs.\n",
    "    Creates a df sorting the synthesized ratings by confidence level, \n",
    "    keeping the highest confidence if an item was recommended more than once.\n",
    "    Evaluates number of ratings needed to reach 500 and keeps only that many ratings with the highest confidence.\n",
    "    For each item kept, logs the synthetic rating to the user;s dictionary\n",
    "    \n",
    "    Inputs:\n",
    "    user: specific user to sort\n",
    "    synthetic_users_dictionary: reference to the dictionary of synthesized items\n",
    "    user_comps_dict: dictionary of synthesized ratings specifically for user\n",
    "    original_num_ratings: The number of ratings the user actually rated\n",
    "    desired_ratings: the number of ratings needed by the user\n",
    "    \n",
    "    '''\n",
    "    print(\"Sorting user items\")\n",
    "    \n",
    "    # showing synthetic ratings only\n",
    "    user_comps_df = pd.DataFrame(user_comps_dict.values(), index=user_comps_dict.keys(), columns=['OverallConfidence', 'SimtoLast', 'RecFrom', 'Iteration', 'DegreesAway', 'SyntheticRating']).sort_values('OverallConfidence', ascending=False).drop_duplicates(keep='first')\n",
    "    \n",
    "    # get a list of the ratings to keep (past the real ratings)\n",
    "    keep_items = sorted(list(user_comps_df[:desired_ratings].index))\n",
    "\n",
    "    # for each item that we keep,\n",
    "    for item in keep_items:\n",
    "    \n",
    "        # add the rating to the real storage dictionary\n",
    "        synthetic_users_dictionary[user][item] = user_comps_dict[item]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f791e0b",
   "metadata": {},
   "source": [
    "## Old style user data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cb4990c",
   "metadata": {},
   "source": [
    "### Test One User"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "036ccda5",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_matrix = pd.read_pickle('data_cleaned/ratings_matrix_cleaned_03.pkl')\n",
    "user_matrix = user_matrix.T\n",
    "user_matrix.index = user_matrix.index.astype('int32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a88afe8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "user = 'Monika1234'\n",
    "user_mean = users_means[user]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55a53f4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_items = user_matrix[user].dropna(axis=0)\n",
    "user_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52103bbc",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "this_user = pd.DataFrame(user_matrix[user].dropna(axis=0))\n",
    "this_user.rename(columns={user:'Rating'}, inplace=True)\n",
    "this_user.reset_index(inplace=True)\n",
    "this_user['Game'] = this_user['index'].astype('int32').map(game_id_lookup)\n",
    "#this_user.drop('index', axis=1, inplace=True)\n",
    "this_user.sort_values('Game', ascending=True).head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c86a8cf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "game_comps_byid_lookup[298352][0][21923]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52184c18",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# set up a synthetic ratings dictionary to store the users and ratings\n",
    "synthetic_users_dictionary = {}\n",
    "\n",
    "temp_users_dictionary = {}\n",
    "    \n",
    "print(\"Starting user \"+user)\n",
    "\n",
    "user_items = user_matrix[user].dropna(axis=0)\n",
    "\n",
    "# copy the current user dictionary to a temp storage dictionary that we can manipulate\n",
    "synthetic_users_dictionary[user] = get_user(user_items, user, game_ids)\n",
    "temp_users_dictionary = copy.deepcopy(synthetic_users_dictionary[user])\n",
    "    \n",
    "# get the original number of ratings by this user\n",
    "original_num_ratings = len(temp_users_dictionary)\n",
    "\n",
    "    \n",
    "# call function to produce synthetic ratings\n",
    "user_comps_dict = produce_synthetic_ratings_all(user, temp_users_dictionary, num_ratings_create) \n",
    "    \n",
    "sort_synthetic_ratings(user, synthetic_users_dictionary, user_comps_dict, original_num_ratings, desired_ratings)\n",
    "\n",
    "synthetic_user_ratings = pd.DataFrame.from_dict(synthetic_users_dictionary)\n",
    "synthetic_user_ratings.reset_index(inplace=True)\n",
    "synthetic_user_ratings.rename(columns={'index':'BGGId', user:'Rating'}, inplace=True)\n",
    "synthetic_user_ratings['Rating'] = synthetic_user_ratings['Rating']+user_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03e06cd7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "temp2 = pd.DataFrame(synthetic_users_dictionary[user].values(), index=synthetic_users_dictionary[user].keys())\n",
    "temp2['Game'] = temp2.index.map(game_id_lookup)\n",
    "temp2['Rating'] = temp2[0]+user_mean\n",
    "temp2.reset_index(inplace=True)\n",
    "temp2.drop(['index', 0], axis=1, inplace=True)\n",
    "temp2.sort_values('Rating', ascending=False).head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8d0eeb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_comps_df = pd.DataFrame(user_comps_dict.values(), index=user_comps_dict.keys(), columns=['OverallConfidence', 'SimtoLast', 'RecFrom', 'DegreesAway', 'SyntheticRating']).sort_values('OverallConfidence', ascending=False).drop_duplicates(keep='first')\n",
    "user_comps_df['SyntheticRating'] = user_comps_df['SyntheticRating']+user_mean\n",
    "user_comps_df['RecommendedItem'] = user_comps_df.index.map(game_id_lookup)\n",
    "user_comps_df['Seed'] = user_comps_df['RecFrom'].map(game_id_lookup)\n",
    "user_comps_df.sort_values('SyntheticRating', ascending=False).head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "721349b5",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(20,10))\n",
    "\n",
    "sns.set(font_scale = 1.5) # set our font scale bigger for this vis\n",
    "\n",
    "# scatter our data\n",
    "sns.set_style('darkgrid')\n",
    "scatter2 = sns.scatterplot(x=\"DegreesAway\", y='SyntheticRating', data=user_comps_df, \n",
    "                           hue='DegreesAway', palette='viridis', s=100)\n",
    "ax.axhline(user_mean)\n",
    "ax.text(x=.5, y=(user_mean+.2), s='User Mean '+str(user_mean), alpha=0.7, color='black')\n",
    "\n",
    "ax.get_legend().remove()\n",
    "\n",
    "plt.title(str(desired_ratings)+\" Synthetic Ratings for a 10-Rating User\", fontsize=30)\n",
    "plt.xlabel(\"Steps Away from True Rating\", fontsize=20)\n",
    "plt.ylabel(\"Rating\", fontsize=20)\n",
    "\n",
    "\n",
    "plt.tight_layout\n",
    "#plt.savefig('images/synthetic_from10.png')\n",
    "plt.show()\n",
    ";"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc1462b0",
   "metadata": {},
   "source": [
    "### Test One User"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3b10265",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_matrix = pd.read_pickle('data_cleaned/ratings_matrix_cleaned_06.pkl')\n",
    "user_matrix = user_matrix.T\n",
    "user_matrix.index = user_matrix.index.astype('int32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bdee3c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "user = 'zusterdoor'\n",
    "user_mean = users_means[user]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb5e0746",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_items = user_matrix[user].dropna(axis=0)\n",
    "user_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b43c7f5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up a synthetic ratings dictionary to store the users and ratings\n",
    "synthetic_users_dictionary = {}\n",
    "\n",
    "temp_users_dictionary = {}\n",
    "    \n",
    "print(\"Starting user \"+user)\n",
    "\n",
    "user_items = user_matrix[user].dropna(axis=0)\n",
    "\n",
    "# copy the current user dictionary to a temp storage dictionary that we can manipulate\n",
    "synthetic_users_dictionary[user] = get_user(user_items, user, game_ids)\n",
    "temp_users_dictionary = copy.deepcopy(synthetic_users_dictionary[user])\n",
    "    \n",
    "# get the original number of ratings by this user\n",
    "original_num_ratings = len(temp_users_dictionary)\n",
    "\n",
    "    \n",
    "# call function to produce synthetic ratings\n",
    "user_comps_dict = produce_synthetic_ratings(user, temp_users_dictionary, num_ratings_create) \n",
    "    \n",
    "sort_synthetic_ratings(user, synthetic_users_dictionary, user_comps_dict, original_num_ratings, desired_ratings)\n",
    "\n",
    "synthetic_user_ratings = pd.DataFrame.from_dict(synthetic_users_dictionary)\n",
    "synthetic_user_ratings.reset_index(inplace=True)\n",
    "synthetic_user_ratings.rename(columns={'index':'BGGId', user:'Rating'}, inplace=True)\n",
    "synthetic_user_ratings['Rating'] = synthetic_user_ratings['Rating']+user_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1963f23c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "temp2 = pd.DataFrame(synthetic_users_dictionary[user].values(), index=synthetic_users_dictionary[user].keys())\n",
    "temp2['Game'] = temp2.index.map(game_id_lookup)\n",
    "temp2['Rating'] = temp2[0]+user_mean\n",
    "temp2.reset_index(inplace=True)\n",
    "temp2.drop(['index', 0], axis=1, inplace=True)\n",
    "temp2.sort_values('Rating', ascending=False).head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d5ccd84",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_comps_df = pd.DataFrame(user_comps_dict.values(), index=user_comps_dict.keys(), columns=['OverallConfidence', 'SimtoLast', 'RecFrom', 'DegreesAway', 'SyntheticRating']).sort_values('OverallConfidence', ascending=False).drop_duplicates(keep='first')\n",
    "user_comps_df['SyntheticRating'] = user_comps_df['SyntheticRating']+user_mean\n",
    "user_comps_df['RecommendedItem'] = user_comps_df.index.map(game_id_lookup)\n",
    "user_comps_df['Seed'] = user_comps_df['RecFrom'].map(game_id_lookup)\n",
    "user_comps_df.sort_values('SyntheticRating', ascending=False).head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a39de3c5",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(20,10))\n",
    "\n",
    "sns.set(font_scale = 2) # set our font scale bigger for this vis\n",
    "\n",
    "# scatter our data\n",
    "sns.set_style('darkgrid')\n",
    "scatter2 = sns.scatterplot(x=\"DegreesAway\", y='SyntheticRating', data=user_comps_df, \n",
    "                           hue='DegreesAway', palette='viridis', s=100)\n",
    "ax.axhline(user_mean)\n",
    "ax.text(x=.2, y=8.1, s='User Mean '+str(user_mean), alpha=0.7, color='black')\n",
    "\n",
    "ax.get_legend().remove()\n",
    "\n",
    "plt.title(\"100 Synthetic Ratings for a 5-Rating User\", fontsize=30)\n",
    "plt.xlabel(\"Steps Away from True Rating\", fontsize=24)\n",
    "plt.ylabel(\"Rating\", fontsize=24)\n",
    "\n",
    "\n",
    "plt.tight_layout\n",
    "#plt.savefig('images/synthetic_from_05.png')\n",
    "plt.show()\n",
    ";"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a7ccfcc",
   "metadata": {},
   "source": [
    "### Test One User"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef547821",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_matrix = pd.read_pickle('data_cleaned/ratings_matrix_cleaned_03.pkl')\n",
    "user_matrix = user_matrix.T\n",
    "user_matrix.index = user_matrix.index.astype('int32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c483d236",
   "metadata": {},
   "outputs": [],
   "source": [
    "user = 'Szczurek83'\n",
    "user_mean = users_means[user]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c44251ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_items = user_matrix[user].dropna(axis=0)\n",
    "user_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51740036",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up a synthetic ratings dictionary to store the users and ratings\n",
    "synthetic_users_dictionary = {}\n",
    "\n",
    "temp_users_dictionary = {}\n",
    "    \n",
    "print(\"Starting user \"+user)\n",
    "\n",
    "user_items = user_matrix[user].dropna(axis=0)\n",
    "\n",
    "# copy the current user dictionary to a temp storage dictionary that we can manipulate\n",
    "synthetic_users_dictionary[user] = get_user(user_items, user, game_ids)\n",
    "temp_users_dictionary = copy.deepcopy(synthetic_users_dictionary[user])\n",
    "    \n",
    "# get the original number of ratings by this user\n",
    "original_num_ratings = len(temp_users_dictionary)\n",
    "\n",
    "    \n",
    "# call function to produce synthetic ratings\n",
    "user_comps_dict = produce_synthetic_ratings(user, temp_users_dictionary, num_ratings_create) \n",
    "    \n",
    "sort_synthetic_ratings(user, synthetic_users_dictionary, user_comps_dict, original_num_ratings, desired_ratings)\n",
    "\n",
    "synthetic_user_ratings = pd.DataFrame.from_dict(synthetic_users_dictionary)\n",
    "synthetic_user_ratings.reset_index(inplace=True)\n",
    "synthetic_user_ratings.rename(columns={'index':'BGGId', user:'Rating'}, inplace=True)\n",
    "synthetic_user_ratings['Rating'] = synthetic_user_ratings['Rating']+user_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb4afb5b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "temp2 = pd.DataFrame(synthetic_users_dictionary[user].values(), index=synthetic_users_dictionary[user].keys())\n",
    "temp2['Game'] = temp2.index.map(game_id_lookup)\n",
    "temp2['Rating'] = temp2[0]+user_mean\n",
    "temp2.reset_index(inplace=True)\n",
    "temp2.drop(['index', 0], axis=1, inplace=True)\n",
    "temp2.sort_values('Rating', ascending=False).head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1ba98ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp2.to_pickle('scaled_content_filter.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a596daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_comps_df = pd.DataFrame(user_comps_dict.values(), index=user_comps_dict.keys(), columns=['OverallConfidence', 'SimtoLast', 'RecFrom', 'DegreesAway', 'SyntheticRating']).sort_values('OverallConfidence', ascending=False).drop_duplicates(keep='first')\n",
    "user_comps_df['SyntheticRating'] = user_comps_df['SyntheticRating']+user_mean\n",
    "user_comps_df['RecommendedItem'] = user_comps_df.index.map(game_id_lookup)\n",
    "user_comps_df['Seed'] = user_comps_df['RecFrom'].map(game_id_lookup)\n",
    "user_comps_df.sort_values('SyntheticRating', ascending=False).head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95f28b78",
   "metadata": {},
   "source": [
    "## Notebook Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a7dc1d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_user(user_items, user, game_ids):\n",
    "    '''\n",
    "    Takes in user's rated items, a the username, and a list of game_ids\n",
    "    Get the mean for the user\n",
    "    Builds a list of user's rated items and subtracts user mean from all ratings\n",
    "    Builds a corresponding list of game ids for the rated games\n",
    "    Gets intersection of user's rated ids with the overall game_ids\n",
    "    Stores user game_id:rating in user ratings dictionary \n",
    "    Returns the user dictionary\n",
    "    \n",
    "    Inputs: \n",
    "    user_items: dataframe column of user's rated items\n",
    "    user: user to retrieve\n",
    "    game_ids: the game_ids we are using in our recommender\n",
    "    \n",
    "    Outputs:\n",
    "    overall_user: user dictionary with user's ratings\n",
    "    '''\n",
    "    \n",
    "    # get the mean rating for that user\n",
    "    user_mean = user_items.mean()\n",
    "    \n",
    "    # normalize the ratings for that user by subtracting their mean from all ratings, store in list\n",
    "    game_ratings_normed =  list(user_items - user_mean)\n",
    "    \n",
    "    # Get a list of all of the game IDs that the user rated\n",
    "    users_game_ids = list(user_items.index)\n",
    "    \n",
    "    # get the set of usable game ids\n",
    "    game_ids_set = set(game_ids).intersection(set(users_game_ids))\n",
    "    \n",
    "    # make user storage dictionary\n",
    "    user_ratings = {}\n",
    "    \n",
    "    # for the key/value pairs of game_ids and normalized ratings\n",
    "    for key, value in zip(users_game_ids, game_ratings_normed):\n",
    "        user_ratings[key] = value\n",
    "    \n",
    "    # make a dictionary to store the intersected ratings\n",
    "    set_dictionary = {}\n",
    "    \n",
    "    # for each matching key, value in game_ids and game_ratings for the user\n",
    "    for item in game_ids_set:\n",
    "        set_dictionary[item] = user_ratings[item]\n",
    "\n",
    "    # store the user's ratings\n",
    "    overall_user = set_dictionary\n",
    "    \n",
    "    return overall_user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a24f4cbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def produce_synthetic_ratings_all(user, temp_users_dictionary, num_ratings_create):\n",
    "    '''\n",
    "    Takes in a dictionary of user's ratings and the number of ratings to synthesize\n",
    "    Synthesizes ratings and creates a dictionary of all synthesized ratings for the user\n",
    "    Returns synthesized ratings\n",
    "    \n",
    "    Inputs:\n",
    "    user: the user id to create ratings for\n",
    "    temp_users_dictionary: dictionary of specific user's real ratings\n",
    "    num_ratings_create : simple number. # Ratings to make in the run.\n",
    "    \n",
    "    Outputs:\n",
    "    user_comps_dict : dictionary of synthesized ratings specifically for user\n",
    "    '''\n",
    "    # start at iteration 0\n",
    "    iteration = 0\n",
    "    \n",
    "    # set up dict to store all specific comps for this user\n",
    "    users_comp_dict = {}\n",
    "\n",
    "    # populate the comps with the user's baseline items\n",
    "    for item in temp_users_dictionary:  \n",
    "        users_comp_dict[item] = [1, 1, item, 0, temp_users_dictionary[item]]\n",
    "       \n",
    "    # while the list of items that the user rated is < the number of ratings needed:\n",
    "    while len(users_comp_dict.keys()) < num_ratings_create:\n",
    "        \n",
    "        users_rated_items = list(temp_users_dictionary.keys())\n",
    "        \n",
    "        iteration += 1 # advance the iteration\n",
    "        \n",
    "        new_items = [] # make a list to hold the items for this iteration        \n",
    "        \n",
    "        # for each rated item:\n",
    "        for rated in users_rated_items:\n",
    "            \n",
    "            print(\"Current item: \"+str(rated))\n",
    "            # get rating for current item\n",
    "            rated_rating = temp_users_dictionary[rated]\n",
    "        \n",
    "            # get current best comp:\n",
    "            current_position = 0\n",
    "            current_comp = game_comps_byid_lookup[rated][0][current_position]\n",
    "            \n",
    "            while current_comp in new_items:\n",
    "                \n",
    "                # increment position\n",
    "                current_position+=1 \n",
    "                \n",
    "                if current_position >= 21923:\n",
    "                    print(current_position)\n",
    "                                                        \n",
    "                else:\n",
    "                    # reset current comp to new position\n",
    "                    current_comp = game_comps_byid_lookup[rated][0][current_position]\n",
    "\n",
    "                    # continue back to check\n",
    "                    continue\n",
    "            \n",
    "            # any time the current comp is in users_rated_items already:\n",
    "            while current_comp in users_comp_dict.keys():\n",
    "                \n",
    "                # increment position\n",
    "                current_position+=1 \n",
    "                \n",
    "                if current_position >= 21923:\n",
    "                    print(current_position)\n",
    "                                    \n",
    "                else:\n",
    "                \n",
    "                    # reset current comp to new position\n",
    "                    current_comp = game_comps_byid_lookup[rated][0][current_position]\n",
    "\n",
    "                    # continue back to check\n",
    "                    continue\n",
    "            \n",
    "            # The next section activates once the current comp is not already in the user's rated items\n",
    "            \n",
    "            if current_position >= 21923:\n",
    "                print(current_position)\n",
    "                            \n",
    "            else:\n",
    "            \n",
    "            \n",
    "                # getting similarity of the current comp\n",
    "                comp_similarity = game_comps_byid_lookup[rated][1][current_position]\n",
    "              \n",
    "                # get the synthetic rating for the item by taking the rating of the base item * similarity\n",
    "                synthetic_rating = rated_rating * comp_similarity\n",
    "        \n",
    "                # get the overall confidence of this rating \n",
    "                # confidence = confidence of prior item * similarity of current item\n",
    "                confidence = users_comp_dict[rated][0] * comp_similarity\n",
    "\n",
    "                # add this item to the list of new items we are adding to the ratings this round\n",
    "                new_items.append(current_comp)\n",
    "            \n",
    "                # make the user's comp dict\n",
    "                users_comp_dict[current_comp] = [confidence, comp_similarity, rated, iteration, synthetic_rating]\n",
    "            \n",
    "                # update the temporary dictionary with the synthetic rating for the item\n",
    "                temp_users_dictionary[current_comp] = synthetic_rating\n",
    "        \n",
    "        continue\n",
    "\n",
    "    print(\"End length of rated items is \"+str(len(users_comp_dict))+'\\n')\n",
    "\n",
    "    return users_comp_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "589c969e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_synthetic_ratings(user, synthetic_users_dictionary, user_comps_dict, original_num_ratings, desired_ratings):\n",
    "    '''\n",
    "    Takes the user's synthesized comps dict, the original number of ratings the user made, \n",
    "    and the desired number of ratings the user needs.\n",
    "    Creates a df sorting the synthesized ratings by confidence level, \n",
    "    keeping the highest confidence if an item was recommended more than once.\n",
    "    Evaluates number of ratings needed to reach 500 and keeps only that many ratings with the highest confidence.\n",
    "    For each item kept, logs the synthetic rating to the user;s dictionary\n",
    "    \n",
    "    Inputs:\n",
    "    user: specific user to sort\n",
    "    synthetic_users_dictionary: reference to the dictionary of synthesized items\n",
    "    user_comps_dict: dictionary of synthesized ratings specifically for user\n",
    "    original_num_ratings: The number of ratings the user actually rated\n",
    "    desired_ratings: the number of ratings needed by the user\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    # showing synthetic ratings only\n",
    "    user_comps_df = pd.DataFrame(user_comps_dict.values(), index=user_comps_dict.keys(), columns=['OverallConfidence', 'SimtoLast', 'RecFrom', 'DegreesAway', 'SyntheticRating']).sort_values('OverallConfidence', ascending=False).drop_duplicates(keep='first')\n",
    "    \n",
    "    # get a list of the ratings to keep (past the real ratings)\n",
    "    keep_items = list(user_comps_df[original_num_ratings:desired_ratings].index)\n",
    "\n",
    "    # for each item that we keep,\n",
    "    for item in keep_items:\n",
    "    \n",
    "        # add the rating to the real storage dictionary\n",
    "        synthetic_users_dictionary[user][item] = user_comps_df.loc[item]['SyntheticRating']\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07d7bb62",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_matrix_to_synthetic(path, num_ratings_create, desired_ratings, game_ids, number):\n",
    "    '''\n",
    "    Process a user matrix and create synthetic data for each user in the matrix\n",
    "    \n",
    "    Inputs:\n",
    "    Path: path appendation for file\n",
    "    num_ratings_create: The total number of minimum ratings per user\n",
    "    desired_ratings: the needed number of ratings per user\n",
    "    '''\n",
    "    \n",
    "    # load and transpose data frame\n",
    "    user_matrix = pd.read_pickle('data_cleaned/ratings_matrix_cleaned_'+path+'.pkl')\n",
    "    user_matrix.drop_duplicates(keep='first', inplace=True)\n",
    "    user_matrix = user_matrix.T\n",
    "    user_matrix.index = user_matrix.index.astype('int32')\n",
    "    \n",
    "    # set up a synthetic ratings dictionary to store the users and ratings\n",
    "    synthetic_users_dictionary = {}\n",
    "\n",
    "    # for each user in the test matrix:\n",
    "    for user in user_matrix.columns:\n",
    "   \n",
    "        print(\"Starting user \"+user)\n",
    "        \n",
    "        user_items = user_matrix[user].dropna(axis=0)\n",
    "        \n",
    "        # copy the current user dictionary to a temp storage dictionary that we can manipulate\n",
    "        synthetic_users_dictionary[user] = get_user(user_items, user, game_ids)\n",
    "        temp_users_dictionary = copy.deepcopy(synthetic_users_dictionary[user])\n",
    "    \n",
    "        # get the original number of ratings by this user\n",
    "        original_num_ratings = len(temp_users_dictionary)\n",
    "        print(\"User starts with \"+str(original_num_ratings)+\" ratings\")\n",
    "    \n",
    "        # call function to produce synthetic ratings\n",
    "        user_comps_dict = produce_synthetic_ratings(user, temp_users_dictionary, num_ratings_create)\n",
    "        # call sort function for top synthetic ratings\n",
    "        sort_synthetic_ratings(user, synthetic_users_dictionary, user_comps_dict, original_num_ratings, desired_ratings)\n",
    "    \n",
    "    # make dataframe from synthetic sort and melt to longform\n",
    "    synthetic_user_ratings = pd.DataFrame.from_dict(synthetic_users_dictionary).T\n",
    "    synthetic_user_ratings.reset_index(inplace=True)\n",
    "    synthetic_user_ratings.rename(columns={'index':'UserID'}, inplace=True)\n",
    "    synthetic_user_ratings_long = synthetic_user_ratings.melt(id_vars='UserID', var_name='BGGId', value_name='Rating').dropna()\n",
    "    synthetic_user_ratings_long.sort_values('UserID', inplace=True)\n",
    "    synthetic_user_ratings_long\n",
    "    \n",
    "    # save longform\n",
    "    synthetic_user_ratings_long.to_pickle('synthetic_ratings_new_scraper/synthetic_ratings_'+path+'_'+number+'.pkl')\n",
    "    \n",
    "    # save dictionary\n",
    "    with open('synthetic_ratings_new_scraper/users_dump_syntheticratings'+path+'_'+number+'.json', 'w') as convert_file:\n",
    "        convert_file.write(json.dumps(synthetic_users_dictionary))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a861416",
   "metadata": {},
   "source": [
    "## Deprecated Matrix Calculations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9500f98",
   "metadata": {},
   "source": [
    "### Using Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73f4413c",
   "metadata": {},
   "outputs": [],
   "source": [
    "larger_matrix = pd.read_pickle('synthetic_ratings/users_synthetic_2193_fullmatrix.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf518050",
   "metadata": {},
   "outputs": [],
   "source": [
    "larger_matrix.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecc9d5ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "users = list(larger_matrix.index)\n",
    "users[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c14e553",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_lookup_table = {}\n",
    "\n",
    "user_key = -1\n",
    "\n",
    "for user in users:\n",
    "    \n",
    "    user_key += 1\n",
    "    \n",
    "    user_lookup_table[int(user_key)] = users[user_key]\n",
    "\n",
    "# save dictionary\n",
    "with open('user_lookup_table.json', 'w') as convert_file:\n",
    "    convert_file.write(json.dumps(user_lookup_table))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa11f2ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "%time values1 = larger_matrix.loc['Torsten'].values\n",
    "values1[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9748a391",
   "metadata": {},
   "outputs": [],
   "source": [
    "%time values2 = larger_matrix.loc['mitnachtKAUBO-I'].values\n",
    "values2[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b468c7de",
   "metadata": {},
   "outputs": [],
   "source": [
    "%time spatial.distance.cosine(values1,values2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b68c5b74",
   "metadata": {},
   "outputs": [],
   "source": [
    "del similarity_dictionary\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e40f097c",
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_dictionary = {}\n",
    "\n",
    "for user in users:\n",
    "    \n",
    "    similarity_dictionary[user] = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e60af8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for user in users[:1]:\n",
    "    \n",
    "    start = time.time()\n",
    "    user_values = larger_matrix.loc[user].values\n",
    "    \n",
    "    for other_user in users:\n",
    "        \n",
    "        if user in similarity_dictionary[other_user]:\n",
    "            continue\n",
    "        \n",
    "        else:\n",
    "        \n",
    "            other_user_values = larger_matrix.loc[other_user].values\n",
    "            similarity = 1 - spatial.distance.cosine(user_values,other_user_values)\n",
    "            similarity_dictionary[user][other_user] = similarity\n",
    "            similarity_dictionary[other_user][user] = similarity\n",
    "    \n",
    "    end = time.time()\n",
    "    print(str(end-start)+' seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84f603a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "del larger_matrix\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da435220",
   "metadata": {},
   "source": [
    "### Using Numpy Arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eac0ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix_array = larger_matrix.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7e32a13",
   "metadata": {},
   "outputs": [],
   "source": [
    "del larger_matrix\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdf5422a",
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14b3035c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%time values1 = matrix_array[0]\n",
    "values1[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dba2758c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%time values2 = matrix_array[1]\n",
    "values2[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2714ff86",
   "metadata": {},
   "outputs": [],
   "source": [
    "%time spatial.distance.cosine(values1,values2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c0e3ccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "%time similarities = np.matmul(matrix_array[0:10000], matrix_array[0:10000].T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e719c4e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "similarities[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1585ceaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "del similarities\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c98abc44",
   "metadata": {},
   "outputs": [],
   "source": [
    "%time similarities = np.dot(matrix_array[0:10000], matrix_array[0:10000].T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "282e5125",
   "metadata": {},
   "outputs": [],
   "source": [
    "similarities[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15f7a99a",
   "metadata": {},
   "outputs": [],
   "source": [
    "del similarities\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "154e6aa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%time similarities = matrix_array[0:10000]@matrix_array[0:10000].T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "076683a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "similarities[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feaebdef",
   "metadata": {},
   "outputs": [],
   "source": [
    "del similarities\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8042285",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6041eb38",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2abdbb83",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "241c87f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "len_users = len(users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0674b3d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "del similarity_dictionary\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c587d57",
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_dictionary = {}\n",
    "\n",
    "for user in np.arange(0, len_users, 1):\n",
    "    \n",
    "    similarity_dictionary[user] = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45805530",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(similarity_dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfce4de6",
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_dictionary[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d997482",
   "metadata": {},
   "outputs": [],
   "source": [
    "for user in np.arange(0, len_users, 1)[:1]:\n",
    "    \n",
    "    start = time.time()\n",
    "    user_values = matrix_array[user].reshape(-1,1)\n",
    "    \n",
    "    other_matrix = matrix_array[user+1:]\n",
    "    \n",
    "    similarities = cosine_similarity(other_matrix, user_values)\n",
    "    \n",
    "    end = time.time()\n",
    "    print(str(end-start)+' seconds')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cca6557",
   "metadata": {},
   "outputs": [],
   "source": [
    "similarities.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8355f899",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ed3a3c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72fbacab",
   "metadata": {},
   "outputs": [],
   "source": [
    "for user in np.arange(0, len_users, 1)[:3]:\n",
    "    \n",
    "    start = time.time()\n",
    "    user_values = matrix_array[user]\n",
    "    \n",
    "    for other_user in np.arange(0, len_users, 1):\n",
    "        \n",
    "        if user in similarity_dictionary[other_user]:\n",
    "            continue\n",
    "        \n",
    "        else:\n",
    "        \n",
    "            other_user_values = matrix_array[other_user]\n",
    "            similarity = 1 - spatial.distance.cosine(user_values,other_user_values)\n",
    "            similarity_dictionary[user][other_user] = similarity\n",
    "            similarity_dictionary[other_user][user] = similarity\n",
    "    \n",
    "    end = time.time()\n",
    "    print(str(end-start)+' seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b532ab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_dictionary[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a702ca58",
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_dictionary[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "654f4075",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccfb189e",
   "metadata": {},
   "outputs": [],
   "source": [
    "del matrix_array\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0933cd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#larger_matrix_T = pd.read_pickle('synthetic_ratings/users_synthetic_2193_fullmatrixT.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66586640",
   "metadata": {},
   "source": [
    "## Different ways to make calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56b71814",
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix_sparsed = pd.read_pickle('synthetic_ratings/users_synthetic_2193_sparsematrix.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9557c865",
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix_sparsed.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c2f0a9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix_sparsed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d710272a",
   "metadata": {},
   "outputs": [],
   "source": [
    "users = list(matrix_sparsed.index)\n",
    "users[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32488712",
   "metadata": {},
   "source": [
    "### Chunks, sparse non-normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09f5cddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "%time sparse_matrix = csr_matrix(matrix_sparsed.sparse.to_coo())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "981a033a",
   "metadata": {},
   "outputs": [],
   "source": [
    "del matrix_sparsed\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "302fe813",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(sparse_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f1169c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c392c36",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sparse_matrix.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6fe5aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "%time similarities = cosine_similarity(sparse_matrix[0:10000], sparse_matrix[0:10000], dense_output=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c34df71",
   "metadata": {},
   "outputs": [],
   "source": [
    "similarities[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58e6f1be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change chunk_size to control resource consumption and speed\n",
    "# Higher chunk_size means more memory/RAM needed but also faster \n",
    "chunk_size = 10000 \n",
    "matrix_len = sparse_matrix.shape[0] \n",
    "\n",
    "def similarity_cosine_by_chunk(start, end, dense):\n",
    "    if end > matrix_len:\n",
    "        end = matrix_len\n",
    "    return cosine_similarity(X=sparse_matrix[start:end], Y=sparse_matrix, dense_output=dense) # scikit-learn function\n",
    "\n",
    "#for chunk_start in range(0, 10, chunk_size):\n",
    "    #cosine_similarity_chunk = similarity_cosine_by_chunk(chunk_start, chunk_start+chunk_size)\n",
    "%time cosine_similarity_chunk = similarity_cosine_by_chunk(0, 10000, dense=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5f3f0dc",
   "metadata": {},
   "source": [
    "- Time for size 1, dense output: 39.4s\n",
    "- Time for size 1000, dense output: 8min 48s\n",
    "- Time for size 1, compact output: 47.8s\n",
    "- Time for size 10000, compact output: 1h 41min 6s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "374fb615",
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_similarity_chunk[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7c88325",
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b94919fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse_matrix[0:10000].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75e2c051",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change chunk_size to control resource consumption and speed\n",
    "# Higher chunk_size means more memory/RAM needed but also faster \n",
    "chunk_size = 10000 \n",
    "matrix_len = sparse_matrix.shape[0] \n",
    "\n",
    "def similarity_cosine_by_chunk(start, end, dense):\n",
    "    if end > matrix_len:\n",
    "        end = matrix_len\n",
    "    return np.matmul(sparse_matrix[start:end], sparse_matrix) # scikit-learn function\n",
    "\n",
    "#for chunk_start in range(0, 10, chunk_size):\n",
    "    #cosine_similarity_chunk = similarity_cosine_by_chunk(chunk_start, chunk_start+chunk_size)\n",
    "#%time cosine_similarity_chunk = similarity_cosine_by_chunk(0, 10000, dense=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c3ab31e",
   "metadata": {},
   "outputs": [],
   "source": [
    "similarities[:1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dc71525",
   "metadata": {},
   "source": [
    "### Chunks, normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef18d6cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "normed_matrix = pp.normalize(sparse_matrix.tocsc(), axis=0)\n",
    "del sparse_matrix\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2806570d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change chunk_size to control resource consumption and speed\n",
    "# Higher chunk_size means more memory/RAM needed but also faster \n",
    "chunk_size = 1000 \n",
    "matrix_len = normed_matrix.shape[0] \n",
    "\n",
    "def similarity_cosine_by_chunk(start, end, dense=False):\n",
    "    if end > matrix_len:\n",
    "        end = matrix_len\n",
    "    return cosine_similarity(X=normed_matrix[start:end], Y=normed_matrix, dense_output=dense) # scikit-learn function\n",
    "\n",
    "#for chunk_start in range(0, 10, chunk_size):\n",
    "    #cosine_similarity_chunk = similarity_cosine_by_chunk(chunk_start, chunk_start+chunk_size)\n",
    "%time cosine_similarity_chunk = similarity_cosine_by_chunk(0, 1, dense=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed842b58",
   "metadata": {},
   "source": [
    "Time for size 1, dense output: 1min 51s\n",
    "Time for size 1000, dense output: 10min 20s\n",
    "Time for size 1, compact output: 1min 51s\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4290b82",
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_similarity_chunk.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5be6f2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_similarity_chunk[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9dc2c78",
   "metadata": {},
   "outputs": [],
   "source": [
    "user = 'Torsten'\n",
    "\n",
    "%time sparse_user =  csr_matrix(matrix_sparsed.loc[user])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "152cc5b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse_user.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4590056f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sparseuser_AB = sparse_matrix.multiply(sparse_user)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0427749b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sparseuser_AB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ffeef70",
   "metadata": {},
   "source": [
    "### Old function with comparison blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25f24bb4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05b51313",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for user_block in block_indices_lookup:\n",
    "    \n",
    "    print(\"Starting block \"+str(user_block))\n",
    "    \n",
    "    starting_block_indexes = block_indices_lookup[user_block]\n",
    "    base_start = starting_block_indexes['Start']\n",
    "    base_end = starting_block_indexes['End']\n",
    "    \n",
    "    array_chunk_a = (matrix_array[base_start:base_end]/10).astype('float32')\n",
    "    \n",
    "    # Opening JSON file\n",
    "    with open('user_similarities/similarity_storage'+str(user_block)+'.json') as json_file:\n",
    "        base_users_storage = json.load(json_file)\n",
    "    \n",
    "    first_block_of_comparison = user_block\n",
    "    end_range = len(block_indices_lookup)+1    \n",
    "    \n",
    "    # TEMPORARY END RANGE FOR TESTINGS\n",
    "    end_range = 2\n",
    "    \n",
    "    for comparison_block in np.arange(first_block_of_comparison, end_range, 1):\n",
    "        \n",
    "        print(\"User Block \"+str(user_block)+' vs Comparison Block '+str(comparison_block))\n",
    "\n",
    "        # Opening JSON file\n",
    "        with open('user_similarities/similarity_storage'+str(comparison_block)+'.json') as json_file:\n",
    "            comparison_users_storage = json.load(json_file)\n",
    "        \n",
    "        comparison_indexes = block_indices_lookup[comparison_block]\n",
    "        compare_start = comparison_indexes['Start']\n",
    "        compare_end = comparison_indexes['End']\n",
    "        \n",
    "        print(\"Making matrices\")\n",
    "        start = time.time()\n",
    "        array_chunk_b = ((matrix_array[compare_start:compare_end].T)/10).astype('float32')\n",
    "        \n",
    "        a = tf.constant(array_chunk_a)\n",
    "        b = tf.constant(array_chunk_b)\n",
    "        \n",
    "        normalize_a = tf.nn.l2_normalize(a,1)\n",
    "        del a\n",
    "        gc.collect()\n",
    "\n",
    "        normalize_b = tf.nn.l2_normalize(b,0)\n",
    "        del b\n",
    "        gc.collect()\n",
    "        \n",
    "        print(\"Getting similarity scores\")\n",
    "        similarities = tf.matmul(normalize_a, normalize_b)#, adjoint_b=True)\n",
    "        del normalize_a\n",
    "        del normalize_b\n",
    "        gc.collect()\n",
    "        \n",
    "        # store user info\n",
    "        \n",
    "        incrementer_base = 0\n",
    "        \n",
    "        print(\"Storing Similarities\")\n",
    "        for base_user in user_blocks_lookup[user_block][:5]:\n",
    "            \n",
    "            print(base_user)\n",
    "                                   \n",
    "            user_similarities = similarities[incrementer_base].numpy()\n",
    "            max_spot = np.argmax(user_similarities.max())\n",
    "            mean_spot = np.median(user_similarities)\n",
    "            user_similarities[max_spot] = mean_spot\n",
    "            scaler = MinMaxScaler(feature_range=(-1,1))\n",
    "            user_similarities = scaler.fit_transform(user_similarities.reshape(-1,1))\n",
    "            user_similarities = list(np.round(user_similarities, 2).ravel())\n",
    "            \n",
    "            for key, value in list(zip(user_blocks_lookup[comparison_block][incrementer_base:], user_similarities[incrementer_base:])):\n",
    "                if value >= .25 or value <= -.25:\n",
    "                    base_users_storage[base_user][key] = float(value)\n",
    "                if user_block != comparison_block:\n",
    "                    comparison_users_storage[key][base_user] = float(value)\n",
    "            \n",
    "            incrementer_base +=1\n",
    "        \n",
    "            # save dictionary\n",
    "            with open('user_similarities/similarity_storage'+str(comparison_block)+'.json', 'w') as convert_file:\n",
    "                convert_file.write(json.dumps(comparison_users_storage))\n",
    "        \n",
    "        print(\"Cleaning up memory for this iteration\")\n",
    "        del comparison_users_storage\n",
    "        #del similarities\n",
    "        gc.collect()\n",
    "        \n",
    "        end = time.time()\n",
    "        print(str(end-start)+' seconds elapsed for this comparison section')\n",
    "    \n",
    "    # save dictionary\n",
    "    with open('user_similarities/similarity_storage'+str(user_block)+'.json', 'w') as convert_file:\n",
    "        convert_file.write(json.dumps(base_users_storage))\n",
    "        \n",
    "    #del base_users_storage\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e97fba49",
   "metadata": {},
   "source": [
    "## Deprecated Tensorflow time reduction attempts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbe39421",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# the basic file required for this work - the full matrix\n",
    "\n",
    "larger_matrix = pd.read_pickle('synthetic_ratings/users_synthetic_2193_sparsematrix_nogameids.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e24e0649",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "larger_matrix.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ccd4233",
   "metadata": {},
   "outputs": [],
   "source": [
    "larger_matrix.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bb8e108",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sparse dataframe into numpy array\n",
    "\n",
    "matrix_array = np.array(larger_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19c694b9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b578027a",
   "metadata": {},
   "source": [
    "Turn single user into a column 21921, 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4584f009",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_id = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78f51021",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get single user from matrix_array\n",
    "\n",
    "%time single_user = matrix_array[user_id]\n",
    "single_user.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09000ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get nonzero indices for user\n",
    "%time indices = list(np.nonzero(single_user)[0])\n",
    "indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbe1d0d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make reduced array for user of nonzero indices\n",
    "%time array_chunk_a = (single_user[indices]).astype('float32').reshape(-1,1)\n",
    "array_chunk_a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8455935",
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize user\n",
    "%time normalize_a = normalize(array_chunk_a, axis=0)\n",
    "normalize_a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c857f4cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7dc87dba",
   "metadata": {},
   "source": [
    "Investigate methods of reducing dataframe or array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d78adf7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make reduced on sparse dataframe\n",
    "%time df_chunk_b = larger_matrix[indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ec72e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_chunk_b.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51f90d17",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_chunk_b.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "932946d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make reduced on array\n",
    "%time array_chunk_b = matrix_array[:, indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cb2e9f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "array_chunk_b.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c108ee95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# turn array into sparse matrix\n",
    "sparse_matrix = sparse.csr_matrix(matrix_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae225589",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make reduced on sparse\n",
    "%time array_chunk_b = sparse_matrix[:, indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ee33c2b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5a8a11c7",
   "metadata": {},
   "source": [
    "Convert dataframe to array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0652460",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert reduced dataframe to sparse matrix\n",
    "\n",
    "%time sparse_array = sparse.csr_matrix(df_chunk_b.sparse.to_coo())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40891e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert reduced dataframe to array\n",
    "%time array_b_matrix = df_chunk_b.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95df43dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "array_b_matrix[0][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36972aae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b85f165",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9d6c57fb",
   "metadata": {},
   "source": [
    "Investigate normalization methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78fc26ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sklearn normalize on dataframe\n",
    "%time normalize_b = normalize(df_chunk_b, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44fc7094",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalize_b[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e9f137b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef1a4019",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sklearn normalize on array\n",
    "%time normalize_b = normalize(array_b_matrix, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47703181",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalize_b[0][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb3b4766",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "141e09ef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a6fa5e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make partial dataframe segment\n",
    "%time partial_df = df_chunk_b[:134400]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "932b8228",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make partial array segment\n",
    "%time partial_array = normalize_b[:134400]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b7e2197",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "365a298f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "370ec231",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a5f3251",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "227.5px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
