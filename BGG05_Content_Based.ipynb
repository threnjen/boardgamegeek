{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "979811e9",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4e48291",
   "metadata": {},
   "source": [
    "## Notebook Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e76df09b",
   "metadata": {},
   "source": [
    "### Package Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57e943a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import regex as re\n",
    "import time\n",
    "import os\n",
    "import gc\n",
    "\n",
    "# ignore warnings (gets rid of Pandas copy warnings)\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.options.display.max_columns = None\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 30)\n",
    "\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder\n",
    "from missingpy import MissForest\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "# preprocessing\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures, MinMaxScaler, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.experimental import enable_halving_search_cv\n",
    "from sklearn.model_selection import train_test_split, cross_validate, HalvingGridSearchCV, validation_curve, cross_val_score, GridSearchCV, KFold, RepeatedKFold, RandomizedSearchCV\n",
    "\n",
    "# model tools\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.formula.api import ols\n",
    "\n",
    "import scipy.stats as stats\n",
    "from scipy.stats import norm\n",
    "\n",
    "from sklearn.linear_model import LinearRegression, BayesianRidge, ElasticNet, GammaRegressor, HuberRegressor,  Lars, Lasso, SGDRegressor\n",
    "from sklearn.linear_model import LassoLars, OrthogonalMatchingPursuit, PassiveAggressiveRegressor, PoissonRegressor, RANSACRegressor, Ridge\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.svm import SVR, LinearSVR, NuSVR\n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor, ExtraTreesRegressor, AdaBoostRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "import xgboost as xgb\n",
    "\n",
    "# scoring and algorithm selection packages\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score \n",
    "from sklearn.inspection import permutation_importance'''\n",
    "\n",
    "# visualization packages\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "from surprise import KNNWithMeans, SVD, Dataset, Reader, dump, accuracy\n",
    "from surprise.model_selection.validation import cross_validate\n",
    "from surprise.model_selection import KFold"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "275f08a9",
   "metadata": {},
   "source": [
    "### Notebook Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e95d7216",
   "metadata": {},
   "source": [
    "##### Processing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64427375",
   "metadata": {},
   "outputs": [],
   "source": [
    "def processing_pipeline(weight_groups, df):\n",
    "    '''Takes in train, validation and test sets as well as lists of the cat, cont and polynomial fields, \n",
    "    as well as a list of fields to drop. Returns processed feature sets.\n",
    "    \n",
    "    Inputs:\n",
    "    train, val, test: feature sets for train, validation and testing\n",
    "    categoricals: list of categorical features\n",
    "    continuous: list of continuous features\n",
    "    poly: list of features that need polynomials\n",
    "    drop_fields: list of features to drop after target encoding\n",
    "    \n",
    "    Outputs:\n",
    "    processed_train, processed_val, processed_test: fully processed inputs'''\n",
    "\n",
    "   \n",
    "    # continuous pipeline\n",
    "    family_encoder = Pipeline([\n",
    "        ('encoder', OneHotEncoder()),\n",
    "        ('scaler', MinMaxScaler(feature_range=weight_groups[6])),\n",
    "         ])\n",
    "    \n",
    "    # Whole pipeline with continuous then categorical transformers\n",
    "    total_pipeline = ColumnTransformer([\n",
    "        ('games_weight_weight', MinMaxScaler(feature_range=weight_groups[0]), ['GameWeight']),\n",
    "        ('rating_weight', MinMaxScaler(feature_range=weight_groups[1]), ['AvgRating']),  \n",
    "        ('bayes_weight', MinMaxScaler(feature_range=weight_groups[2]), ['BayesAvgRating']),  \n",
    "        ('players_weight', MinMaxScaler(feature_range=weight_groups[3]), ['BestPlayers']),\n",
    "        ('playtime_weight', MinMaxScaler(feature_range=weight_groups[4]), ['Playtime']),\n",
    "        ('language_weight', MinMaxScaler(feature_range=weight_groups[5]), ['LanguageEase']),\n",
    "        ('remainder_weight', MinMaxScaler(feature_range=weight_groups[6]), ['Cat:Thematic', 'Cat:Strategy', 'Cat:War',\n",
    "                       'Cat:Family','Cat:CGS','Cat:Abstract','Cat:Party','Cat:Childrens']),\n",
    "        #('family_encoder', family_encoder, ['Family'])\n",
    "                            ]) #, sparse_threshold=0\n",
    "    \n",
    "    # Fit and tranform the pipeline on x_train, then transform x_test\n",
    "    processed = total_pipeline.fit_transform(df)\n",
    "    \n",
    "    return processed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31b690f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfidf_dataset(dataset, weights, transpose_toggle=True):\n",
    "    \n",
    "    # drop BGG Id\n",
    "    try: \n",
    "        dataset_pared = dataset.drop('BGGId', axis=1)\n",
    "    except: \n",
    "        dataset_pared = dataset\n",
    "    \n",
    "    # get list of titles to reapply to DF after transformation\n",
    "    titles = list(dataset_pared.columns)\n",
    "\n",
    "    # set up weighted scaler\n",
    "    scaler = MinMaxScaler(feature_range = weights)\n",
    "    \n",
    "    if transpose_toggle:\n",
    "        #instantiate tfidf transformer\n",
    "        tfidf = TfidfTransformer()\n",
    "    \n",
    "        #convert matrix to tfidf \n",
    "        tfidf_dataset = pd.DataFrame(tfidf.fit_transform(dataset_pared).toarray(), columns=titles)\n",
    "    \n",
    "        # run scaler on transpose (scale by row not column)\n",
    "        transpose_scaled = scaler.fit_transform(tfidf_dataset.T)\n",
    "    \n",
    "        # rebuild data frame\n",
    "        scaled_dataset = pd.DataFrame(transpose_scaled.T, columns=titles)\n",
    "    \n",
    "    else: \n",
    "        scaled_dataset = pd.DataFrame(scaler.fit_transform(dataset_pared), columns=titles)\n",
    "    \n",
    "    return scaled_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d0e6662",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_dataset(dataset, weights, transpose_toggle=True):\n",
    "    \n",
    "    # drop BGG Id\n",
    "    try: \n",
    "        dataset_pared = dataset.drop('BGGId', axis=1)\n",
    "    except: \n",
    "        dataset_pared = dataset\n",
    "    \n",
    "    # get list of titles to reapply to DF after transformation\n",
    "    titles = list(dataset_pared.columns)\n",
    "\n",
    "    # set up weighted scaler\n",
    "    scaler = MinMaxScaler(feature_range = weights)\n",
    "\n",
    "    total_entries = sum(dataset.sum())\n",
    "    \n",
    "    for item in list(dataset_pared.columns):\n",
    "        dataset_pared.loc[dataset_pared[item]>0, item] = dataset_pared[item].sum()/total_entries\n",
    "    \n",
    "    transpose_scaled = scaler.fit_transform(dataset_pared.T)\n",
    "    \n",
    "    scaled_dataset = pd.DataFrame(transpose_scaled.T, columns=titles)\n",
    "    \n",
    "    return scaled_dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ee8e79f",
   "metadata": {},
   "source": [
    "# Content Based Filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37589a3d",
   "metadata": {},
   "source": [
    "## TO DO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b68898e0",
   "metadata": {},
   "source": [
    "Fix duplicate game names (for example, Coup)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbfc4e58",
   "metadata": {},
   "source": [
    "## Set Weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54436791",
   "metadata": {},
   "source": [
    "A weight set I am very happy with\n",
    "\n",
    "games_weight_weight = (0, 1)\n",
    "rating_weight = (0, 1)\n",
    "bayes_weight = (0, 1)\n",
    "players_weight = (0, .5)\n",
    "playtime_weight = (0, .75) \n",
    "language_weight = (0, .25)\n",
    "mechanics_weight = (0, .5)\n",
    "designers_weight = (0, .5)\n",
    "#publisher_weight = (0, .1)\n",
    "categories_weight = (0, .5)\n",
    "#artist_weights = (0, .25)\n",
    "#awards_weights = (0, .25)\n",
    "family_weights = (0, .75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dcad1af",
   "metadata": {},
   "outputs": [],
   "source": [
    "games_weight_weight = (0, 1)\n",
    "rating_weight = (0, 1)\n",
    "bayes_weight = (0, 1)\n",
    "players_weight = (0, .5)\n",
    "playtime_weight = (0, .75) \n",
    "language_weight = (0, .25)\n",
    "mechanics_weight = (0, .5)\n",
    "designers_weight = (0, .5)\n",
    "#publisher_weight = (0, .1)\n",
    "categories_weight = (0, .5)\n",
    "#artist_weights = (0, .25)\n",
    "#awards_weights = (0, .25)\n",
    "family_weights = (0, .75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d0b6c15",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5c29560c",
   "metadata": {},
   "source": [
    "## Load and Prep Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5875ed4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "games = pd.read_pickle('data_cleaned/games.pkl')\n",
    "\n",
    "games['Playtime'] = 0\n",
    "games['Playtime'] = games.apply(lambda x: np.mean(x['ComMinPlaytime'] + x['ComMaxPlaytime']), axis=1)\n",
    "\n",
    "over_6_hours = list(games.loc[games['Playtime']>360].index)\n",
    "games.loc[over_6_hours, 'Playtime']=360\n",
    "\n",
    "\n",
    "mechanics = pd.read_pickle('data_cleaned/mechanics.pkl')\n",
    "designers = pd.read_pickle('data_cleaned/designers_reduced.pkl')\n",
    "publishers = pd.read_pickle('data_cleaned/publishers_reduced.pkl')\n",
    "artists = pd.read_pickle('data_cleaned/artists_reduced.pkl')\n",
    "awards = pd.read_pickle('data_cleaned/awards_reduced.pkl')\n",
    "\n",
    "\n",
    "games.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dc912fa",
   "metadata": {},
   "source": [
    "### TF-IDF and Scale Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78ce3cd1",
   "metadata": {},
   "source": [
    "##### Clean up mechanics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b378de1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up mechanics\n",
    "\n",
    "auction_list = ['Auction: Dexterity','Auction: Dutch','Auction: Dutch Priority',\n",
    "                'Auction: Fixed Placement','Auction: English','Auction: Once Around','Auction: Sealed Bid',\n",
    "                'Auction: Turn Order Until Pass','Multiple-Lot Auction','Closed Economy Auction','Selection Order Bid',\n",
    "                'Constrained Bidding']\n",
    "\n",
    "turn_order_list = ['Turn Order: Auction','Turn Order: Claim Action','Turn Order: Pass Order',\n",
    "                   'Turn Order: Progressive','Turn Order: Random','Turn Order: Role Order','Turn Order: Stat-Based']\n",
    "\n",
    "dumb_physical_list = ['Acting','Hot Potato','Singing','Rock-Paper-Scissors']\n",
    "\n",
    "drafting = ['Card Drafting']\n",
    "\n",
    "legacy = ['Legacy']\n",
    "\n",
    "worker_placement = ['Worker Placement with Dice Workers','Worker Placement, Different Worker Types'] #'Worker Placement',\n",
    "\n",
    "for item in worker_placement:\n",
    "    mechanics.loc[mechanics[item]==1, 'Worker Placement'] = int(1)\n",
    "    mechanics.drop([item], axis=1, inplace=True)\n",
    "\n",
    "for item in auction_list:\n",
    "    mechanics.loc[mechanics[item]==1, 'Auction/Bidding'] = int(1)\n",
    "    mechanics.drop([item], axis=1, inplace=True)\n",
    "\n",
    "mechanics['Physical'] = int(0)\n",
    "for item in dumb_physical_list:\n",
    "    mechanics.loc[mechanics[item]==1, 'Physical'] = int(1)\n",
    "    mechanics.drop([item], axis=1, inplace=True)\n",
    "    \n",
    "mechanics.loc[mechanics['Card Drafting']==1, 'Drafting'] = int(1)\n",
    "\n",
    "mechanics.loc[mechanics['Legacy']==1, 'Legacy Game'] = int(1)\n",
    "\n",
    "mechanics.drop(turn_order_list, axis=1, inplace=True)\n",
    "mechanics.drop(['Card Drafting','Legacy'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "364e4580",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scaled mechanics\n",
    "scaled_mechanics = tfidf_dataset(mechanics, mechanics_weight)\n",
    "\n",
    "# make new column for games without any mechanics information\n",
    "no_mechanics_index = list(scaled_mechanics.loc[scaled_mechanics.sum(axis=1)==0].index)\n",
    "scaled_mechanics['No Mechanics'] = 0\n",
    "scaled_mechanics.loc[no_mechanics_index, 'No Mechanics'] = (1/mechanics_weight[1])\n",
    "\n",
    "scaled_mechanics.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdbdd98e",
   "metadata": {},
   "source": [
    "##### TF-Scale Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aadf82bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scaled awards through tdidf/scaler\n",
    "#scaled_awards = scale_dataset(awards, awards_weights)\n",
    "\n",
    "# scaled designers\n",
    "scaled_designers = scale_dataset(designers, designers_weight)\n",
    "\n",
    "# scaled publishers\n",
    "#scaled_publishers = scale_dataset(publishers, publisher_weight)\n",
    "\n",
    "# scaled artists\n",
    "#scaled_artists = scale_dataset(artists, artist_weights)\n",
    "\n",
    "# scaled game families\n",
    "game_families = pd.get_dummies(games['Family'])\n",
    "scaled_families = scale_dataset(game_families, family_weights)\n",
    "\n",
    "scaled_designers.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8984f9f7",
   "metadata": {},
   "source": [
    "### Master CBF Frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d60ca69a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "games_included_columns = ['GameWeight', 'AvgRating', 'BayesAvgRating', 'BestPlayers', 'Playtime', 'LanguageEase',  'Cat:Thematic', 'Cat:Strategy', 'Cat:War', 'Cat:Family', 'Cat:CGS', 'Cat:Abstract', 'Cat:Party', 'Cat:Childrens']\n",
    "\n",
    "scaled_games = games[games_included_columns]\n",
    "game_names = list(games['Name'])\n",
    "game_ids = list(games['BGGId'])\n",
    "\n",
    "game_lookup = {}\n",
    "for key, value in zip(game_names, game_ids):\n",
    "    game_lookup[key] = value\n",
    "\n",
    "imputer = MissForest()\n",
    "scaled_games = pd.DataFrame(imputer.fit_transform(scaled_games), columns=games_included_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be35687a",
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_groups = [games_weight_weight, rating_weight, bayes_weight, players_weight, playtime_weight, language_weight, categories_weight, family_weights]\n",
    "scaled_games = pd.DataFrame(processing_pipeline(weight_groups, scaled_games), columns=games_included_columns)\n",
    "\n",
    "master_games = pd.concat((scaled_games, scaled_mechanics, scaled_families, scaled_designers), axis=1)\n",
    "# , scaled_artists, scaled_awards, , scaled_publishers\n",
    "\n",
    "game_and_id = list(zip(game_names, game_ids))\n",
    "master_games['Name'] = game_names\n",
    "\n",
    "master_games.set_index('Name', inplace=True)\n",
    "\n",
    "master_games.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c569d830",
   "metadata": {},
   "outputs": [],
   "source": [
    "master_games.loc[['Pandemic', 'Pandemic: Reign of Cthulhu', 'World of Warcraft: Wrath of the Lich King', 'Pandemic: Fall of Rome', 'Pandemic Legacy: Season 1', 'Pandemic Legacy: Season 0']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd859967",
   "metadata": {},
   "source": [
    "## Item Similarity via Cosine Distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6f9b1ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_sims = cosine_similarity(master_games)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2af683df",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sims_byname = pd.DataFrame(cosine_sims, columns=game_names)\n",
    "sims_byname['Game_Name'] = game_names\n",
    "sims_byname.set_index('Game_Name', inplace=True, drop=True)\n",
    "sims_byname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4950179",
   "metadata": {},
   "outputs": [],
   "source": [
    "sims_byid = pd.DataFrame(cosine_sims, columns=game_ids)\n",
    "sims_byid['Game_Id'] = game_ids\n",
    "sims_byid.set_index('Game_Id', inplace=True, drop=True)\n",
    "sims_byid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94804ad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler(feature_range = (-1, 1))\n",
    "\n",
    "scaled_comps = pd.DataFrame(scaler.fit_transform(sims_byid), columns=game_ids)\n",
    "scaled_comps['Game_Id'] = game_ids\n",
    "scaled_comps.set_index('Game_Id', inplace=True, drop=True)\n",
    "scaled_comps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58031081",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sims_byname.to_pickle('data_cleaned/game_cosine_similarity.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdda5478",
   "metadata": {},
   "source": [
    "### CHECK GAME HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9342e480",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "game_lookup['Pandemic Legacy: Season 0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7669caea",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dict = {'Dominion':list(sims_byname['Dominion'].sort_values(ascending=False)[1:15].index), 'D_Sim':list(sims_byname['Dominion'].sort_values(ascending=False)[1:15]),\n",
    "            'Gloomhaven':list(sims_byname['Gloomhaven'].sort_values(ascending=False)[1:15].index), 'G_Sim':list(sims_byname['Gloomhaven'].sort_values(ascending=False)[1:15]),\n",
    "            'Pandemic':list(sims_byname['Pandemic'].sort_values(ascending=False)[1:15].index), 'Pa_Sim':list(sims_byname['Pandemic'].sort_values(ascending=False)[1:15]),\n",
    "            'Splendor':list(sims_byname['Splendor'].sort_values(ascending=False)[1:15].index), 'Sp_Sim':list(sims_byname['Splendor'].sort_values(ascending=False)[1:15]),\n",
    "            'Viticulture Essential Edition':list(sims_byname['Viticulture Essential Edition'].sort_values(ascending=False)[1:15].index), 'V_Sim':list(sims_byname['Viticulture Essential Edition'].sort_values(ascending=False)[1:15]),\n",
    "            'Agricola':list(sims_byname['Agricola'].sort_values(ascending=False)[1:15].index), 'Ag_Sim':list(sims_byname['Agricola'].sort_values(ascending=False)[1:15]),\n",
    "            'Homesteaders':list(sims_byname['Homesteaders'].sort_values(ascending=False)[1:15].index), 'H_Sim':list(sims_byname['Homesteaders'].sort_values(ascending=False)[1:15]),\n",
    "            'Puerto Rico':list(sims_byname['Puerto Rico'].sort_values(ascending=False)[1:15].index), 'Pu_Sim':list(sims_byname['Puerto Rico'].sort_values(ascending=False)[1:15]),\n",
    "            'Chess':list(sims_byname['Chess'].sort_values(ascending=False)[1:15].index), 'Ch_Sim':list(sims_byname['Chess'].sort_values(ascending=False)[1:15]),\n",
    "            'Backgammon':list(sims_byname['Backgammon'].sort_values(ascending=False)[1:15].index), 'B_Sim':list(sims_byname['Backgammon'].sort_values(ascending=False)[1:15]),\n",
    "            'Sagrada':list(sims_byname['Sagrada'].sort_values(ascending=False)[1:15].index), 'Sa_Sim':list(sims_byname['Sagrada'].sort_values(ascending=False)[1:15]),\n",
    "            'Azul':list(sims_byname['Azul'].sort_values(ascending=False)[1:15].index), 'Az_Sim':list(sims_byname['Azul'].sort_values(ascending=False)[1:15]),\n",
    "            'Codenames':list(sims_byname['Codenames'].sort_values(ascending=False)[1:15].index), 'Co_Sim':list(sims_byname['Codenames'].sort_values(ascending=False)[1:15]),\n",
    "            'Secret Hitler':list(sims_byname['Secret Hitler'].sort_values(ascending=False)[1:15].index), 'Se_Sim':list(sims_byname['Secret Hitler'].sort_values(ascending=False)[1:15]),\n",
    "            'Monopoly':list(sims_byname['Monopoly'].sort_values(ascending=False)[1:15].index), 'M_Sim':list(sims_byname['Monopoly'].sort_values(ascending=False)[1:15]), \n",
    "            'Lords of Waterdeep':list(sims_byname['Lords of Waterdeep'].sort_values(ascending=False)[1:15].index), 'L_Sim':list(sims_byname['Lords of Waterdeep'].sort_values(ascending=False)[1:15]),\n",
    "            'Stone Age':list(sims_byname['Stone Age'].sort_values(ascending=False)[1:15].index), 'St_Sim':list(sims_byname['Stone Age'].sort_values(ascending=False)[1:15]),\n",
    "            'Century: Spice Road':list(sims_byname['Century: Spice Road'].sort_values(ascending=False)[1:15].index), 'Ce_Sim':list(sims_byname['Century: Spice Road'].sort_values(ascending=False)[1:15]),\n",
    "            'Scrabble':list(sims_byname['Scrabble'].sort_values(ascending=False)[1:15].index), 'Sc_Sim':list(sims_byname['Scrabble'].sort_values(ascending=False)[1:15])\n",
    "            }\n",
    "\n",
    "pd.DataFrame(test_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a77c126c",
   "metadata": {},
   "outputs": [],
   "source": [
    "game = 'Caylus'\n",
    "\n",
    "results = pd.DataFrame(data={'Similarity': sims_byname[game].sort_values(ascending=False)[1:]})\n",
    "results.head(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f04dae66",
   "metadata": {},
   "source": [
    "Dominion, Gloomhaven, Pandemic, Splendor, Viticulture Essential Edition, Agricola, Secret Hitler, Codenames, Azul, Sagrada, Homesteaders, Puerto Rico, Chess, Backgammon, Monopoly, Lords of Waterdeep, Stone Age, Century: Spice Road"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6f49926",
   "metadata": {},
   "outputs": [],
   "source": [
    "break"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7486fb3a",
   "metadata": {},
   "source": [
    "## Up Next\n",
    "\n",
    "With things tuned we do this:\n",
    "\n",
    "* Load in the user matrix\n",
    "* for each user,\n",
    "    * get the user's average rating\n",
    "    * for each item that the user has rated, get the full list of comps with similarities\n",
    "    \n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5019781b",
   "metadata": {},
   "source": [
    "## Load User Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a25a93df",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_matrix = user_user_filter = pd.read_pickle('data_cleaned/ratings_matrix.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64f131f9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "user_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af012865",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a list of items with more than 30 ratings\n",
    "sums = pd.DataFrame(user_matrix.count()>=30)\n",
    "\n",
    "# get indices for the columns with more than 30 ratings\n",
    "keep_these = sums.loc[sums[0]==True].index\n",
    "\n",
    "smaller_matrix = user_matrix[keep_these]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b773b334",
   "metadata": {},
   "outputs": [],
   "source": [
    "smaller_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95f916b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a list of users with fewer than 100 ratings\n",
    "sums = pd.DataFrame(smaller_matrix.count(axis=1)>=100)\n",
    "\n",
    "# get indices for the columns with fewer than 100 ratings\n",
    "drop_these = sums.loc[sums[0]==False].index\n",
    "\n",
    "# drop the columns with fewer than 100 ratings\n",
    "smaller_matrix.drop(drop_these, axis=0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11945318",
   "metadata": {},
   "outputs": [],
   "source": [
    "smaller_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13b8d54f",
   "metadata": {},
   "outputs": [],
   "source": [
    "del user_matrix\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaf60f09",
   "metadata": {},
   "outputs": [],
   "source": [
    "smaller_matrix = smaller_matrix[:10]\n",
    "smaller_matrix = smaller_matrix.T\n",
    "smaller_matrix = smaller_matrix[:100]\n",
    "smaller_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a26f0e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c79f23e",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_dictionary = {}\n",
    "\n",
    "for user in smaller_matrix.columns:\n",
    "    \n",
    "    user_mean = smaller_matrix[user].mean()\n",
    "    game_ratings_normed = list(smaller_matrix.loc[smaller_matrix[user].notna()][user] - user_mean)\n",
    "    game_ids = list(smaller_matrix.loc[smaller_matrix[user].notna()][user].index)\n",
    "    \n",
    "    user_game_ratings = {}\n",
    "    \n",
    "    for key, value in zip(game_ids, game_ratings_normed):\n",
    "        user_game_ratings[key] = value\n",
    "    \n",
    "    overall_user = {}\n",
    "    \n",
    "    overall_user['Mean'] = user_mean\n",
    "    overall_user['Ratings'] = user_game_ratings\n",
    "    \n",
    "    user_dictionary[user] = overall_user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f3e4fb8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "user_dictionary['-Johnny-']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b3ee902",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_df = pd.DataFrame(user_dictionary['-Johnny-']['Ratings'].values(), index=user_dictionary['-Johnny-']['Ratings'].keys(), columns=['Rating'])\n",
    "user_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3849ea51",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_dictionary['-Johnny-']['Ratings']['21241']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "739aa693",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "len(user_dictionary['-Johnny-']['Ratings'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87d6ab3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(sims_byid[21241].sort_values(ascending=False)[1:21])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ad18738",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for item in user_dictionary['-Johnny-']['Ratings'].keys():\n",
    "    \n",
    "    item_int = int(item)\n",
    "    \n",
    "    comps_index = list(sims_byid[item_int].sort_values(ascending=False)[1:21].index)\n",
    "    comps_similarity = list(sims_byid[item_int].sort_values(ascending=False)[1:21])\n",
    "    \n",
    "    for position, item in enumerate(comps_index):\n",
    "    \n",
    "        if item in user_dictionary['-Johnny-']['Ratings'].keys():\n",
    "            print(\"Top comp is already rated, trying again\")\n",
    "            continue\n",
    "        else:\n",
    "            print(\"Computing comp\")\n",
    "            similarity = comps_similarity[position]\n",
    "            print(similarity)\n",
    "            print(user_dictionary['-Johnny-']['Ratings'][item])\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfa60037",
   "metadata": {},
   "outputs": [],
   "source": [
    "sims_byid[68448].sort_values(ascending=False)[1:21]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53fa94ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "comps = {}\n",
    "\n",
    "for game in rated_items_sample:\n",
    "    \n",
    "    game = int(game)\n",
    "    \n",
    "    this_comps = list(sims_byid[game].sort_values(ascending=False)[1:11].index)\n",
    "    this_similarities = list(sims_byid[game].sort_values(ascending=False)[1:11])\n",
    "    \n",
    "    for key, value in zip(this_comps, this_similarities):\n",
    "        \n",
    "        comps[key] = value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d41d54d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(comps.values(), index=comps.keys()).sort_values(0, ascending=False).drop_duplicates(keep='first')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99245c95",
   "metadata": {},
   "source": [
    "### CLEAR VARIABLES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31f339fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "del mechanics\n",
    "del designers\n",
    "del publishers\n",
    "del artists\n",
    "del awards\n",
    "del games\n",
    "del scaled_games\n",
    "del scaled_mechanics\n",
    "del scaled_families\n",
    "del scaled_designers\n",
    "del scaled_publishers\n",
    "del scaled_artists\n",
    "del scaled_awards\n",
    "del master_games_list\n",
    "del game_families\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7143b708",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "292.865px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
