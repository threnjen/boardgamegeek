{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a3bcc119",
   "metadata": {},
   "source": [
    "# Notebook Objective and Setup\n",
    "\n",
    "BGG06 is where synthetic ratings are produced for each user, using the content-based item filter from BGG05."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60b8b86c",
   "metadata": {},
   "source": [
    "## Package Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a7da13e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have 16 cores to work on!\n"
     ]
    }
   ],
   "source": [
    "import multiprocessing\n",
    "ncpus = multiprocessing.cpu_count()\n",
    "print(\"We have {} cores to work on!\".format(ncpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5f40cb89",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import numba as nb\n",
    "import requests\n",
    "import regex as re\n",
    "import time\n",
    "import gc\n",
    "import copy\n",
    "import json\n",
    "from statistics import mean\n",
    "\n",
    "from numba import jit, cuda, prange\n",
    "from numpy.linalg import norm\n",
    "\n",
    "# ignore warnings (gets rid of Pandas copy warnings)\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.options.display.max_columns = None\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "\n",
    "#from scipy import sparse\n",
    "#from scipy.sparse import csr_matrix\n",
    "#from scipy import spatial\n",
    "\n",
    "#from sklearn.metrics.pairwise import cosine_similarity\n",
    "#import sklearn.preprocessing as pp\n",
    "#from sklearn.preprocessing import StandardScaler, PolynomialFeatures, MinMaxScaler, OneHotEncoder\n",
    "from sklearn.preprocessing import MinMaxScaler, normalize\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.compat.v1.losses import cosine_distance\n",
    "from tensorflow.keras.losses import CosineSimilarity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ef316c6",
   "metadata": {},
   "source": [
    "## Notebook Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "722f1152",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the basic file required for this work - the full matrix\n",
    "\n",
    "#larger_matrix = pd.read_pickle('synthetic_ratings/users_synthetic_1000_fullmatrix.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "233dd54d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR! Session/line number was not unique in database. History logging moved to new session 4717\n"
     ]
    }
   ],
   "source": [
    "# the basic file required for this work - the full matrix\n",
    "\n",
    "larger_matrix = pd.read_pickle('real_ratings/users_real_scaled_fullmatrix.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a0421a3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert full matrix to numpy and delete matrix\n",
    "\n",
    "matrix_array = larger_matrix.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c2994f85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['213788',\n",
       " '206593',\n",
       " '267333',\n",
       " '249824',\n",
       " '301085',\n",
       " '261321',\n",
       " '264212',\n",
       " '249552',\n",
       " '290618',\n",
       " '302933']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gameids_columnorder = list(larger_matrix.columns)\n",
    "gameids_columnorder[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5f460db5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "105"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del larger_matrix\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f6ccd95c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"storage_dict_1 = {}\\nstorage_dict_2 = {}\\n    \\nfor game in gameids_columnorder[:10000]:\\n    storage_dict_1[game] = {}\\n    \\nfor game in gameids_columnorder[10000:]:\\n    storage_dict_2[game] = {}\\n    \\n\\n\\n    # save dictionary\\nwith open('item_similarities/similarity_storage_real_scaled_temp_1.json', 'w') as convert_file:\\n    convert_file.write(json.dumps(storage_dict_1))    \\n\\nwith open('item_similarities/similarity_storage_real_scaled_temp_2.json', 'w') as convert_file:\\n    convert_file.write(json.dumps(storage_dict_2))    \\n#del storage_dict\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''storage_dict_1 = {}\n",
    "storage_dict_2 = {}\n",
    "    \n",
    "for game in gameids_columnorder[:10000]:\n",
    "    storage_dict_1[game] = {}\n",
    "    \n",
    "for game in gameids_columnorder[10000:]:\n",
    "    storage_dict_2[game] = {}\n",
    "    \n",
    "\n",
    "\n",
    "    # save dictionary\n",
    "with open('item_similarities/similarity_storage_real_scaled_temp_1.json', 'w') as convert_file:\n",
    "    convert_file.write(json.dumps(storage_dict_1))    \n",
    "\n",
    "with open('item_similarities/similarity_storage_real_scaled_temp_2.json', 'w') as convert_file:\n",
    "    convert_file.write(json.dumps(storage_dict_2))    \n",
    "#del storage_dict'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cb59f98",
   "metadata": {},
   "source": [
    "##### Files:\n",
    "\n",
    "- user_blocks_lookup  dict in format  dict[file_append]:[list of users in block]\n",
    "- user_id_lookup  dict in format dict[user_id] = username\n",
    "- block_indices_lookup  dict in format dict[file_append]: {'Start': start index, 'End': end index}\n",
    "- storage dictionaries located at 'user_similarities/similarity_storage'+str(file_append)+'.json'\n",
    "- matrix_array  numpy array which must be numerically indexed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e60d8df",
   "metadata": {},
   "source": [
    "# Similarity Calculations - Parallelization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "357089e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "item1 = 43\n",
    "item2 = 99"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "919e7c02",
   "metadata": {},
   "source": [
    "### With Jit and common_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "554d76eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "range(0, 21925)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR! Session/line number was not unique in database. History logging moved to new session 4676\n"
     ]
    }
   ],
   "source": [
    "prange(len(number_of_games))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f254e5ff",
   "metadata": {},
   "source": [
    "#### To do:\n",
    "\n",
    "re-organize the output as a dictionary with the actual game id as key (from gameids_columnorder) and corresponding item similarity\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2ae47c28",
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit(nopython=True, parallel=True, fastmath=True)\n",
    "def math_function(game, matrix_array, number_of_games):\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    # make the single user matrix for the one user\n",
    "    single_item = matrix_array[:, game].copy()\n",
    "    # get the indices where the user is nonzero\n",
    "    indices = np.nonzero(single_item)[0]\n",
    "    \n",
    "    for game2 in number_of_games:\n",
    "    \n",
    "        next_item = matrix_array[:, game2].copy()\n",
    "        indices2 = np.nonzero(next_item)[0]\n",
    "            \n",
    "        common_indices = np.intersect1d(indices, indices2)\n",
    "        \n",
    "        if len(common_indices)<4:\n",
    "            results.append(0)\n",
    "            continue      \n",
    "        \n",
    "        else:\n",
    "            a = single_item[common_indices].astype(np.float32)\n",
    "            b = next_item[common_indices].astype(np.float32)\n",
    "        \n",
    "            try:\n",
    "                item_similarity = a @ b.T / (norm(a)*norm(b))\n",
    "                results.append(item_similarity)\n",
    "            except:\n",
    "                results.append(0)\n",
    "        \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f6486e10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "number_of_games = np.arange(0, matrix_array.shape[1], 1)\n",
    "\n",
    "games_range = len(number_of_games[:100])\n",
    "games_range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfb0c431",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting game: 0\n"
     ]
    }
   ],
   "source": [
    "number_of_games = np.arange(0, matrix_array.shape[1], 1)\n",
    "\n",
    "global_start = time.time()\n",
    "\n",
    "# Load the storage dictionary for this block\n",
    "with open('item_similarities/similarity_storage_real_scaled_temp_1.json') as json_file:\n",
    "    base_items_storage = json.load(json_file) \n",
    "    \n",
    "# for each user block in the block_indices_lookup. The user blocks are integers from 1-20\n",
    "for game in number_of_games[:100]:\n",
    "    \n",
    "    print(\"\\nStarting game: \"+str(game))\n",
    "    start = time.time()\n",
    "    \n",
    "    gameid_1 = gameids_columnorder[game]\n",
    "                     \n",
    "    results = math_function(game, matrix_array, number_of_games) #, results_a, results_b\n",
    "    \n",
    "    base_items_storage[gameid_1]['Sims'] = results\n",
    "    \n",
    "    end=time.time()\n",
    "    \n",
    "    print(end-start)\n",
    "\n",
    "print(time.time()-global_start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9a68749a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save dictionary\n",
    "with open('item_similarities/similarity_storage_real_scaled_1_timestamp.json', 'w') as convert_file:\n",
    "    convert_file.write(json.dumps(base_items_storage))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd4c6ea1",
   "metadata": {},
   "source": [
    "Time total: 50587 for 10000 entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d9f8a26",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n",
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n",
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Wadkins\\miniconda3\\envs\\gpu\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3444, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"C:\\Users\\Wadkins\\AppData\\Local\\Temp/ipykernel_9492/1747575373.py\", line 1, in <module>\n",
      "    base_items_storage['174430']\n",
      "NameError: name 'base_items_storage' is not defined\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Wadkins\\miniconda3\\envs\\gpu\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2064, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'NameError' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Wadkins\\miniconda3\\envs\\gpu\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1101, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"C:\\Users\\Wadkins\\miniconda3\\envs\\gpu\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 248, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Users\\Wadkins\\miniconda3\\envs\\gpu\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 281, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"C:\\Users\\Wadkins\\miniconda3\\envs\\gpu\\lib\\inspect.py\", line 1502, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "  File \"C:\\Users\\Wadkins\\miniconda3\\envs\\gpu\\lib\\inspect.py\", line 1460, in getframeinfo\n",
      "    filename = getsourcefile(frame) or getfile(frame)\n",
      "  File \"C:\\Users\\Wadkins\\miniconda3\\envs\\gpu\\lib\\inspect.py\", line 696, in getsourcefile\n",
      "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
      "  File \"C:\\Users\\Wadkins\\miniconda3\\envs\\gpu\\lib\\inspect.py\", line 733, in getmodule\n",
      "    if ismodule(module) and hasattr(module, '__file__'):\n",
      "  File \"C:\\Users\\Wadkins\\miniconda3\\envs\\gpu\\lib\\site-packages\\tensorflow\\__init__.py\", line 50, in __getattr__\n",
      "    module = self._load()\n",
      "  File \"C:\\Users\\Wadkins\\miniconda3\\envs\\gpu\\lib\\site-packages\\tensorflow\\__init__.py\", line 44, in _load\n",
      "    module = _importlib.import_module(self.__name__)\n",
      "  File \"C:\\Users\\Wadkins\\miniconda3\\envs\\gpu\\lib\\importlib\\__init__.py\", line 127, in import_module\n",
      "    return _bootstrap._gcd_import(name[level:], package, level)\n",
      "  File \"<frozen importlib._bootstrap>\", line 1006, in _gcd_import\n",
      "  File \"<frozen importlib._bootstrap>\", line 983, in _find_and_load\n",
      "  File \"<frozen importlib._bootstrap>\", line 965, in _find_and_load_unlocked\n",
      "ModuleNotFoundError: No module named 'tensorflow_core.estimator'\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Wadkins\\miniconda3\\envs\\gpu\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3444, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"C:\\Users\\Wadkins\\AppData\\Local\\Temp/ipykernel_9492/1747575373.py\", line 1, in <module>\n",
      "    base_items_storage['174430']\n",
      "NameError: name 'base_items_storage' is not defined\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Wadkins\\miniconda3\\envs\\gpu\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2064, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'NameError' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Wadkins\\miniconda3\\envs\\gpu\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3364, in run_ast_nodes\n",
      "    if (await self.run_code(code, result,  async_=asy)):\n",
      "  File \"C:\\Users\\Wadkins\\miniconda3\\envs\\gpu\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3461, in run_code\n",
      "    self.showtraceback(running_compiled_code=True)\n",
      "  File \"C:\\Users\\Wadkins\\miniconda3\\envs\\gpu\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2067, in showtraceback\n",
      "    value, tb, tb_offset=tb_offset)\n",
      "  File \"C:\\Users\\Wadkins\\miniconda3\\envs\\gpu\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1368, in structured_traceback\n",
      "    self, etype, value, tb, tb_offset, number_of_lines_of_context)\n",
      "  File \"C:\\Users\\Wadkins\\miniconda3\\envs\\gpu\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1268, in structured_traceback\n",
      "    self, etype, value, tb, tb_offset, number_of_lines_of_context\n",
      "  File \"C:\\Users\\Wadkins\\miniconda3\\envs\\gpu\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1125, in structured_traceback\n",
      "    tb_offset)\n",
      "  File \"C:\\Users\\Wadkins\\miniconda3\\envs\\gpu\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1082, in format_exception_as_a_whole\n",
      "    last_unique, recursion_repeat = find_recursion(orig_etype, evalue, records)\n",
      "  File \"C:\\Users\\Wadkins\\miniconda3\\envs\\gpu\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 382, in find_recursion\n",
      "    return len(records), 0\n",
      "TypeError: object of type 'NoneType' has no len()\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Wadkins\\miniconda3\\envs\\gpu\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2064, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'TypeError' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Wadkins\\miniconda3\\envs\\gpu\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1101, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"C:\\Users\\Wadkins\\miniconda3\\envs\\gpu\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 248, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Users\\Wadkins\\miniconda3\\envs\\gpu\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 281, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"C:\\Users\\Wadkins\\miniconda3\\envs\\gpu\\lib\\inspect.py\", line 1502, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "  File \"C:\\Users\\Wadkins\\miniconda3\\envs\\gpu\\lib\\inspect.py\", line 1460, in getframeinfo\n",
      "    filename = getsourcefile(frame) or getfile(frame)\n",
      "  File \"C:\\Users\\Wadkins\\miniconda3\\envs\\gpu\\lib\\inspect.py\", line 696, in getsourcefile\n",
      "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
      "  File \"C:\\Users\\Wadkins\\miniconda3\\envs\\gpu\\lib\\inspect.py\", line 733, in getmodule\n",
      "    if ismodule(module) and hasattr(module, '__file__'):\n",
      "  File \"C:\\Users\\Wadkins\\miniconda3\\envs\\gpu\\lib\\site-packages\\tensorflow\\__init__.py\", line 50, in __getattr__\n",
      "    module = self._load()\n",
      "  File \"C:\\Users\\Wadkins\\miniconda3\\envs\\gpu\\lib\\site-packages\\tensorflow\\__init__.py\", line 44, in _load\n",
      "    module = _importlib.import_module(self.__name__)\n",
      "  File \"C:\\Users\\Wadkins\\miniconda3\\envs\\gpu\\lib\\importlib\\__init__.py\", line 127, in import_module\n",
      "    return _bootstrap._gcd_import(name[level:], package, level)\n",
      "  File \"<frozen importlib._bootstrap>\", line 1006, in _gcd_import\n",
      "  File \"<frozen importlib._bootstrap>\", line 983, in _find_and_load\n",
      "  File \"<frozen importlib._bootstrap>\", line 965, in _find_and_load_unlocked\n",
      "ModuleNotFoundError: No module named 'tensorflow_core.estimator'\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Wadkins\\miniconda3\\envs\\gpu\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3444, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"C:\\Users\\Wadkins\\AppData\\Local\\Temp/ipykernel_9492/1747575373.py\", line 1, in <module>\n",
      "    base_items_storage['174430']\n",
      "NameError: name 'base_items_storage' is not defined\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Wadkins\\miniconda3\\envs\\gpu\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2064, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'NameError' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Wadkins\\miniconda3\\envs\\gpu\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3364, in run_ast_nodes\n",
      "    if (await self.run_code(code, result,  async_=asy)):\n",
      "  File \"C:\\Users\\Wadkins\\miniconda3\\envs\\gpu\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3461, in run_code\n",
      "    self.showtraceback(running_compiled_code=True)\n",
      "  File \"C:\\Users\\Wadkins\\miniconda3\\envs\\gpu\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2067, in showtraceback\n",
      "    value, tb, tb_offset=tb_offset)\n",
      "  File \"C:\\Users\\Wadkins\\miniconda3\\envs\\gpu\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1368, in structured_traceback\n",
      "    self, etype, value, tb, tb_offset, number_of_lines_of_context)\n",
      "  File \"C:\\Users\\Wadkins\\miniconda3\\envs\\gpu\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1268, in structured_traceback\n",
      "    self, etype, value, tb, tb_offset, number_of_lines_of_context\n",
      "  File \"C:\\Users\\Wadkins\\miniconda3\\envs\\gpu\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1125, in structured_traceback\n",
      "    tb_offset)\n",
      "  File \"C:\\Users\\Wadkins\\miniconda3\\envs\\gpu\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1082, in format_exception_as_a_whole\n",
      "    last_unique, recursion_repeat = find_recursion(orig_etype, evalue, records)\n",
      "  File \"C:\\Users\\Wadkins\\miniconda3\\envs\\gpu\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 382, in find_recursion\n",
      "    return len(records), 0\n",
      "TypeError: object of type 'NoneType' has no len()\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Wadkins\\miniconda3\\envs\\gpu\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2064, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'TypeError' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Wadkins\\miniconda3\\envs\\gpu\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2947, in _run_cell\n",
      "    return runner(coro)\n",
      "  File \"C:\\Users\\Wadkins\\miniconda3\\envs\\gpu\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 68, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"C:\\Users\\Wadkins\\miniconda3\\envs\\gpu\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3173, in run_cell_async\n",
      "    interactivity=interactivity, compiler=compiler, result=result)\n",
      "  File \"C:\\Users\\Wadkins\\miniconda3\\envs\\gpu\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3383, in run_ast_nodes\n",
      "    self.showtraceback()\n",
      "  File \"C:\\Users\\Wadkins\\miniconda3\\envs\\gpu\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2067, in showtraceback\n",
      "    value, tb, tb_offset=tb_offset)\n",
      "  File \"C:\\Users\\Wadkins\\miniconda3\\envs\\gpu\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1368, in structured_traceback\n",
      "    self, etype, value, tb, tb_offset, number_of_lines_of_context)\n",
      "  File \"C:\\Users\\Wadkins\\miniconda3\\envs\\gpu\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1268, in structured_traceback\n",
      "    self, etype, value, tb, tb_offset, number_of_lines_of_context\n",
      "  File \"C:\\Users\\Wadkins\\miniconda3\\envs\\gpu\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1143, in structured_traceback\n",
      "    chained_exceptions_tb_offset)\n",
      "  File \"C:\\Users\\Wadkins\\miniconda3\\envs\\gpu\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1082, in format_exception_as_a_whole\n",
      "    last_unique, recursion_repeat = find_recursion(orig_etype, evalue, records)\n",
      "  File \"C:\\Users\\Wadkins\\miniconda3\\envs\\gpu\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 382, in find_recursion\n",
      "    return len(records), 0\n",
      "TypeError: object of type 'NoneType' has no len()\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Wadkins\\miniconda3\\envs\\gpu\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2064, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'TypeError' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Wadkins\\miniconda3\\envs\\gpu\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1101, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"C:\\Users\\Wadkins\\miniconda3\\envs\\gpu\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 248, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Users\\Wadkins\\miniconda3\\envs\\gpu\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 281, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"C:\\Users\\Wadkins\\miniconda3\\envs\\gpu\\lib\\inspect.py\", line 1502, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "  File \"C:\\Users\\Wadkins\\miniconda3\\envs\\gpu\\lib\\inspect.py\", line 1460, in getframeinfo\n",
      "    filename = getsourcefile(frame) or getfile(frame)\n",
      "  File \"C:\\Users\\Wadkins\\miniconda3\\envs\\gpu\\lib\\inspect.py\", line 696, in getsourcefile\n",
      "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
      "  File \"C:\\Users\\Wadkins\\miniconda3\\envs\\gpu\\lib\\inspect.py\", line 733, in getmodule\n",
      "    if ismodule(module) and hasattr(module, '__file__'):\n",
      "  File \"C:\\Users\\Wadkins\\miniconda3\\envs\\gpu\\lib\\site-packages\\tensorflow\\__init__.py\", line 50, in __getattr__\n",
      "    module = self._load()\n",
      "  File \"C:\\Users\\Wadkins\\miniconda3\\envs\\gpu\\lib\\site-packages\\tensorflow\\__init__.py\", line 44, in _load\n",
      "    module = _importlib.import_module(self.__name__)\n",
      "  File \"C:\\Users\\Wadkins\\miniconda3\\envs\\gpu\\lib\\importlib\\__init__.py\", line 127, in import_module\n",
      "    return _bootstrap._gcd_import(name[level:], package, level)\n",
      "  File \"<frozen importlib._bootstrap>\", line 1006, in _gcd_import\n",
      "  File \"<frozen importlib._bootstrap>\", line 983, in _find_and_load\n",
      "  File \"<frozen importlib._bootstrap>\", line 965, in _find_and_load_unlocked\n",
      "ModuleNotFoundError: No module named 'tensorflow_core.estimator'\n"
     ]
    }
   ],
   "source": [
    "base_items_storage['174430']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2893475",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "from __future__ import print_function, division, absolute_import\n",
    "\n",
    "import math\n",
    "import threading\n",
    "from timeit import repeat\n",
    "\n",
    "import numpy as np\n",
    "from numba import jit\n",
    "\n",
    "nthreads = 4\n",
    "size = 10**7  # CHANGED\n",
    "\n",
    "# CHANGED\n",
    "def func_np(a, b):\n",
    "    \"\"\"\n",
    "    Control function using Numpy.\n",
    "    \"\"\"\n",
    "    return a + b\n",
    "\n",
    "# CHANGED\n",
    "@jit('void(double[:], double[:], double[:])', nopython=True, nogil=True)\n",
    "def inner_func_nb(result, a, b):\n",
    "    \"\"\"\n",
    "    Function under test.\n",
    "    \"\"\"\n",
    "    for i in range(len(result)):\n",
    "        result[i] = a[i] + b[i]\n",
    "\n",
    "def timefunc(correct, s, func, *args, **kwargs):\n",
    "    \"\"\"\n",
    "    Benchmark *func* and print out its runtime.\n",
    "    \"\"\"\n",
    "    print(s.ljust(20), end=\" \")\n",
    "    # Make sure the function is compiled before we start the benchmark\n",
    "    res = func(*args, **kwargs)\n",
    "    if correct is not None:\n",
    "        assert np.allclose(res, correct), (res, correct)\n",
    "    # time it\n",
    "    print('{:>5.0f} ms'.format(min(repeat(lambda: func(*args, **kwargs),\n",
    "                                          number=5, repeat=2)) * 1000))\n",
    "    return res\n",
    "\n",
    "def make_singlethread(inner_func):\n",
    "    \"\"\"\n",
    "    Run the given function inside a single thread.\n",
    "    \"\"\"\n",
    "    def func(*args):\n",
    "        length = len(args[0])\n",
    "        result = np.empty(length, dtype=np.float64)\n",
    "        inner_func(result, *args)\n",
    "        return result\n",
    "    return func\n",
    "\n",
    "def make_multithread(inner_func, numthreads):\n",
    "    \"\"\"\n",
    "    Run the given function inside *numthreads* threads, splitting its\n",
    "    arguments into equal-sized chunks.\n",
    "    \"\"\"\n",
    "    def func_mt(*args):\n",
    "        length = len(args[0])\n",
    "        result = np.empty(length, dtype=np.float64)\n",
    "        args = (result,) + args\n",
    "        chunklen = (length + numthreads - 1) // numthreads\n",
    "        # Create argument tuples for each input chunk\n",
    "        chunks = [[arg[i * chunklen:(i + 1) * chunklen] for arg in args]\n",
    "                  for i in range(numthreads)]\n",
    "        # Spawn one thread per chunk\n",
    "        threads = [threading.Thread(target=inner_func, args=chunk)\n",
    "                   for chunk in chunks]\n",
    "        for thread in threads:\n",
    "            thread.start()\n",
    "        for thread in threads:\n",
    "            thread.join()\n",
    "        return result\n",
    "    return func_mt\n",
    "\n",
    "\n",
    "func_nb = make_singlethread(inner_func_nb)\n",
    "func_nb_mt = make_multithread(inner_func_nb, nthreads)\n",
    "\n",
    "a = np.random.rand(size)\n",
    "b = np.random.rand(size)\n",
    "\n",
    "correct = timefunc(None, \"numpy (1 thread)\", func_np, a, b)\n",
    "timefunc(correct, \"numba (1 thread)\", func_nb, a, b)\n",
    "timefunc(correct, \"numba (%d threads)\" % nthreads, func_nb_mt, a, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "214ab804",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Similarity Calculations - Tensorflow (GPU only)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd39c8ca",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Code Work - Item to Item tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb165c85",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "larger_matrix.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0e99202",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "matrix_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1b6f42b",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "matrix_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "070bbdb0",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "item1 = 63\n",
    "item2 = 68"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7e9936b",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Step one\n",
    "# make the single item matrix for the one item\n",
    "%time single_item = matrix_array[:, item1]\n",
    "single_item.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e190cee",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# get the indices where the item is nonzero\n",
    "%time indices = list(np.nonzero(single_item)[0])\n",
    "len(indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30e8ae28",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Step 2\n",
    "# make the single item matrix for the next item\n",
    "%time next_item = matrix_array[:, item2]\n",
    "# get the indices where the item is nonzero\n",
    "%time indices2 = list(np.nonzero(next_item)[0])\n",
    "\n",
    "len(indices2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "349bcd03",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "%time common_indices = list(set.intersection(set(indices), set(indices2)))\n",
    "len(common_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b53666d",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "\n",
    "# Step one\n",
    "# make the single item matrix for the one item\n",
    "%time single_item = matrix_array[:, item1]\n",
    "# get the indices where the item is nonzero\n",
    "%time indices = list(np.nonzero(single_item)[0])\n",
    "#%time indices = np.nonzero(single_item)[0]\n",
    "\n",
    "# Step 2\n",
    "# make the single item matrix for the next item\n",
    "%time next_item = matrix_array[:, item2]\n",
    "# get the indices where the item is nonzero\n",
    "%time indices2 = list(np.nonzero(next_item)[0])\n",
    "#%time indices2 = np.nonzero(next_item)[0]\n",
    "\n",
    "# step 3\n",
    "# get the indices in common between the two\n",
    "#%time common_indices = list(set.intersection(set(indices), set(indices2)))\n",
    "%time common_indices = list(set.intersection(set(indices), set(indices2)))\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "print(end-start)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6618477",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Method - TF Matmul"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "198de629",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# step 4\n",
    "%time reduced_item1 = single_item[common_indices].reshape(1,-1)\n",
    "%time reduced_item2 = next_item[common_indices].reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "797b8eb5",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "reduced_item1.shape, reduced_item2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf2a8103",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# step 5\n",
    "%time a = tf.constant(reduced_item1, dtype=tf.float32)\n",
    "%time b = tf.constant(reduced_item2, dtype=tf.float32)\n",
    "%time a = tf.nn.l2_normalize(a)\n",
    "%time b = tf.nn.l2_normalize(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09e3beb7",
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# step 6\n",
    "item_similarity = round(float(tf.matmul(a, b)),2)\n",
    "item_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "276e6f84",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "\n",
    "# Step one\n",
    "# make the single item matrix for the one item\n",
    "%time single_item = matrix_array[:, item1]\n",
    "# get the indices where the item is nonzero\n",
    "%time indices = np.nonzero(single_item)[0]\n",
    "#%time indices = np.nonzero(single_item)[0]\n",
    "\n",
    "# Step 2\n",
    "# make the single item matrix for the next item\n",
    "%time next_item = matrix_array[:, item2]\n",
    "# get the indices where the item is nonzero\n",
    "%time indices2 = np.nonzero(next_item)[0]\n",
    "#%time indices2 = np.nonzero(next_item)[0]\n",
    "\n",
    "# step 3\n",
    "# get the indices in common between the two\n",
    "%time common_indices = list(set.intersection(set(indices), set(indices2)))\n",
    "\n",
    "# step 4\n",
    "%time reduced_item1 = single_item[common_indices].reshape(1,-1)\n",
    "%time reduced_item2 = next_item[common_indices].reshape(-1,1)\n",
    "\n",
    "# step 5\n",
    "%time a = tf.constant(reduced_item1, dtype=tf.float32)\n",
    "%time b = tf.constant(reduced_item2, dtype=tf.float32)\n",
    "%time a = tf.nn.l2_normalize(a)\n",
    "%time b = tf.nn.l2_normalize(b)\n",
    "\n",
    "# step 6\n",
    "%time item_similarity = float(tf.matmul(a, b))\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "print(end-start)\n",
    "\n",
    "print(item_similarity)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab1339b4",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Method- TF cosine_distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59c459b7",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "\n",
    "# Step one\n",
    "# make the single item matrix for the one item\n",
    "%time single_item = matrix_array[:, item1]\n",
    "# get the indices where the item is nonzero\n",
    "%time indices = np.nonzero(single_item)[0]\n",
    "\n",
    "# Step 2\n",
    "# make the single item matrix for the next item\n",
    "%time next_item = matrix_array[:, item2]\n",
    "# get the indices where the item is nonzero\n",
    "%time indices2 = np.nonzero(next_item)[0]\n",
    "\n",
    "# step 3\n",
    "# get the indices in common between the two\n",
    "%time common_indices = list(set.intersection(set(indices), set(indices2)))\n",
    "\n",
    "# step 4\n",
    "%time reduced_item1 = single_item[common_indices]\n",
    "%time reduced_item2 = next_item[common_indices]\n",
    "\n",
    "# step 5\n",
    "%time a = tf.constant(reduced_item1, dtype=tf.float32)\n",
    "%time b = tf.constant(reduced_item2, dtype=tf.float32)\n",
    "%time a = tf.nn.l2_normalize(a)\n",
    "%time b = tf.nn.l2_normalize(b)\n",
    "\n",
    "# step 6\n",
    "%time item_similarity = 1-cosine_distance(a, b, axis=0).numpy()\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "print(end-start)\n",
    "\n",
    "print(item_similarity)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6620dfcd",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Method - TF Cosine similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23d707c6",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "cos_sim = CosineSimilarity()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97dbd084",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "\n",
    "# Step one\n",
    "# make the single item matrix for the one item\n",
    "%time single_item = matrix_array[:, item1]\n",
    "# get the indices where the item is nonzero\n",
    "%time indices = np.nonzero(single_item)[0]\n",
    "\n",
    "# Step 2\n",
    "# make the single item matrix for the next item\n",
    "%time next_item = matrix_array[:, item2]\n",
    "# get the indices where the item is nonzero\n",
    "%time indices2 = np.nonzero(next_item)[0]\n",
    "\n",
    "# step 3\n",
    "# get the indices in common between the two\n",
    "%time common_indices = list(set.intersection(set(indices), set(indices2)))\n",
    "\n",
    "# step 4\n",
    "%time reduced_item1 = single_item[common_indices]\n",
    "%time reduced_item2 = next_item[common_indices]\n",
    "\n",
    "# step 5\n",
    "%time a = tf.constant(reduced_item1, dtype=tf.float32)\n",
    "%time b = tf.constant(reduced_item2, dtype=tf.float32)\n",
    "%time a = tf.nn.l2_normalize(a)\n",
    "%time b = tf.nn.l2_normalize(b)\n",
    "\n",
    "# step 6\n",
    "%time item_similarity = cos_sim(a, b).numpy()\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "print(end-start)\n",
    "\n",
    "print(item_similarity)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7024e7e7",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Make item-item calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6828d3f6",
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "number_of_games = np.arange(0, matrix_array.shape[1], 1)\n",
    "\n",
    "time_test = []\n",
    "\n",
    "\n",
    "# Load the storage dictionary for this block\n",
    "with open('item_similarities/similarity_storage_real_scaled.json') as json_file:\n",
    "    base_items_storage = json.load(json_file) \n",
    "    \n",
    "# for each user block in the block_indices_lookup. The user blocks are integers from 1-20\n",
    "for game in number_of_games[:500]:\n",
    "    \n",
    "    print(\"\\nStarting game: \"+str(game))\n",
    "    start = time.time()\n",
    "    \n",
    "    gameid_1 = gameids_columnorder[game]\n",
    "         \n",
    "    # make the single user matrix for the one user\n",
    "    single_item = matrix_array[:, game]\n",
    "    # get the indices where the user is nonzero\n",
    "    indices = np.nonzero(single_item)[0]\n",
    "    checkpoint1 = time.time()\n",
    "    #print(\"Reduce item 1: \"+str(checkpoint1-start))\n",
    "        \n",
    "        \n",
    "    for game2 in number_of_games:\n",
    "            \n",
    "        if game == game2:\n",
    "            continue\n",
    "        \n",
    "        gameid_2 = gameids_columnorder[game2]\n",
    "        \n",
    "        if gameid_2 in base_items_storage[gameid_1]:\n",
    "            pass\n",
    "        \n",
    "        else:\n",
    "                        \n",
    "            next_item =  matrix_array[:, game2]\n",
    "            indices2 = np.nonzero(next_item)[0]\n",
    "            checkpoint3 = time.time()\n",
    "            #print(\"\\nReduce item 2: \"+str(checkpoint3-checkpoint1))\n",
    "            \n",
    "            common_indices = list(set.intersection(set(indices), set(indices2)))\n",
    "            \n",
    "            if len(common_indices) < 3:\n",
    "                checkpoint7 = time.time()\n",
    "                item_similarity = 0\n",
    "            \n",
    "            else:\n",
    "                \n",
    "                reduced_item1 = single_item[common_indices]\n",
    "                reduced_item2 = next_item[common_indices]\n",
    "                checkpoint4 = time.time()\n",
    "                #print(\"Reduce both to common indices: \"+str(checkpoint4-checkpoint3))\n",
    "        \n",
    "                a = tf.constant(reduced_item1, dtype=tf.float32)\n",
    "                b = tf.constant(reduced_item2, dtype=tf.float32)\n",
    "                checkpoint5 = time.time()\n",
    "                #print(\"Load to tensors: \"+str(checkpoint5-checkpoint4))\n",
    "            \n",
    "                a = tf.nn.l2_normalize(a)\n",
    "                b = tf.nn.l2_normalize(b)\n",
    "                checkpoint6 = time.time()\n",
    "                #print(\"Normalize tensors: \"+str(checkpoint6-checkpoint5))\n",
    "            \n",
    "                item_similarity = 1-cosine_distance(a, b, axis=0).numpy()\n",
    "                checkpoint7 = time.time()\n",
    "                #print(\"Get similarity: \"+str(checkpoint7-checkpoint6))\n",
    "            \n",
    "            base_items_storage[gameid_1][gameid_2] = item_similarity\n",
    "            base_items_storage[gameid_2][gameid_1] = item_similarity\n",
    "            checkpoint8 = time.time()\n",
    "            #print(\"Store similarity: \"+str(checkpoint8-checkpoint7))\n",
    "    \n",
    "    end = time.time()\n",
    "    \n",
    "    print(\"Time for this game: \"+str(end-start)+'\\n')        \n",
    "    \n",
    "# save dictionary\n",
    "with open('item_similarities/similarity_storage_real_scaled.json', 'w') as convert_file:\n",
    "    convert_file.write(json.dumps(base_items_storage))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95335d27",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# save dictionary\n",
    "with open('item_similarities/similarity_storage_real_scaled.json', 'w') as convert_file:\n",
    "    convert_file.write(json.dumps(base_items_storage))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fe08e6a",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "base_items_storage['84776']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cbb8993",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Code Work - User to User Tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d526756",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Preparing the user blocks and user storage dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "048f1ad5",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "'''users_list = list(larger_matrix.index)\n",
    "\n",
    "user_blocks_lookup = {}\n",
    "\n",
    "chunk_size = int(np.ceil(matrix_array.shape[0]/20))\n",
    "\n",
    "start = 0\n",
    "incrementer = 0\n",
    "\n",
    "while start < matrix_array.shape[0]:\n",
    "    \n",
    "    end = start + chunk_size\n",
    "    incrementer += 1\n",
    "    \n",
    "    user_blocks_lookup[incrementer] = users_list[start:end]\n",
    "\n",
    "    start += chunk_size\n",
    "\n",
    "print(\"\\nLookup dictionary complete\")'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb055117",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "'''user_id_lookup = {}\n",
    "\n",
    "increment=0\n",
    "for user in users_list:\n",
    "    \n",
    "    increment+=1\n",
    "    user_id_lookup[increment] = user'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "081f2178",
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "'''block_indices_lookup = {}\n",
    "\n",
    "start = 0\n",
    "incrementer = 0\n",
    "\n",
    "while start < matrix_array.shape[0]:\n",
    "    \n",
    "    end = start + chunk_size\n",
    "    incrementer += 1\n",
    "    \n",
    "    block_indices_lookup[incrementer] = {}\n",
    "    block_indices_lookup[incrementer]['Start'] = start\n",
    "    block_indices_lookup[incrementer]['End'] = end\n",
    "    \n",
    "    start += chunk_size\n",
    "\n",
    "print(\"\\nLookup dictionary complete\")'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc2d6e5b",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "del larger_matrix\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "059ade53",
   "metadata": {
    "hidden": true
   },
   "source": [
    "ONLY RUN THIS AGAIN IF THE USER LIST CHANGES !!!!!  THIS WILL RESET ALL STORAGE DICTIONARIES ON DISK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ead2ec6",
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "'''max_range = len(user_blocks_lookup)+1\n",
    "\n",
    "for item in np.arange(1,max_range,1):\n",
    "    \n",
    "    storage_dict = {}\n",
    "    \n",
    "    for user in user_blocks_lookup[item]:\n",
    "        storage_dict[user] = {}\n",
    "    \n",
    "    # save dictionary\n",
    "    with open('synthetic_ratings/similarity_storage_synth_items2k'+str(item)+'.json', 'w') as convert_file:\n",
    "        convert_file.write(json.dumps(storage_dict))\n",
    "    \n",
    "    del storage_dict'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fdf7c5e",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "precompute_matrix = matrix_array[:134000].T\n",
    "precompute_matrix2 = matrix_array[134000:].T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5f23f16",
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# TEMP STUFF\n",
    "\n",
    "block_indices_lookup = {}\n",
    "\n",
    "start = 0\n",
    "incrementer = 0\n",
    "\n",
    "\n",
    "end = start + chunk_size\n",
    "    \n",
    "block_indices_lookup[1] = {}\n",
    "block_indices_lookup[1]['Start'] = start\n",
    "block_indices_lookup[1]['End'] = end\n",
    "\n",
    "print(\"\\nLookup dictionary complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6d5dcc3",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def process_user_block(array_chunk_a, matrix, indices):\n",
    "    \n",
    "    this_start = time.time()\n",
    "    array_chunk_b = (matrix[indices, :])#.astype('float32'))\n",
    "    checkpoint = time.time()\n",
    "    \n",
    "    a = tf.constant(array_chunk_a, dtype=tf.float32)\n",
    "    \n",
    "    b = tf.constant(array_chunk_b, dtype=tf.float32)\n",
    "       \n",
    "    checkpoint1 = time.time()      \n",
    "    #print(str(checkpoint1-checkpoint)+\" Loaded into Tensors\")\n",
    "\n",
    "    a = tf.nn.l2_normalize(a, 1)\n",
    "    b = tf.nn.l2_normalize(b, 0)\n",
    "    \n",
    "    checkpoint2 = time.time()      \n",
    "    #print(str(checkpoint2-checkpoint1)+\" normalized\")    \n",
    "    \n",
    "    similarities = tf.matmul(a, b)\n",
    "    checkpoint3 = time.time()\n",
    "    #print(str(checkpoint3-checkpoint2)+\" Got Similarity Scores\")\n",
    "           \n",
    "    user_similarities = similarities.numpy().reshape(-1,1)\n",
    "        \n",
    "    return user_similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ddc9d35",
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "time_test = []\n",
    "\n",
    "# for each user block in the block_indices_lookup. The user blocks are integers from 1-20\n",
    "for user_block in block_indices_lookup:\n",
    "       \n",
    "    print(\"Starting block \"+str(user_block))\n",
    "    \n",
    "    # Get the start and end indexes for the block\n",
    "    starting_block_indexes = block_indices_lookup[user_block]\n",
    "    base_start = starting_block_indexes['Start'] # starting user\n",
    "    base_end = starting_block_indexes['End'] # ending user\n",
    "    \n",
    "    # Load the storage dictionary for this block\n",
    "    with open('real_ratings/similarity_storage_real_'+str(user_block)+'.json') as json_file:\n",
    "        base_users_storage = json.load(json_file) \n",
    "    \n",
    "    # only do the user ids in this block, then save to the fils\n",
    "    for user_id in np.arange(base_start, 1001, 1):#base_end, 1):\n",
    "        print(user_id)\n",
    "        \n",
    "        user_name = user_id_lookup[user_id+1]\n",
    "        #print(user_name)\n",
    "    \n",
    "        # log start time\n",
    "        #print(\"Making matrices\")\n",
    "        start = time.time()\n",
    "               \n",
    "        # make the single user matrix for the one user\n",
    "        single_user = matrix_array[user_id].reshape(1,-1)\n",
    "        # get the indices where the user is nonzero\n",
    "        indices = list(np.nonzero(single_user)[1])\n",
    "        # make the user with only the nonzero indices\n",
    "        array_chunk_a = (single_user[:, indices])#.astype('float32')\n",
    "        #normalize_a = normalize(array_chunk_a, axis=1)\n",
    "        checkpoint = time.time()\n",
    "        #print(str(checkpoint-start)+\" Processed single user\")\n",
    "        \n",
    "        #process_user_block(a, precompute_matrix, indices)\n",
    "        user_similarities_1 = process_user_block(array_chunk_a, precompute_matrix, indices)\n",
    "        user_similarities_2 = process_user_block(array_chunk_a, precompute_matrix2, indices)\n",
    "        #user_similarities = process_user_block(array_chunk_a, precompute_matrix, indices)\n",
    "        \n",
    "        \n",
    "        checkpoint3 = time.time()\n",
    "        user_similarities = np.append(user_similarities_1, user_similarities_2)   \n",
    "        max_spot = np.argmax(user_similarities)\n",
    "        mean_spot = np.median(user_similarities)\n",
    "        user_similarities[max_spot] = mean_spot\n",
    "        scaler = MinMaxScaler(feature_range=(-1,1))\n",
    "        user_similarities = scaler.fit_transform(user_similarities.reshape(-1,1)).ravel()\n",
    "        #user_similarities = list(np.round(user_similarities, 2).ravel())\n",
    "        \n",
    "        checkpoint4 = time.time()\n",
    "        #print(str(checkpoint4-checkpoint3)+\" Processed/Scaled Similarity scores\") \n",
    "        \n",
    "        over75 = list((user_similarities >= .6).nonzero()[0])\n",
    "        under75 = list((user_similarities <= -.6).nonzero()[0])\n",
    "        all_comps = over75 + under75\n",
    "        \n",
    "        for item in all_comps:\n",
    "            item = int(item)\n",
    "            base_users_storage[user_name][item] = round(float(user_similarities[item]), 2)\n",
    "          \n",
    "        checkpoint5 = time.time()\n",
    "        #print(str(checkpoint5-checkpoint4)+\" Stored scores in dictionary\\n\")\n",
    "        \n",
    "        end = time.time()\n",
    "        elapsed = end-start\n",
    "        #print(str(elapsed)+' seconds elapsed for this user\\n\\n')\n",
    "        time_test.append(elapsed)\n",
    "    \n",
    "    print(\"Saving dictionary for this set of users\")\n",
    "    # save dictionary\n",
    "    with open('real_ratings/similarity_storage_real_'+str(user_block)+'.json', 'w') as convert_file:\n",
    "        convert_file.write(json.dumps(base_users_storage))\n",
    "    \n",
    "    avg_time = mean(time_test)\n",
    "    print(\"Average time per user: \"+str(avg_time))\n",
    "    \n",
    "    del base_users_storage\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d038f44",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "base_users_storage['cfarrell']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc913a32",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "len(base_users_storage['Torsten'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68819867",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Make smaller ratings blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69d58dfe",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Opening JSON file\n",
    "with open('real_ratings/user_ratings_unscaled.json') as json_file:\n",
    "    user_ratings = json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a4fd98e",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "all_users = list(user_ratings.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eac2a65",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "len(all_users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "420130aa",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "user_block_1 = all_users[:40000]\n",
    "user_block_2 = all_users[40000:80000]\n",
    "user_block_3 = all_users[80000:120000]\n",
    "user_block_4 = all_users[120000:160000]\n",
    "user_block_5 = all_users[160000:200000]\n",
    "user_block_6 = all_users[200000:240000]\n",
    "user_block_7 = all_users[240000:]\n",
    "\n",
    "user_blocks = [user_block_1, user_block_2, user_block_3, user_block_4, user_block_5, user_block_6, user_block_7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e284a26",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "iteration = 0\n",
    "\n",
    "for block in user_blocks:\n",
    "    \n",
    "    iteration += 1\n",
    "    \n",
    "    print(\"Starting block \"+str(iteration))\n",
    "    \n",
    "    block_of_users = {key: value for key, value in user_ratings.items() if key in block}\n",
    "    \n",
    "    #for scaled only:\n",
    "    for person in block_of_users:\n",
    "        #user_mean = mean(block_of_users[person].values())\n",
    "        for item in block_of_users[person]:\n",
    "            #new_value = round((block_of_users[person][item] - user_mean), 2)\n",
    "            new_value = block_of_users[person][item]\n",
    "            block_of_users[person][item] = new_value\n",
    "    \n",
    "    # save dictionary\n",
    "    with open('real_ratings/user_ratings_block_unscaled_'+str(iteration)+'.json', 'w') as convert_file:\n",
    "        convert_file.write(json.dumps(block_of_users))\n",
    "        \n",
    "    del block_of_users\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76180a55",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "del user_blocks\n",
    "del user_ratings\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d28d9172",
   "metadata": {},
   "source": [
    "# Deprecated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ee8c631",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make dataframe from synthetic sort and melt to longform\n",
    "synthetic_user_ratings = pd.DataFrame.from_dict(synthetic_users_dictionary)\n",
    "synthetic_user_ratings.reset_index(inplace=True)\n",
    "synthetic_user_ratings.rename(columns={'index':'BGGId', user:'Rating'}, inplace=True)\n",
    "synthetic_user_ratings['Rating'] = synthetic_user_ratings['Rating']+user_mean\n",
    "    \n",
    "    \n",
    "synthetic_user_ratings = pd.DataFrame.from_dict(synthetic_users_dictionary).T\n",
    "synthetic_user_ratings.reset_index(inplace=True)\n",
    "synthetic_user_ratings.rename(columns={'index':'UserID'}, inplace=True)\n",
    "synthetic_user_ratings_long = synthetic_user_ratings.melt(id_vars='UserID', var_name='BGGId', value_name='Rating').dropna()\n",
    "synthetic_user_ratings_long.sort_values('UserID', inplace=True)\n",
    "synthetic_user_ratings_long\n",
    "    \n",
    "# save longform\n",
    "synthetic_user_ratings_long.to_pickle('synthetic_ratings_new_scraper/synthetic_ratings_'+path+'_'+number+'.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45d17159",
   "metadata": {},
   "outputs": [],
   "source": [
    "def produce_synthetic_ratings(user, temp_users_dictionary, num_ratings_create):\n",
    "    '''\n",
    "    Takes in a dictionary of user's ratings and the number of ratings to synthesize\n",
    "    Synthesizes ratings and creates a dictionary of all synthesized ratings for the user\n",
    "    Returns synthesized ratings\n",
    "    \n",
    "    Inputs:\n",
    "    user: the user id to create ratings for\n",
    "    temp_users_dictionary: dictionary of specific user's real ratings\n",
    "    num_ratings_create : simple number. # Ratings to make in the run.\n",
    "    \n",
    "    Outputs:\n",
    "    user_comps_dict : dictionary of synthesized ratings specifically for user\n",
    "    '''\n",
    "    \n",
    "    print(\"Producing items for user\")\n",
    "    \n",
    "    # start at iteration 0\n",
    "    iteration = 0\n",
    "    \n",
    "    # set up dict to store all specific comps for this user\n",
    "    users_comp_dict = {}\n",
    "\n",
    "    # populate the comps with the user's baseline items\n",
    "    for item in temp_users_dictionary:  \n",
    "        users_comp_dict[item] = [1, 1, item, 0, 0, temp_users_dictionary[item]]\n",
    "        #overall confidence, this item similarity, item, iteration, degrees away, item name\n",
    "       \n",
    "    # while the list of items that the user rated is < the number of ratings needed:\n",
    "    while len(users_comp_dict.keys()) < num_ratings_create:\n",
    "        \n",
    "        users_rated_items = list(temp_users_dictionary.keys())\n",
    "        \n",
    "        iteration += 1 # advance the iteration\n",
    "        \n",
    "        new_items = [] # make a list to hold the items for this iteration        \n",
    "        \n",
    "        # for each rated item:\n",
    "        for rated in users_rated_items:\n",
    "            \n",
    "            print(\"\\nCurrent item: \"+str(rated))\n",
    "            # get rating for current item\n",
    "            rated_rating = temp_users_dictionary[rated]\n",
    "            print(rated_rating)\n",
    "        \n",
    "            # get current best comp:\n",
    "            current_position = 0\n",
    "            current_comp = game_comps_byid_lookup[rated][0][current_position]\n",
    "            \n",
    "            while current_comp in new_items:\n",
    "                \n",
    "                # increment position\n",
    "                current_position+=1 \n",
    "                \n",
    "                if current_position >= 21923:\n",
    "                    #print(current_position)\n",
    "                    break\n",
    "                                                        \n",
    "                else:\n",
    "                    # reset current comp to new position new_items\n",
    "                    current_comp = game_comps_byid_lookup[rated][0][current_position]\n",
    "\n",
    "                    # continue back to check\n",
    "                    continue\n",
    "            \n",
    "            # any time the current comp is in users_rated_items already:\n",
    "            while current_comp in users_comp_dict.keys():\n",
    "                \n",
    "                # increment position\n",
    "                current_position+=1 \n",
    "                \n",
    "                if current_position >= 21923:\n",
    "                    #print(current_position)\n",
    "                    break\n",
    "                                    \n",
    "                else:\n",
    "                \n",
    "                    # reset current comp to new position users_comp_dict\n",
    "                    current_comp = game_comps_byid_lookup[rated][0][current_position]\n",
    "\n",
    "                    # continue back to check\n",
    "                    continue\n",
    "            \n",
    "            # The next section activates once the current comp is not already in the user's rated items\n",
    "            \n",
    "            if current_position >= 21923:\n",
    "                #print(current_position)\n",
    "                break\n",
    "                            \n",
    "            else:\n",
    "            \n",
    "            \n",
    "                # getting similarity of the current comp\n",
    "                comp_similarity = game_comps_byid_lookup[rated][1][current_position]\n",
    "                print(current_position)\n",
    "                print(comp_similarity)\n",
    "              \n",
    "                # get the synthetic rating for the item by taking the rating of the base item * similarity\n",
    "                synthetic_rating = rated_rating * comp_similarity\n",
    "                print(synthetic_rating)\n",
    "                \n",
    "                # get the overall confidence of this rating \n",
    "                # confidence = confidence of prior item * similarity of current item\n",
    "                confidence = users_comp_dict[rated][0] * comp_similarity\n",
    "                degrees = users_comp_dict[rated][4] + 1\n",
    "\n",
    "                # add this item to the list of new items we are adding to the ratings this round\n",
    "                new_items.append(current_comp)\n",
    "            \n",
    "                # make the user's comp dict\n",
    "                users_comp_dict[current_comp] = [confidence, comp_similarity, rated, iteration, degrees, synthetic_rating]\n",
    "            \n",
    "                # update the temporary dictionary with the synthetic rating for the item\n",
    "                temp_users_dictionary[current_comp] = synthetic_rating\n",
    "        \n",
    "        continue\n",
    "\n",
    "    print(\"End length of rated items is \"+str(len(users_comp_dict))+'\\n')\n",
    "\n",
    "    return users_comp_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10cf1052",
   "metadata": {},
   "outputs": [],
   "source": [
    "#user_matrix = pd.read_pickle('data_cleaned/ratings_matrix_cleaned_03.pkl')\n",
    "#user_matrix = user_matrix.T\n",
    "#user_matrix.index = user_matrix.index.astype('int32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5554cf0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# run the data synthesizer for each of the 6 ratings matrix files\n",
    "process_to_synthetic(item, num_ratings_create, desired_ratings, game_ids, '250')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb2bfc33",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_user(user_items, user, game_ids):\n",
    "    '''\n",
    "    Takes in user's rated items, a the username, and a list of game_ids\n",
    "    Get the mean for the user\n",
    "    Builds a list of user's rated items and subtracts user mean from all ratings\n",
    "    Builds a corresponding list of game ids for the rated games\n",
    "    Gets intersection of user's rated ids with the overall game_ids\n",
    "    Stores user game_id:rating in user ratings dictionary \n",
    "    Returns the user dictionary\n",
    "    \n",
    "    Inputs: \n",
    "    user_items: dataframe column of user's rated items\n",
    "    user: user to retrieve\n",
    "    game_ids: the game_ids we are using in our recommender\n",
    "    \n",
    "    Outputs:\n",
    "    overall_user: user dictionary with user's ratings\n",
    "    '''\n",
    "    \n",
    "    # get the mean rating for that user\n",
    "    user_mean = user_items.mean()\n",
    "    \n",
    "    # normalize the ratings for that user by subtracting their mean from all ratings, store in list\n",
    "    game_ratings_normed =  list(user_items - user_mean)\n",
    "    \n",
    "    # Get a list of all of the game IDs that the user rated\n",
    "    users_game_ids = list(user_items.index)\n",
    "    \n",
    "    # get the set of usable game ids\n",
    "    game_ids_set = set(game_ids).intersection(set(users_game_ids))\n",
    "    \n",
    "    # make user storage dictionary\n",
    "    user_ratings = {}\n",
    "    \n",
    "    # for the key/value pairs of game_ids and normalized ratings\n",
    "    for key, value in zip(users_game_ids, game_ratings_normed):\n",
    "        user_ratings[key] = value\n",
    "    \n",
    "    # make a dictionary to store the intersected ratings\n",
    "    set_dictionary = {}\n",
    "    \n",
    "    # for each matching key, value in game_ids and game_ratings for the user\n",
    "    for item in game_ids_set:\n",
    "        set_dictionary[item] = user_ratings[item]\n",
    "\n",
    "    # store the user's ratings\n",
    "    overall_user = set_dictionary\n",
    "    \n",
    "    return overall_user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "168fd9a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_matrix_to_synthetic(path, num_ratings_create, desired_ratings, game_ids, number):\n",
    "    '''\n",
    "    Process a user matrix and create synthetic data for each user in the matrix\n",
    "    \n",
    "    Inputs:\n",
    "    Path: path appendation for file\n",
    "    num_ratings_create: The total number of minimum ratings per user\n",
    "    desired_ratings: the needed number of ratings per user\n",
    "    '''\n",
    "    \n",
    "    # load and transpose data frame\n",
    "    user_matrix = pd.read_pickle('data_cleaned/ratings_matrix_cleaned_'+path+'.pkl')\n",
    "    user_matrix.drop_duplicates(keep='first', inplace=True)\n",
    "    user_matrix = user_matrix.T\n",
    "    user_matrix.index = user_matrix.index.astype('int32')\n",
    "    \n",
    "    # set up a synthetic ratings dictionary to store the users and ratings\n",
    "    synthetic_users_dictionary = {}\n",
    "\n",
    "    # for each user in the test matrix:\n",
    "    for user in user_matrix.columns:\n",
    "   \n",
    "        print(\"Starting user \"+user)\n",
    "        \n",
    "        user_items = user_matrix[user].dropna(axis=0)\n",
    "        \n",
    "        # copy the current user dictionary to a temp storage dictionary that we can manipulate\n",
    "        synthetic_users_dictionary[user] = get_user(user_items, user, game_ids)\n",
    "        temp_users_dictionary = copy.deepcopy(synthetic_users_dictionary[user])\n",
    "    \n",
    "        # get the original number of ratings by this user\n",
    "        original_num_ratings = len(temp_users_dictionary)\n",
    "        print(\"User starts with \"+str(original_num_ratings)+\" ratings\")\n",
    "    \n",
    "        # call function to produce synthetic ratings\n",
    "        user_comps_dict = produce_synthetic_ratings(user, temp_users_dictionary, num_ratings_create)\n",
    "        # call sort function for top synthetic ratings\n",
    "        sort_synthetic_ratings(user, synthetic_users_dictionary, user_comps_dict, original_num_ratings, desired_ratings)\n",
    "    \n",
    "    # make dataframe from synthetic sort and melt to longform\n",
    "    synthetic_user_ratings = pd.DataFrame.from_dict(synthetic_users_dictionary).T\n",
    "    synthetic_user_ratings.reset_index(inplace=True)\n",
    "    synthetic_user_ratings.rename(columns={'index':'UserID'}, inplace=True)\n",
    "    synthetic_user_ratings_long = synthetic_user_ratings.melt(id_vars='UserID', var_name='BGGId', value_name='Rating').dropna()\n",
    "    synthetic_user_ratings_long.sort_values('UserID', inplace=True)\n",
    "    synthetic_user_ratings_long\n",
    "    \n",
    "    # save longform\n",
    "    synthetic_user_ratings_long.to_pickle('synthetic_ratings_new_scraper/synthetic_ratings_'+path+'_'+number+'.pkl')\n",
    "    \n",
    "    # save dictionary\n",
    "    with open('synthetic_ratings_new_scraper/users_dump_syntheticratings'+path+'_'+number+'.json', 'w') as convert_file:\n",
    "        convert_file.write(json.dumps(synthetic_users_dictionary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c86f2ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_synthetic_ratings(user, synthetic_users_dictionary, user_comps_dict, original_num_ratings, desired_ratings):\n",
    "    '''\n",
    "    Takes the user's synthesized comps dict, the original number of ratings the user made, \n",
    "    and the desired number of ratings the user needs.\n",
    "    Creates a df sorting the synthesized ratings by confidence level, \n",
    "    keeping the highest confidence if an item was recommended more than once.\n",
    "    Evaluates number of ratings needed to reach 500 and keeps only that many ratings with the highest confidence.\n",
    "    For each item kept, logs the synthetic rating to the user;s dictionary\n",
    "    \n",
    "    Inputs:\n",
    "    user: specific user to sort\n",
    "    synthetic_users_dictionary: reference to the dictionary of synthesized items\n",
    "    user_comps_dict: dictionary of synthesized ratings specifically for user\n",
    "    original_num_ratings: The number of ratings the user actually rated\n",
    "    desired_ratings: the number of ratings needed by the user\n",
    "    \n",
    "    '''\n",
    "    print(\"Sorting user items\")\n",
    "    \n",
    "    # showing synthetic ratings only\n",
    "    user_comps_df = pd.DataFrame(user_comps_dict.values(), index=user_comps_dict.keys(), columns=['OverallConfidence', 'SimtoLast', 'RecFrom', 'Iteration', 'DegreesAway', 'SyntheticRating']).sort_values('OverallConfidence', ascending=False).drop_duplicates(keep='first')\n",
    "    \n",
    "    # get a list of the ratings to keep (past the real ratings)\n",
    "    keep_items = sorted(list(user_comps_df[:desired_ratings].index))\n",
    "\n",
    "    # for each item that we keep,\n",
    "    for item in keep_items:\n",
    "    \n",
    "        # add the rating to the real storage dictionary\n",
    "        synthetic_users_dictionary[user][item] = user_comps_dict[item]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a861416",
   "metadata": {},
   "source": [
    "## Deprecated Matrix Calculations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9500f98",
   "metadata": {},
   "source": [
    "### Using Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73f4413c",
   "metadata": {},
   "outputs": [],
   "source": [
    "larger_matrix = pd.read_pickle('synthetic_ratings/users_synthetic_2193_fullmatrix.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf518050",
   "metadata": {},
   "outputs": [],
   "source": [
    "larger_matrix.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecc9d5ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "users = list(larger_matrix.index)\n",
    "users[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c14e553",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_lookup_table = {}\n",
    "\n",
    "user_key = -1\n",
    "\n",
    "for user in users:\n",
    "    \n",
    "    user_key += 1\n",
    "    \n",
    "    user_lookup_table[int(user_key)] = users[user_key]\n",
    "\n",
    "# save dictionary\n",
    "with open('user_lookup_table.json', 'w') as convert_file:\n",
    "    convert_file.write(json.dumps(user_lookup_table))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa11f2ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "%time values1 = larger_matrix.loc['Torsten'].values\n",
    "values1[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9748a391",
   "metadata": {},
   "outputs": [],
   "source": [
    "%time values2 = larger_matrix.loc['mitnachtKAUBO-I'].values\n",
    "values2[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b468c7de",
   "metadata": {},
   "outputs": [],
   "source": [
    "%time spatial.distance.cosine(values1,values2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b68c5b74",
   "metadata": {},
   "outputs": [],
   "source": [
    "del similarity_dictionary\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e40f097c",
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_dictionary = {}\n",
    "\n",
    "for user in users:\n",
    "    \n",
    "    similarity_dictionary[user] = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e60af8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for user in users[:1]:\n",
    "    \n",
    "    start = time.time()\n",
    "    user_values = larger_matrix.loc[user].values\n",
    "    \n",
    "    for other_user in users:\n",
    "        \n",
    "        if user in similarity_dictionary[other_user]:\n",
    "            continue\n",
    "        \n",
    "        else:\n",
    "        \n",
    "            other_user_values = larger_matrix.loc[other_user].values\n",
    "            similarity = 1 - spatial.distance.cosine(user_values,other_user_values)\n",
    "            similarity_dictionary[user][other_user] = similarity\n",
    "            similarity_dictionary[other_user][user] = similarity\n",
    "    \n",
    "    end = time.time()\n",
    "    print(str(end-start)+' seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84f603a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "del larger_matrix\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da435220",
   "metadata": {},
   "source": [
    "### Using Numpy Arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eac0ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix_array = larger_matrix.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7e32a13",
   "metadata": {},
   "outputs": [],
   "source": [
    "del larger_matrix\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdf5422a",
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14b3035c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%time values1 = matrix_array[0]\n",
    "values1[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dba2758c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%time values2 = matrix_array[1]\n",
    "values2[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2714ff86",
   "metadata": {},
   "outputs": [],
   "source": [
    "%time spatial.distance.cosine(values1,values2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c0e3ccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "%time similarities = np.matmul(matrix_array[0:10000], matrix_array[0:10000].T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e719c4e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "similarities[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1585ceaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "del similarities\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c98abc44",
   "metadata": {},
   "outputs": [],
   "source": [
    "%time similarities = np.dot(matrix_array[0:10000], matrix_array[0:10000].T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "282e5125",
   "metadata": {},
   "outputs": [],
   "source": [
    "similarities[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15f7a99a",
   "metadata": {},
   "outputs": [],
   "source": [
    "del similarities\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "154e6aa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%time similarities = matrix_array[0:10000]@matrix_array[0:10000].T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "076683a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "similarities[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feaebdef",
   "metadata": {},
   "outputs": [],
   "source": [
    "del similarities\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8042285",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6041eb38",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2abdbb83",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "241c87f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "len_users = len(users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0674b3d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "del similarity_dictionary\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c587d57",
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_dictionary = {}\n",
    "\n",
    "for user in np.arange(0, len_users, 1):\n",
    "    \n",
    "    similarity_dictionary[user] = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45805530",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(similarity_dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfce4de6",
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_dictionary[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d997482",
   "metadata": {},
   "outputs": [],
   "source": [
    "for user in np.arange(0, len_users, 1)[:1]:\n",
    "    \n",
    "    start = time.time()\n",
    "    user_values = matrix_array[user].reshape(-1,1)\n",
    "    \n",
    "    other_matrix = matrix_array[user+1:]\n",
    "    \n",
    "    similarities = cosine_similarity(other_matrix, user_values)\n",
    "    \n",
    "    end = time.time()\n",
    "    print(str(end-start)+' seconds')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cca6557",
   "metadata": {},
   "outputs": [],
   "source": [
    "similarities.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8355f899",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ed3a3c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72fbacab",
   "metadata": {},
   "outputs": [],
   "source": [
    "for user in np.arange(0, len_users, 1)[:3]:\n",
    "    \n",
    "    start = time.time()\n",
    "    user_values = matrix_array[user]\n",
    "    \n",
    "    for other_user in np.arange(0, len_users, 1):\n",
    "        \n",
    "        if user in similarity_dictionary[other_user]:\n",
    "            continue\n",
    "        \n",
    "        else:\n",
    "        \n",
    "            other_user_values = matrix_array[other_user]\n",
    "            similarity = 1 - spatial.distance.cosine(user_values,other_user_values)\n",
    "            similarity_dictionary[user][other_user] = similarity\n",
    "            similarity_dictionary[other_user][user] = similarity\n",
    "    \n",
    "    end = time.time()\n",
    "    print(str(end-start)+' seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b532ab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_dictionary[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a702ca58",
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_dictionary[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "654f4075",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccfb189e",
   "metadata": {},
   "outputs": [],
   "source": [
    "del matrix_array\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0933cd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#larger_matrix_T = pd.read_pickle('synthetic_ratings/users_synthetic_2193_fullmatrixT.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66586640",
   "metadata": {},
   "source": [
    "## Different ways to make calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56b71814",
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix_sparsed = pd.read_pickle('synthetic_ratings/users_synthetic_2193_sparsematrix.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9557c865",
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix_sparsed.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c2f0a9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix_sparsed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d710272a",
   "metadata": {},
   "outputs": [],
   "source": [
    "users = list(matrix_sparsed.index)\n",
    "users[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32488712",
   "metadata": {},
   "source": [
    "### Chunks, sparse non-normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09f5cddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "%time sparse_matrix = csr_matrix(matrix_sparsed.sparse.to_coo())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "981a033a",
   "metadata": {},
   "outputs": [],
   "source": [
    "del matrix_sparsed\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "302fe813",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(sparse_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f1169c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c392c36",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sparse_matrix.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6fe5aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "%time similarities = cosine_similarity(sparse_matrix[0:10000], sparse_matrix[0:10000], dense_output=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c34df71",
   "metadata": {},
   "outputs": [],
   "source": [
    "similarities[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58e6f1be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change chunk_size to control resource consumption and speed\n",
    "# Higher chunk_size means more memory/RAM needed but also faster \n",
    "chunk_size = 10000 \n",
    "matrix_len = sparse_matrix.shape[0] \n",
    "\n",
    "def similarity_cosine_by_chunk(start, end, dense):\n",
    "    if end > matrix_len:\n",
    "        end = matrix_len\n",
    "    return cosine_similarity(X=sparse_matrix[start:end], Y=sparse_matrix, dense_output=dense) # scikit-learn function\n",
    "\n",
    "#for chunk_start in range(0, 10, chunk_size):\n",
    "    #cosine_similarity_chunk = similarity_cosine_by_chunk(chunk_start, chunk_start+chunk_size)\n",
    "%time cosine_similarity_chunk = similarity_cosine_by_chunk(0, 10000, dense=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5f3f0dc",
   "metadata": {},
   "source": [
    "- Time for size 1, dense output: 39.4s\n",
    "- Time for size 1000, dense output: 8min 48s\n",
    "- Time for size 1, compact output: 47.8s\n",
    "- Time for size 10000, compact output: 1h 41min 6s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "374fb615",
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_similarity_chunk[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7c88325",
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b94919fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse_matrix[0:10000].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75e2c051",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change chunk_size to control resource consumption and speed\n",
    "# Higher chunk_size means more memory/RAM needed but also faster \n",
    "chunk_size = 10000 \n",
    "matrix_len = sparse_matrix.shape[0] \n",
    "\n",
    "def similarity_cosine_by_chunk(start, end, dense):\n",
    "    if end > matrix_len:\n",
    "        end = matrix_len\n",
    "    return np.matmul(sparse_matrix[start:end], sparse_matrix) # scikit-learn function\n",
    "\n",
    "#for chunk_start in range(0, 10, chunk_size):\n",
    "    #cosine_similarity_chunk = similarity_cosine_by_chunk(chunk_start, chunk_start+chunk_size)\n",
    "#%time cosine_similarity_chunk = similarity_cosine_by_chunk(0, 10000, dense=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c3ab31e",
   "metadata": {},
   "outputs": [],
   "source": [
    "similarities[:1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dc71525",
   "metadata": {},
   "source": [
    "### Chunks, normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef18d6cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "normed_matrix = pp.normalize(sparse_matrix.tocsc(), axis=0)\n",
    "del sparse_matrix\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2806570d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change chunk_size to control resource consumption and speed\n",
    "# Higher chunk_size means more memory/RAM needed but also faster \n",
    "chunk_size = 1000 \n",
    "matrix_len = normed_matrix.shape[0] \n",
    "\n",
    "def similarity_cosine_by_chunk(start, end, dense=False):\n",
    "    if end > matrix_len:\n",
    "        end = matrix_len\n",
    "    return cosine_similarity(X=normed_matrix[start:end], Y=normed_matrix, dense_output=dense) # scikit-learn function\n",
    "\n",
    "#for chunk_start in range(0, 10, chunk_size):\n",
    "    #cosine_similarity_chunk = similarity_cosine_by_chunk(chunk_start, chunk_start+chunk_size)\n",
    "%time cosine_similarity_chunk = similarity_cosine_by_chunk(0, 1, dense=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed842b58",
   "metadata": {},
   "source": [
    "Time for size 1, dense output: 1min 51s\n",
    "Time for size 1000, dense output: 10min 20s\n",
    "Time for size 1, compact output: 1min 51s\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4290b82",
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_similarity_chunk.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5be6f2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_similarity_chunk[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9dc2c78",
   "metadata": {},
   "outputs": [],
   "source": [
    "user = 'Torsten'\n",
    "\n",
    "%time sparse_user =  csr_matrix(matrix_sparsed.loc[user])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "152cc5b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse_user.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4590056f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sparseuser_AB = sparse_matrix.multiply(sparse_user)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0427749b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sparseuser_AB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ffeef70",
   "metadata": {},
   "source": [
    "### Old function with comparison blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25f24bb4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05b51313",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for user_block in block_indices_lookup:\n",
    "    \n",
    "    print(\"Starting block \"+str(user_block))\n",
    "    \n",
    "    starting_block_indexes = block_indices_lookup[user_block]\n",
    "    base_start = starting_block_indexes['Start']\n",
    "    base_end = starting_block_indexes['End']\n",
    "    \n",
    "    array_chunk_a = (matrix_array[base_start:base_end]/10).astype('float32')\n",
    "    \n",
    "    # Opening JSON file\n",
    "    with open('user_similarities/similarity_storage'+str(user_block)+'.json') as json_file:\n",
    "        base_users_storage = json.load(json_file)\n",
    "    \n",
    "    first_block_of_comparison = user_block\n",
    "    end_range = len(block_indices_lookup)+1    \n",
    "    \n",
    "    # TEMPORARY END RANGE FOR TESTINGS\n",
    "    end_range = 2\n",
    "    \n",
    "    for comparison_block in np.arange(first_block_of_comparison, end_range, 1):\n",
    "        \n",
    "        print(\"User Block \"+str(user_block)+' vs Comparison Block '+str(comparison_block))\n",
    "\n",
    "        # Opening JSON file\n",
    "        with open('user_similarities/similarity_storage'+str(comparison_block)+'.json') as json_file:\n",
    "            comparison_users_storage = json.load(json_file)\n",
    "        \n",
    "        comparison_indexes = block_indices_lookup[comparison_block]\n",
    "        compare_start = comparison_indexes['Start']\n",
    "        compare_end = comparison_indexes['End']\n",
    "        \n",
    "        print(\"Making matrices\")\n",
    "        start = time.time()\n",
    "        array_chunk_b = ((matrix_array[compare_start:compare_end].T)/10).astype('float32')\n",
    "        \n",
    "        a = tf.constant(array_chunk_a)\n",
    "        b = tf.constant(array_chunk_b)\n",
    "        \n",
    "        normalize_a = tf.nn.l2_normalize(a,1)\n",
    "        del a\n",
    "        gc.collect()\n",
    "\n",
    "        normalize_b = tf.nn.l2_normalize(b,0)\n",
    "        del b\n",
    "        gc.collect()\n",
    "        \n",
    "        print(\"Getting similarity scores\")\n",
    "        similarities = tf.matmul(normalize_a, normalize_b)#, adjoint_b=True)\n",
    "        del normalize_a\n",
    "        del normalize_b\n",
    "        gc.collect()\n",
    "        \n",
    "        # store user info\n",
    "        \n",
    "        incrementer_base = 0\n",
    "        \n",
    "        print(\"Storing Similarities\")\n",
    "        for base_user in user_blocks_lookup[user_block][:5]:\n",
    "            \n",
    "            print(base_user)\n",
    "                                   \n",
    "            user_similarities = similarities[incrementer_base].numpy()\n",
    "            max_spot = np.argmax(user_similarities.max())\n",
    "            mean_spot = np.median(user_similarities)\n",
    "            user_similarities[max_spot] = mean_spot\n",
    "            scaler = MinMaxScaler(feature_range=(-1,1))\n",
    "            user_similarities = scaler.fit_transform(user_similarities.reshape(-1,1))\n",
    "            user_similarities = list(np.round(user_similarities, 2).ravel())\n",
    "            \n",
    "            for key, value in list(zip(user_blocks_lookup[comparison_block][incrementer_base:], user_similarities[incrementer_base:])):\n",
    "                if value >= .25 or value <= -.25:\n",
    "                    base_users_storage[base_user][key] = float(value)\n",
    "                if user_block != comparison_block:\n",
    "                    comparison_users_storage[key][base_user] = float(value)\n",
    "            \n",
    "            incrementer_base +=1\n",
    "        \n",
    "            # save dictionary\n",
    "            with open('user_similarities/similarity_storage'+str(comparison_block)+'.json', 'w') as convert_file:\n",
    "                convert_file.write(json.dumps(comparison_users_storage))\n",
    "        \n",
    "        print(\"Cleaning up memory for this iteration\")\n",
    "        del comparison_users_storage\n",
    "        #del similarities\n",
    "        gc.collect()\n",
    "        \n",
    "        end = time.time()\n",
    "        print(str(end-start)+' seconds elapsed for this comparison section')\n",
    "    \n",
    "    # save dictionary\n",
    "    with open('user_similarities/similarity_storage'+str(user_block)+'.json', 'w') as convert_file:\n",
    "        convert_file.write(json.dumps(base_users_storage))\n",
    "        \n",
    "    #del base_users_storage\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e97fba49",
   "metadata": {},
   "source": [
    "## Deprecated Tensorflow time reduction attempts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbe39421",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# the basic file required for this work - the full matrix\n",
    "\n",
    "larger_matrix = pd.read_pickle('synthetic_ratings/users_synthetic_2193_sparsematrix_nogameids.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e24e0649",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "larger_matrix.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ccd4233",
   "metadata": {},
   "outputs": [],
   "source": [
    "larger_matrix.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bb8e108",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sparse dataframe into numpy array\n",
    "\n",
    "matrix_array = np.array(larger_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19c694b9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b578027a",
   "metadata": {},
   "source": [
    "Turn single user into a column 21921, 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4584f009",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_id = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78f51021",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get single user from matrix_array\n",
    "\n",
    "%time single_user = matrix_array[user_id]\n",
    "single_user.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09000ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get nonzero indices for user\n",
    "%time indices = list(np.nonzero(single_user)[0])\n",
    "indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbe1d0d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make reduced array for user of nonzero indices\n",
    "%time array_chunk_a = (single_user[indices]).astype('float32').reshape(-1,1)\n",
    "array_chunk_a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8455935",
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize user\n",
    "%time normalize_a = normalize(array_chunk_a, axis=0)\n",
    "normalize_a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c857f4cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7dc87dba",
   "metadata": {},
   "source": [
    "Investigate methods of reducing dataframe or array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d78adf7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make reduced on sparse dataframe\n",
    "%time df_chunk_b = larger_matrix[indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ec72e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_chunk_b.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51f90d17",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_chunk_b.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "932946d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make reduced on array\n",
    "%time array_chunk_b = matrix_array[:, indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cb2e9f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "array_chunk_b.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c108ee95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# turn array into sparse matrix\n",
    "sparse_matrix = sparse.csr_matrix(matrix_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae225589",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make reduced on sparse\n",
    "%time array_chunk_b = sparse_matrix[:, indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ee33c2b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5a8a11c7",
   "metadata": {},
   "source": [
    "Convert dataframe to array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0652460",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert reduced dataframe to sparse matrix\n",
    "\n",
    "%time sparse_array = sparse.csr_matrix(df_chunk_b.sparse.to_coo())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40891e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert reduced dataframe to array\n",
    "%time array_b_matrix = df_chunk_b.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95df43dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "array_b_matrix[0][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36972aae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b85f165",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9d6c57fb",
   "metadata": {},
   "source": [
    "Investigate normalization methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78fc26ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sklearn normalize on dataframe\n",
    "%time normalize_b = normalize(df_chunk_b, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44fc7094",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalize_b[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e9f137b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef1a4019",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sklearn normalize on array\n",
    "%time normalize_b = normalize(array_b_matrix, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47703181",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalize_b[0][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb3b4766",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "141e09ef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a6fa5e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make partial dataframe segment\n",
    "%time partial_df = df_chunk_b[:134400]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "932b8228",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make partial array segment\n",
    "%time partial_array = normalize_b[:134400]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ba7ae80",
   "metadata": {},
   "source": [
    "## Deprecated Parallelization Techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c98037e",
   "metadata": {},
   "source": [
    "### Test common indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1626e2a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "\n",
    "# Step one\n",
    "# make the single item matrix for the one item\n",
    "%time single_item = matrix_array[:, item1]\n",
    "# get the indices where the item is nonzero\n",
    "%time indices = np.nonzero(single_item)[0]\n",
    "\n",
    "# Step 2\n",
    "# make the single item matrix for the next item\n",
    "%time next_item = matrix_array[:, item2]\n",
    "# get the indices where the item is nonzero\n",
    "%time indices2 = np.nonzero(next_item)[0]\n",
    "\n",
    "# step 3\n",
    "# get the indices in common between the two\n",
    "%time common_indices = list(set.intersection(set(indices), set(indices2)))\n",
    "\n",
    "# step 4\n",
    "%time reduced_item1 = single_item[common_indices]\n",
    "%time reduced_item2 = next_item[common_indices]\n",
    "\n",
    "# step 5\n",
    "%time a = tf.constant(reduced_item1, dtype=tf.float32)\n",
    "%time b = tf.constant(reduced_item2, dtype=tf.float32)\n",
    "%time a = tf.nn.l2_normalize(a)\n",
    "%time b = tf.nn.l2_normalize(b)\n",
    "\n",
    "# step 6\n",
    "%time item_similarity = 1-cosine_distance(a, b, axis=0).numpy()\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "print(end-start)\n",
    "\n",
    "print(item_similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef153a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "reduced_item1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f94bc4e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "reduced_item2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5677acbb",
   "metadata": {},
   "source": [
    "### Test xor1d indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a8aa400",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "\n",
    "# Step one\n",
    "# make the single item matrix for the one item\n",
    "%time single_item = matrix_array[:, item1].copy()\n",
    "# get the indices where the item is nonzero\n",
    "%time indices = np.nonzero(single_item)[0]\n",
    "\n",
    "# Step 2\n",
    "# make the single item matrix for the next item\n",
    "%time next_item = matrix_array[:, item2].copy()\n",
    "# get the indices where the item is nonzero\n",
    "%time indices2 = np.nonzero(next_item)[0]\n",
    "\n",
    "# step 3\n",
    "# get the indices in common between the two\n",
    "%time indices_diff = np.setxor1d(indices, indices2)\n",
    "\n",
    "%time first_item = single_item.copy()\n",
    "# step 4\n",
    "%time first_item[[indices_diff]]=0\n",
    "%time next_item[[indices_diff]]=0\n",
    "\n",
    "# step 5\n",
    "%time a = tf.constant(first_item, dtype=tf.float32)\n",
    "%time b = tf.constant(next_item, dtype=tf.float32)\n",
    "%time a = tf.nn.l2_normalize(a)\n",
    "%time b = tf.nn.l2_normalize(b)\n",
    "\n",
    "# step 6\n",
    "%time item_similarity = 1-cosine_distance(a, b, axis=0).numpy()\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "print(end-start)\n",
    "\n",
    "print(item_similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaf594fe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f6d4311",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "836d9be0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "76ec3a58",
   "metadata": {},
   "source": [
    "### No Jit and xor1d indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e19ec6b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@jit(nopython=True, parallel=True, fastmath=True)\n",
    "def math_function(game, matrix_array, number_of_games):\n",
    "    \n",
    "    #results = []\n",
    "    results_a = []\n",
    "    results_b =[]\n",
    "    \n",
    "    # make the single user matrix for the one user\n",
    "    single_item = matrix_array[:, game].copy()\n",
    "    # get the indices where the user is nonzero\n",
    "    indices = np.nonzero(single_item)[0]\n",
    "    \n",
    "    for game2 in number_of_games:\n",
    "    \n",
    "        next_item = matrix_array[:, game2].copy()\n",
    "        indices2 = np.nonzero(next_item)[0]\n",
    "            \n",
    "        indices_diff = np.setxor1d(indices, indices2)\n",
    "        \n",
    "        #if len(common_indices)<4:\n",
    "            #results.append(0)\n",
    "            #continue      \n",
    "        \n",
    "        first_item = single_item.copy()\n",
    "        first_item[[indices_diff]]=0\n",
    "        next_item[[indices_diff]]=0\n",
    "        \n",
    "        # step 5\n",
    "        #a = tf.constant(first_item, dtype=tf.float32)\n",
    "        #b = tf.constant(next_item, dtype=tf.float32)\n",
    "        #a = tf.nn.l2_normalize(a)\n",
    "        #b = tf.nn.l2_normalize(b)\n",
    "\n",
    "        # step 6\n",
    "        #item_similarity = 1-cosine_distance(a, b, axis=0).numpy()\n",
    "        \n",
    "        #results.append(item_similarity)\n",
    "        results_a.append(first_item)\n",
    "        results_b.append(next_item)\n",
    "        \n",
    "    return results_a, results_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d40afc9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "number_of_games = np.arange(0, matrix_array.shape[1], 1)\n",
    "\n",
    "global_start = time.time()\n",
    "\n",
    "# Load the storage dictionary for this block\n",
    "with open('item_similarities/similarity_storage_real_scaled_temp.json') as json_file:\n",
    "    base_items_storage = json.load(json_file) \n",
    "    \n",
    "# for each user block in the block_indices_lookup. The user blocks are integers from 1-20\n",
    "for game in number_of_games[0:100]:\n",
    "    \n",
    "    print(\"\\nStarting game: \"+str(game))\n",
    "    start = time.time()\n",
    "    \n",
    "    gameid_1 = gameids_columnorder[game]\n",
    "                     \n",
    "    results_a, results_b = math_function(game, matrix_array, number_of_games)\n",
    "    \n",
    "    #base_items_storage[gameid_1]['Sims'] = results\n",
    "    base_items_storage[gameid_1]['Left'] = results_a\n",
    "    base_items_storage[gameid_1]['Right'] = results_b\n",
    "    \n",
    "    end=time.time()\n",
    "    \n",
    "    print(end-start)\n",
    "\n",
    "print(time.time()-global_start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc26d366",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b5b7f8f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29b3f863",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a025daaa",
   "metadata": {},
   "source": [
    "No GPU. No filtering: 2572sec for 97 entries\n",
    "No GPU. Filtering < 4: 2453sec for 97 entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e282cae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54313e88",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4509b69",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a91138fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3b43ce3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b7e2197",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "365a298f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "370ec231",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a5f3251",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "353.056px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
