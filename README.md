# Boardgamegeek

All projects should be run from within the priamry `boardgamegeek` directory or relative paths will be a problem.

AWS is required for this project to run as presented. Running on the cloud is not a free service. The authors of this repo are not responsible for any costs incurred by your usage of AWS services.

## Project Requirements

This project uses Pipenv as its environment manager. Documentation on pipenv is outside the scope of this repo.

A `.env` must be made in the project root directory. Copy in the variables from `env.example` and populate as needed for your use case. 


## Project Order

Most steps in this project are explicitly dependent on a prior step. Dependencies are noted and explained.

### Step 01 - get boardgames_ranks.csv file from BGG and save it to S3
- `bgg_boardgame_file_retrieval.get_bgg_games_file.py`
- Gets the `boardgames_ranks.csv` file from BGG and saves it to S3. A BGG account is required for this.

### 02 - Generate GAME scraping URLS
- `lambda_functions.generate_game_urls_lambda.py`
- Must have `boardgames_ranks.csv` in directory `data` OR on S3 from the [prior step](#01---get-boardgames_ranks.csv-file-from-bgg-and-save-it-to-s3). Download from BGG or use [Step 01](#01---get-boardgames_ranks.csv-file-from-bgg-and-save-it-to-s3) to write it to S3.
- Opens the `boardgames_ranks.csv` file and generates urls for the game scraper. Writes URLS locally when run locally, and always writes URLs to S3.

### 03 - Scrape games from URLS

- PROD - `lambda_functions.boardgame_scraper_fargate_trigger` for GAME will trigger process to run and write scraping on S3    
    - Must have generated game urls first with step 02.
    - Scrapes the URLs generated by step #2. This script will always trigger tasks on AWS. DO NOT RUN WITHOUT INTENT costs ~$2 per run.
    - On AWS, navigate to lambda
    - From lambda, select `boardgame_scraper_fargate_trigger`
    - To manually run, go to the "Test" tab
    - In the "Event JSON" section, replace the existing keys with "scraper_type": "game"`.  It is recommended to enter in an event name and save the json for future.
    - Click "Test" to run.

- TEST ON AWS - `lambda_functions.boardgame_scraper_fargate_trigger_dev` for GAME will trigger process to run and write scraping on S3    
    - Must have generated game urls first with step 02.
    - Scrapes the URLs generated by step #2. This script will always trigger tasks on AWS. The DEV function will only run a single URL from a single file, so this is safely inexpensive.
    - On AWS, navigate to lambda
    - From lambda, select `boardgame_scraper_fargate_trigger_dev`
    - To manually run, go to the "Test" tab
    - In the "Event JSON" section, replace the existing keys with `"scraper_type": "game"`.  It is recommended to enter in an event name and save the json for future.
    - Click "Test" to run.

- TEST LOCAL - `game_data_scraper.main.py` for GAME to test a single file locally
    - Use to test a single specific url file. Must have generated game urls first with step 02.
    - Run locally and pass both `scraper_type` and an existing filename without directory or suffix from `data/scraper_urls_raw_game`
    - Example: `python game_data_scraper/main.py game group1_game_scraper_urls_raw`
    - Only saves data locally to `data/scraped_xml_raw_games`

### 04 - Clean raw scraped GAME data

- `game_data_cleaner.main.py`
    - Takes the scraped files and composes into various dirty data frames of full data. Writes these locally. Will only write to S3 if run on AWS.
    - Step 03 needs to have run at least once for this to work, although two sample files from local will also suffice for testing.
    - If files are present on S3, it will download all of them for this process. If there are no files on S3 yet, it will use files in `data/scraped_xml_raw_games`

### 05 - Generate USER scraping URLS

- `lambda_functions.generate_user_urls_lambda.py`
- Must have `games.pkl` in directory `data/game_dfs_dirty` OR on S3 from prior step.
- Loads the `games.pkl` file generated by 04 and generates user ratings urls. Will attempt to load games.pkl locally, otherwise will retrieve it from S3.

### 06 - Scrape users from URLS

- PROD - `lambda_functions.boardgame_scraper_fargate_trigger` for USER will trigger process to run and write scraping on S3
    - Must have generated game urls first with step 5.
    - Scrapes the URLs generated by step #5. This script will always trigger tasks on AWS. DO NOT RUN WITHOUT INTENT costs over $15 per run.
    - Must run with arg for scraper type "user" example `python lambda_functions.boardgame_scraper_fargate_trigger.py user`

- TEST - `game_data_scraper.main.py` for USER
    - Use to test a single specific url file. Must have generated user urls first with step 05.
    - Run locally and pass both scraper_type arg, and an existing filename without directory or suffix from `data/scraper_urls_raw_user`
    - Example: `python game_data_scraper/main.py user group1_user_scraper_urls_raw`
    - Only saves data locally to `data/scraped_xml_raw_users`