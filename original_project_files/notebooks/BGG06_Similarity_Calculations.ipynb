{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a3bcc119",
   "metadata": {},
   "source": [
    "# Notebook Objective and Setup\n",
    "\n",
    "BGG06 is where synthetic ratings are validated. The actual synthetic ratings are produced in an external file - similarity_process.py\n",
    "\n",
    "This notebook involved refining the code for that external file, and then includes the validation checks to check the data produced in the external file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60b8b86c",
   "metadata": {},
   "source": [
    "## Package Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f40cb89",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import numba as nb\n",
    "import requests\n",
    "import regex as re\n",
    "import time\n",
    "import gc\n",
    "import copy\n",
    "import json\n",
    "from statistics import mean\n",
    "\n",
    "from numba import jit, cuda, prange, typeof, typed, types\n",
    "from numpy.linalg import norm\n",
    "\n",
    "from multiprocessing import Pool, Manager\n",
    "\n",
    "# ignore warnings (gets rid of Pandas copy warnings)\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "pd.options.display.max_columns = None\n",
    "\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "pd.set_option(\"display.max_rows\", 100)\n",
    "\n",
    "# from scipy import sparse\n",
    "# from scipy.sparse import csr_matrix\n",
    "# from scipy import spatial\n",
    "\n",
    "# from sklearn.metrics.pairwise import cosine_similarity\n",
    "# import sklearn.preprocessing as pp\n",
    "# from sklearn.preprocessing import StandardScaler, PolynomialFeatures, MinMaxScaler, OneHotEncoder\n",
    "from sklearn.preprocessing import MinMaxScaler, normalize\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.compat.v1.losses import cosine_distance\n",
    "from tensorflow.keras.losses import CosineSimilarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bc1e262",
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing as mp\n",
    "\n",
    "ncpus = mp.cpu_count()\n",
    "print(\"We have {} cores to work on!\".format(ncpus))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ef316c6",
   "metadata": {},
   "source": [
    "## Notebook Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca924803",
   "metadata": {},
   "outputs": [],
   "source": [
    "global_start = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "722f1152",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the basic file required for this work - the full matrix\n",
    "\n",
    "# larger_matrix = pd.read_pickle('synthetic_ratings/users_synthetic_1000_fullmatrix.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "233dd54d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the basic file required for this work - the full matrix\n",
    "\n",
    "larger_matrix = pd.read_pickle(\"real_ratings/users_real_unscaled_fullmatrix.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0421a3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert full matrix to numpy and delete matrix\n",
    "\n",
    "matrix_array = larger_matrix.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2994f85",
   "metadata": {},
   "outputs": [],
   "source": [
    "gameids_columnorder = list(larger_matrix.columns)\n",
    "gameids_columnorder[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f460db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "del larger_matrix\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e60d8df",
   "metadata": {},
   "source": [
    "# Similarity Calculations - Jit Parallelization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ae47c28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell is appending all results to a list in order\n",
    "# As long as the inner function itself is not asynchronous, this should be fine\n",
    "\n",
    "\n",
    "@jit(nopython=True, parallel=True, fastmath=True)\n",
    "def math_function(game, matrix_array, all_games):\n",
    "\n",
    "    results = []\n",
    "\n",
    "    # make the single user matrix for the one user\n",
    "    single_item = matrix_array[:, game].copy()\n",
    "    # get the indices where the user is nonzero\n",
    "    indices = np.nonzero(single_item)[0]\n",
    "\n",
    "    for game2 in all_games:\n",
    "\n",
    "        next_item = matrix_array[:, game2].copy()\n",
    "        indices2 = np.nonzero(next_item)[0]\n",
    "\n",
    "        common_indices = np.intersect1d(indices, indices2)\n",
    "\n",
    "        if len(common_indices) < 4:\n",
    "            results.append(0)\n",
    "            continue\n",
    "\n",
    "        else:\n",
    "            a = single_item[common_indices].astype(np.float32)\n",
    "            b = next_item[common_indices].astype(np.float32)\n",
    "\n",
    "            try:\n",
    "                item_similarity = a @ b.T / (norm(a) * norm(b))\n",
    "                results.append(item_similarity)\n",
    "            except:\n",
    "                results.append(0)\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6486e10",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_games = np.arange(0, matrix_array.shape[1], 1)\n",
    "\n",
    "games_range = len(all_games[:100])\n",
    "games_range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfb0c431",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_games = np.arange(0, matrix_array.shape[1], 1)\n",
    "\n",
    "\n",
    "# Load the storage dictionary for this block\n",
    "# with open('item_similarities/similarity_storage_real_scaled_temp_1.json') as json_file:\n",
    "#    base_items_storage = json.load(json_file)\n",
    "base_items_storage_1 = {}\n",
    "\n",
    "# for each user block in the block_indices_lookup. The user blocks are integers from 1-20\n",
    "for game in all_games[:10]:\n",
    "\n",
    "    print(\"\\nStarting game: \" + str(game))\n",
    "    start = time.time()\n",
    "\n",
    "    gameid_1 = gameids_columnorder[game]\n",
    "\n",
    "    results = math_function(game, matrix_array, all_games)  # , results_a, results_b\n",
    "\n",
    "    base_items_storage_1[gameid_1] = results\n",
    "\n",
    "    end = time.time()\n",
    "\n",
    "    print(end - start)\n",
    "\n",
    "print(time.time() - global_start)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aa206ae",
   "metadata": {},
   "source": [
    "Time check no multiprocessing, 50 entries: 903\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b533365",
   "metadata": {},
   "source": [
    "Time check first 300 in 3 notebooks:\n",
    "2511+511+3171= 6193\n",
    "\n",
    "2535+538+=3108"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d8a35a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "gameids_columnorder[258]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a3631f7",
   "metadata": {},
   "source": [
    "Time check first 300 in 3 Pool processes:\n",
    "3219"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a68749a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save dictionary\n",
    "with open(\n",
    "    \"item_similarities/similarity_storage_real_scaled_test_0100.json\", \"w\"\n",
    ") as convert_file:\n",
    "    convert_file.write(json.dumps(base_items_storage_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37bf1100",
   "metadata": {},
   "outputs": [],
   "source": [
    "del matrix_array\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d744f201",
   "metadata": {},
   "outputs": [],
   "source": [
    "break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd4c6ea1",
   "metadata": {},
   "source": [
    "Time total: 50587 for 10000 entries\n",
    "\n",
    "Time total: 582220 for all entries 250synth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c38cd4c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "time = 82220\n",
    "\n",
    "time / 60 / 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d5be51e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c02bbec",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_items_storage[\"174430\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9df2c32",
   "metadata": {},
   "source": [
    "# Data Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "338bbb49",
   "metadata": {},
   "source": [
    "## Load the multiprocessed file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46f1b156",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the storage dictionary for this block\n",
    "with open(\"item_similarities/item_similarity_storage_real.json\") as json_file:\n",
    "    item_similarity_storage_real = json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "434d0933",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(item_similarity_storage_real)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c65e965c",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(item_similarity_storage_real[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a6e720a",
   "metadata": {},
   "outputs": [],
   "source": [
    "item_similarity_storage_real[0][\"822\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ac18892",
   "metadata": {},
   "outputs": [],
   "source": [
    "item_similarity_storage_real[0][\"174430\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9fc384f",
   "metadata": {},
   "source": [
    "## Load the manual files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaa00f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the storage dictionary for this block\n",
    "with open(\n",
    "    \"item_similarities/similarity_storage_real_scaled_test_0100.json\"\n",
    ") as json_file:\n",
    "    similarity_storage_real_scaled_test_0100 = json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1004e0db",
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_storage_real_scaled_test_0100[\"174430\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c7ddfec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the storage dictionary for this block\n",
    "with open(\n",
    "    \"item_similarities/similarity_storage_real_scaled_test_200300.json\"\n",
    ") as json_file:\n",
    "    similarity_storage_real_scaled_test_200300 = json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "258070a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_storage_real_scaled_test_200300[\"822\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d28d9172",
   "metadata": {},
   "source": [
    "# Deprecated"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc5afa49",
   "metadata": {},
   "source": [
    "## Similarity Calculations - Tensorflow (GPU only)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "054561ca",
   "metadata": {},
   "source": [
    "### Code Work - Item to Item tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6bd7489",
   "metadata": {},
   "outputs": [],
   "source": [
    "larger_matrix.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "754c9c3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7183129",
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90d299c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "item1 = 63\n",
    "item2 = 68"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "732680a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step one\n",
    "# make the single item matrix for the one item\n",
    "%time single_item = matrix_array[:, item1]\n",
    "single_item.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d5e881f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the indices where the item is nonzero\n",
    "%time indices = list(np.nonzero(single_item)[0])\n",
    "len(indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e541ac74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2\n",
    "# make the single item matrix for the next item\n",
    "%time next_item = matrix_array[:, item2]\n",
    "# get the indices where the item is nonzero\n",
    "%time indices2 = list(np.nonzero(next_item)[0])\n",
    "\n",
    "len(indices2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63206b09",
   "metadata": {},
   "outputs": [],
   "source": [
    "%time common_indices = list(set.intersection(set(indices), set(indices2)))\n",
    "len(common_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2acc879f",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "\n",
    "# Step one\n",
    "# make the single item matrix for the one item\n",
    "%time single_item = matrix_array[:, item1]\n",
    "# get the indices where the item is nonzero\n",
    "%time indices = list(np.nonzero(single_item)[0])\n",
    "# %time indices = np.nonzero(single_item)[0]\n",
    "\n",
    "# Step 2\n",
    "# make the single item matrix for the next item\n",
    "%time next_item = matrix_array[:, item2]\n",
    "# get the indices where the item is nonzero\n",
    "%time indices2 = list(np.nonzero(next_item)[0])\n",
    "# %time indices2 = np.nonzero(next_item)[0]\n",
    "\n",
    "# step 3\n",
    "# get the indices in common between the two\n",
    "# %time common_indices = list(set.intersection(set(indices), set(indices2)))\n",
    "%time common_indices = list(set.intersection(set(indices), set(indices2)))\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e0505bd",
   "metadata": {},
   "source": [
    "#### Method - TF Matmul"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b64f997d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 4\n",
    "%time reduced_item1 = single_item[common_indices].reshape(1,-1)\n",
    "%time reduced_item2 = next_item[common_indices].reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44a46e0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "reduced_item1.shape, reduced_item2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d4585fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 5\n",
    "%time a = tf.constant(reduced_item1, dtype=tf.float32)\n",
    "%time b = tf.constant(reduced_item2, dtype=tf.float32)\n",
    "%time a = tf.nn.l2_normalize(a)\n",
    "%time b = tf.nn.l2_normalize(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e174648",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# step 6\n",
    "item_similarity = round(float(tf.matmul(a, b)), 2)\n",
    "item_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d57b38e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "\n",
    "# Step one\n",
    "# make the single item matrix for the one item\n",
    "%time single_item = matrix_array[:, item1]\n",
    "# get the indices where the item is nonzero\n",
    "%time indices = np.nonzero(single_item)[0]\n",
    "# %time indices = np.nonzero(single_item)[0]\n",
    "\n",
    "# Step 2\n",
    "# make the single item matrix for the next item\n",
    "%time next_item = matrix_array[:, item2]\n",
    "# get the indices where the item is nonzero\n",
    "%time indices2 = np.nonzero(next_item)[0]\n",
    "# %time indices2 = np.nonzero(next_item)[0]\n",
    "\n",
    "# step 3\n",
    "# get the indices in common between the two\n",
    "%time common_indices = list(set.intersection(set(indices), set(indices2)))\n",
    "\n",
    "# step 4\n",
    "%time reduced_item1 = single_item[common_indices].reshape(1,-1)\n",
    "%time reduced_item2 = next_item[common_indices].reshape(-1,1)\n",
    "\n",
    "# step 5\n",
    "%time a = tf.constant(reduced_item1, dtype=tf.float32)\n",
    "%time b = tf.constant(reduced_item2, dtype=tf.float32)\n",
    "%time a = tf.nn.l2_normalize(a)\n",
    "%time b = tf.nn.l2_normalize(b)\n",
    "\n",
    "# step 6\n",
    "%time item_similarity = float(tf.matmul(a, b))\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "print(end - start)\n",
    "\n",
    "print(item_similarity)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8f6246b",
   "metadata": {},
   "source": [
    "#### Method- TF cosine_distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17e0fad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "\n",
    "# Step one\n",
    "# make the single item matrix for the one item\n",
    "%time single_item = matrix_array[:, item1]\n",
    "# get the indices where the item is nonzero\n",
    "%time indices = np.nonzero(single_item)[0]\n",
    "\n",
    "# Step 2\n",
    "# make the single item matrix for the next item\n",
    "%time next_item = matrix_array[:, item2]\n",
    "# get the indices where the item is nonzero\n",
    "%time indices2 = np.nonzero(next_item)[0]\n",
    "\n",
    "# step 3\n",
    "# get the indices in common between the two\n",
    "%time common_indices = list(set.intersection(set(indices), set(indices2)))\n",
    "\n",
    "# step 4\n",
    "%time reduced_item1 = single_item[common_indices]\n",
    "%time reduced_item2 = next_item[common_indices]\n",
    "\n",
    "# step 5\n",
    "%time a = tf.constant(reduced_item1, dtype=tf.float32)\n",
    "%time b = tf.constant(reduced_item2, dtype=tf.float32)\n",
    "%time a = tf.nn.l2_normalize(a)\n",
    "%time b = tf.nn.l2_normalize(b)\n",
    "\n",
    "# step 6\n",
    "%time item_similarity = 1-cosine_distance(a, b, axis=0).numpy()\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "print(end - start)\n",
    "\n",
    "print(item_similarity)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4868c523",
   "metadata": {},
   "source": [
    "#### Method - TF Cosine similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1874a19",
   "metadata": {},
   "outputs": [],
   "source": [
    "cos_sim = CosineSimilarity()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5541d85",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "\n",
    "# Step one\n",
    "# make the single item matrix for the one item\n",
    "%time single_item = matrix_array[:, item1]\n",
    "# get the indices where the item is nonzero\n",
    "%time indices = np.nonzero(single_item)[0]\n",
    "\n",
    "# Step 2\n",
    "# make the single item matrix for the next item\n",
    "%time next_item = matrix_array[:, item2]\n",
    "# get the indices where the item is nonzero\n",
    "%time indices2 = np.nonzero(next_item)[0]\n",
    "\n",
    "# step 3\n",
    "# get the indices in common between the two\n",
    "%time common_indices = list(set.intersection(set(indices), set(indices2)))\n",
    "\n",
    "# step 4\n",
    "%time reduced_item1 = single_item[common_indices]\n",
    "%time reduced_item2 = next_item[common_indices]\n",
    "\n",
    "# step 5\n",
    "%time a = tf.constant(reduced_item1, dtype=tf.float32)\n",
    "%time b = tf.constant(reduced_item2, dtype=tf.float32)\n",
    "%time a = tf.nn.l2_normalize(a)\n",
    "%time b = tf.nn.l2_normalize(b)\n",
    "\n",
    "# step 6\n",
    "%time item_similarity = cos_sim(a, b).numpy()\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "print(end - start)\n",
    "\n",
    "print(item_similarity)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fd3ef76",
   "metadata": {},
   "source": [
    "## Make item-item calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba782666",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "number_of_games = np.arange(0, matrix_array.shape[1], 1)\n",
    "\n",
    "time_test = []\n",
    "\n",
    "\n",
    "# Load the storage dictionary for this block\n",
    "with open(\"item_similarities/similarity_storage_real_scaled.json\") as json_file:\n",
    "    base_items_storage = json.load(json_file)\n",
    "\n",
    "# for each user block in the block_indices_lookup. The user blocks are integers from 1-20\n",
    "for game in number_of_games[:500]:\n",
    "\n",
    "    print(\"\\nStarting game: \" + str(game))\n",
    "    start = time.time()\n",
    "\n",
    "    gameid_1 = gameids_columnorder[game]\n",
    "\n",
    "    # make the single user matrix for the one user\n",
    "    single_item = matrix_array[:, game]\n",
    "    # get the indices where the user is nonzero\n",
    "    indices = np.nonzero(single_item)[0]\n",
    "    checkpoint1 = time.time()\n",
    "    # print(\"Reduce item 1: \"+str(checkpoint1-start))\n",
    "\n",
    "    for game2 in number_of_games:\n",
    "\n",
    "        if game == game2:\n",
    "            continue\n",
    "\n",
    "        gameid_2 = gameids_columnorder[game2]\n",
    "\n",
    "        if gameid_2 in base_items_storage[gameid_1]:\n",
    "            pass\n",
    "\n",
    "        else:\n",
    "\n",
    "            next_item = matrix_array[:, game2]\n",
    "            indices2 = np.nonzero(next_item)[0]\n",
    "            checkpoint3 = time.time()\n",
    "            # print(\"\\nReduce item 2: \"+str(checkpoint3-checkpoint1))\n",
    "\n",
    "            common_indices = list(set.intersection(set(indices), set(indices2)))\n",
    "\n",
    "            if len(common_indices) < 3:\n",
    "                checkpoint7 = time.time()\n",
    "                item_similarity = 0\n",
    "\n",
    "            else:\n",
    "\n",
    "                reduced_item1 = single_item[common_indices]\n",
    "                reduced_item2 = next_item[common_indices]\n",
    "                checkpoint4 = time.time()\n",
    "                # print(\"Reduce both to common indices: \"+str(checkpoint4-checkpoint3))\n",
    "\n",
    "                a = tf.constant(reduced_item1, dtype=tf.float32)\n",
    "                b = tf.constant(reduced_item2, dtype=tf.float32)\n",
    "                checkpoint5 = time.time()\n",
    "                # print(\"Load to tensors: \"+str(checkpoint5-checkpoint4))\n",
    "\n",
    "                a = tf.nn.l2_normalize(a)\n",
    "                b = tf.nn.l2_normalize(b)\n",
    "                checkpoint6 = time.time()\n",
    "                # print(\"Normalize tensors: \"+str(checkpoint6-checkpoint5))\n",
    "\n",
    "                item_similarity = 1 - cosine_distance(a, b, axis=0).numpy()\n",
    "                checkpoint7 = time.time()\n",
    "                # print(\"Get similarity: \"+str(checkpoint7-checkpoint6))\n",
    "\n",
    "            base_items_storage[gameid_1][gameid_2] = item_similarity\n",
    "            base_items_storage[gameid_2][gameid_1] = item_similarity\n",
    "            checkpoint8 = time.time()\n",
    "            # print(\"Store similarity: \"+str(checkpoint8-checkpoint7))\n",
    "\n",
    "    end = time.time()\n",
    "\n",
    "    print(\"Time for this game: \" + str(end - start) + \"\\n\")\n",
    "\n",
    "# save dictionary\n",
    "with open(\"item_similarities/similarity_storage_real_scaled.json\", \"w\") as convert_file:\n",
    "    convert_file.write(json.dumps(base_items_storage))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "773d3b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save dictionary\n",
    "with open(\"item_similarities/similarity_storage_real_scaled.json\", \"w\") as convert_file:\n",
    "    convert_file.write(json.dumps(base_items_storage))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc4b41f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_items_storage[\"84776\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b9b0239",
   "metadata": {},
   "source": [
    "## Code Work - User to User Tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05216370",
   "metadata": {},
   "source": [
    "### Preparing the user blocks and user storage dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ea7b019",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"users_list = list(larger_matrix.index)\n",
    "\n",
    "user_blocks_lookup = {}\n",
    "\n",
    "chunk_size = int(np.ceil(matrix_array.shape[0]/20))\n",
    "\n",
    "start = 0\n",
    "incrementer = 0\n",
    "\n",
    "while start < matrix_array.shape[0]:\n",
    "    \n",
    "    end = start + chunk_size\n",
    "    incrementer += 1\n",
    "    \n",
    "    user_blocks_lookup[incrementer] = users_list[start:end]\n",
    "\n",
    "    start += chunk_size\n",
    "\n",
    "print(\"\\nLookup dictionary complete\")\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deee32ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"user_id_lookup = {}\n",
    "\n",
    "increment=0\n",
    "for user in users_list:\n",
    "    \n",
    "    increment+=1\n",
    "    user_id_lookup[increment] = user\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f4479ee",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"block_indices_lookup = {}\n",
    "\n",
    "start = 0\n",
    "incrementer = 0\n",
    "\n",
    "while start < matrix_array.shape[0]:\n",
    "    \n",
    "    end = start + chunk_size\n",
    "    incrementer += 1\n",
    "    \n",
    "    block_indices_lookup[incrementer] = {}\n",
    "    block_indices_lookup[incrementer]['Start'] = start\n",
    "    block_indices_lookup[incrementer]['End'] = end\n",
    "    \n",
    "    start += chunk_size\n",
    "\n",
    "print(\"\\nLookup dictionary complete\")\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5713713b",
   "metadata": {},
   "outputs": [],
   "source": [
    "del larger_matrix\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4416a5de",
   "metadata": {},
   "source": [
    "ONLY RUN THIS AGAIN IF THE USER LIST CHANGES !!!!!  THIS WILL RESET ALL STORAGE DICTIONARIES ON DISK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "344ff069",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"max_range = len(user_blocks_lookup)+1\n",
    "\n",
    "for item in np.arange(1,max_range,1):\n",
    "    \n",
    "    storage_dict = {}\n",
    "    \n",
    "    for user in user_blocks_lookup[item]:\n",
    "        storage_dict[user] = {}\n",
    "    \n",
    "    # save dictionary\n",
    "    with open('synthetic_ratings/similarity_storage_synth_items2k'+str(item)+'.json', 'w') as convert_file:\n",
    "        convert_file.write(json.dumps(storage_dict))\n",
    "    \n",
    "    del storage_dict\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82e092bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "precompute_matrix = matrix_array[:134000].T\n",
    "precompute_matrix2 = matrix_array[134000:].T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "780bc756",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# TEMP STUFF\n",
    "\n",
    "block_indices_lookup = {}\n",
    "\n",
    "start = 0\n",
    "incrementer = 0\n",
    "\n",
    "\n",
    "end = start + chunk_size\n",
    "\n",
    "block_indices_lookup[1] = {}\n",
    "block_indices_lookup[1][\"Start\"] = start\n",
    "block_indices_lookup[1][\"End\"] = end\n",
    "\n",
    "print(\"\\nLookup dictionary complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82091282",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_user_block(array_chunk_a, matrix, indices):\n",
    "\n",
    "    this_start = time.time()\n",
    "    array_chunk_b = matrix[indices, :]  # .astype('float32'))\n",
    "    checkpoint = time.time()\n",
    "\n",
    "    a = tf.constant(array_chunk_a, dtype=tf.float32)\n",
    "\n",
    "    b = tf.constant(array_chunk_b, dtype=tf.float32)\n",
    "\n",
    "    checkpoint1 = time.time()\n",
    "    # print(str(checkpoint1-checkpoint)+\" Loaded into Tensors\")\n",
    "\n",
    "    a = tf.nn.l2_normalize(a, 1)\n",
    "    b = tf.nn.l2_normalize(b, 0)\n",
    "\n",
    "    checkpoint2 = time.time()\n",
    "    # print(str(checkpoint2-checkpoint1)+\" normalized\")\n",
    "\n",
    "    similarities = tf.matmul(a, b)\n",
    "    checkpoint3 = time.time()\n",
    "    # print(str(checkpoint3-checkpoint2)+\" Got Similarity Scores\")\n",
    "\n",
    "    user_similarities = similarities.numpy().reshape(-1, 1)\n",
    "\n",
    "    return user_similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fd29801",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "time_test = []\n",
    "\n",
    "# for each user block in the block_indices_lookup. The user blocks are integers from 1-20\n",
    "for user_block in block_indices_lookup:\n",
    "\n",
    "    print(\"Starting block \" + str(user_block))\n",
    "\n",
    "    # Get the start and end indexes for the block\n",
    "    starting_block_indexes = block_indices_lookup[user_block]\n",
    "    base_start = starting_block_indexes[\"Start\"]  # starting user\n",
    "    base_end = starting_block_indexes[\"End\"]  # ending user\n",
    "\n",
    "    # Load the storage dictionary for this block\n",
    "    with open(\n",
    "        \"real_ratings/similarity_storage_real_\" + str(user_block) + \".json\"\n",
    "    ) as json_file:\n",
    "        base_users_storage = json.load(json_file)\n",
    "\n",
    "    # only do the user ids in this block, then save to the fils\n",
    "    for user_id in np.arange(base_start, 1001, 1):  # base_end, 1):\n",
    "        print(user_id)\n",
    "\n",
    "        user_name = user_id_lookup[user_id + 1]\n",
    "        # print(user_name)\n",
    "\n",
    "        # log start time\n",
    "        # print(\"Making matrices\")\n",
    "        start = time.time()\n",
    "\n",
    "        # make the single user matrix for the one user\n",
    "        single_user = matrix_array[user_id].reshape(1, -1)\n",
    "        # get the indices where the user is nonzero\n",
    "        indices = list(np.nonzero(single_user)[1])\n",
    "        # make the user with only the nonzero indices\n",
    "        array_chunk_a = single_user[:, indices]  # .astype('float32')\n",
    "        # normalize_a = normalize(array_chunk_a, axis=1)\n",
    "        checkpoint = time.time()\n",
    "        # print(str(checkpoint-start)+\" Processed single user\")\n",
    "\n",
    "        # process_user_block(a, precompute_matrix, indices)\n",
    "        user_similarities_1 = process_user_block(\n",
    "            array_chunk_a, precompute_matrix, indices\n",
    "        )\n",
    "        user_similarities_2 = process_user_block(\n",
    "            array_chunk_a, precompute_matrix2, indices\n",
    "        )\n",
    "        # user_similarities = process_user_block(array_chunk_a, precompute_matrix, indices)\n",
    "\n",
    "        checkpoint3 = time.time()\n",
    "        user_similarities = np.append(user_similarities_1, user_similarities_2)\n",
    "        max_spot = np.argmax(user_similarities)\n",
    "        mean_spot = np.median(user_similarities)\n",
    "        user_similarities[max_spot] = mean_spot\n",
    "        scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "        user_similarities = scaler.fit_transform(\n",
    "            user_similarities.reshape(-1, 1)\n",
    "        ).ravel()\n",
    "        # user_similarities = list(np.round(user_similarities, 2).ravel())\n",
    "\n",
    "        checkpoint4 = time.time()\n",
    "        # print(str(checkpoint4-checkpoint3)+\" Processed/Scaled Similarity scores\")\n",
    "\n",
    "        over75 = list((user_similarities >= 0.6).nonzero()[0])\n",
    "        under75 = list((user_similarities <= -0.6).nonzero()[0])\n",
    "        all_comps = over75 + under75\n",
    "\n",
    "        for item in all_comps:\n",
    "            item = int(item)\n",
    "            base_users_storage[user_name][item] = round(\n",
    "                float(user_similarities[item]), 2\n",
    "            )\n",
    "\n",
    "        checkpoint5 = time.time()\n",
    "        # print(str(checkpoint5-checkpoint4)+\" Stored scores in dictionary\\n\")\n",
    "\n",
    "        end = time.time()\n",
    "        elapsed = end - start\n",
    "        # print(str(elapsed)+' seconds elapsed for this user\\n\\n')\n",
    "        time_test.append(elapsed)\n",
    "\n",
    "    print(\"Saving dictionary for this set of users\")\n",
    "    # save dictionary\n",
    "    with open(\n",
    "        \"real_ratings/similarity_storage_real_\" + str(user_block) + \".json\", \"w\"\n",
    "    ) as convert_file:\n",
    "        convert_file.write(json.dumps(base_users_storage))\n",
    "\n",
    "    avg_time = mean(time_test)\n",
    "    print(\"Average time per user: \" + str(avg_time))\n",
    "\n",
    "    del base_users_storage\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a259b5e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_users_storage[\"cfarrell\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "300e0ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(base_users_storage[\"Torsten\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3601beb",
   "metadata": {},
   "source": [
    "## Make smaller ratings blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29c789fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Opening JSON file\n",
    "with open(\"real_ratings/user_ratings_unscaled.json\") as json_file:\n",
    "    user_ratings = json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66bd98bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_users = list(user_ratings.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "047cb8f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(all_users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41dcc29d",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_block_1 = all_users[:40000]\n",
    "user_block_2 = all_users[40000:80000]\n",
    "user_block_3 = all_users[80000:120000]\n",
    "user_block_4 = all_users[120000:160000]\n",
    "user_block_5 = all_users[160000:200000]\n",
    "user_block_6 = all_users[200000:240000]\n",
    "user_block_7 = all_users[240000:]\n",
    "\n",
    "user_blocks = [\n",
    "    user_block_1,\n",
    "    user_block_2,\n",
    "    user_block_3,\n",
    "    user_block_4,\n",
    "    user_block_5,\n",
    "    user_block_6,\n",
    "    user_block_7,\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abd756d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "iteration = 0\n",
    "\n",
    "for block in user_blocks:\n",
    "\n",
    "    iteration += 1\n",
    "\n",
    "    print(\"Starting block \" + str(iteration))\n",
    "\n",
    "    block_of_users = {key: value for key, value in user_ratings.items() if key in block}\n",
    "\n",
    "    # for scaled only:\n",
    "    for person in block_of_users:\n",
    "        # user_mean = mean(block_of_users[person].values())\n",
    "        for item in block_of_users[person]:\n",
    "            # new_value = round((block_of_users[person][item] - user_mean), 2)\n",
    "            new_value = block_of_users[person][item]\n",
    "            block_of_users[person][item] = new_value\n",
    "\n",
    "    # save dictionary\n",
    "    with open(\n",
    "        \"real_ratings/user_ratings_block_unscaled_\" + str(iteration) + \".json\", \"w\"\n",
    "    ) as convert_file:\n",
    "        convert_file.write(json.dumps(block_of_users))\n",
    "\n",
    "    del block_of_users\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc55066c",
   "metadata": {},
   "outputs": [],
   "source": [
    "del user_blocks\n",
    "del user_ratings\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ee8c631",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make dataframe from synthetic sort and melt to longform\n",
    "synthetic_user_ratings = pd.DataFrame.from_dict(synthetic_users_dictionary)\n",
    "synthetic_user_ratings.reset_index(inplace=True)\n",
    "synthetic_user_ratings.rename(columns={\"index\": \"BGGId\", user: \"Rating\"}, inplace=True)\n",
    "synthetic_user_ratings[\"Rating\"] = synthetic_user_ratings[\"Rating\"] + user_mean\n",
    "\n",
    "\n",
    "synthetic_user_ratings = pd.DataFrame.from_dict(synthetic_users_dictionary).T\n",
    "synthetic_user_ratings.reset_index(inplace=True)\n",
    "synthetic_user_ratings.rename(columns={\"index\": \"UserID\"}, inplace=True)\n",
    "synthetic_user_ratings_long = synthetic_user_ratings.melt(\n",
    "    id_vars=\"UserID\", var_name=\"BGGId\", value_name=\"Rating\"\n",
    ").dropna()\n",
    "synthetic_user_ratings_long.sort_values(\"UserID\", inplace=True)\n",
    "synthetic_user_ratings_long\n",
    "\n",
    "# save longform\n",
    "synthetic_user_ratings_long.to_pickle(\n",
    "    \"synthetic_ratings_new_scraper/synthetic_ratings_\" + path + \"_\" + number + \".pkl\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45d17159",
   "metadata": {},
   "outputs": [],
   "source": [
    "def produce_synthetic_ratings(user, temp_users_dictionary, num_ratings_create):\n",
    "    \"\"\"\n",
    "    Takes in a dictionary of user's ratings and the number of ratings to synthesize\n",
    "    Synthesizes ratings and creates a dictionary of all synthesized ratings for the user\n",
    "    Returns synthesized ratings\n",
    "\n",
    "    Inputs:\n",
    "    user: the user id to create ratings for\n",
    "    temp_users_dictionary: dictionary of specific user's real ratings\n",
    "    num_ratings_create : simple number. # Ratings to make in the run.\n",
    "\n",
    "    Outputs:\n",
    "    user_comps_dict : dictionary of synthesized ratings specifically for user\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"Producing items for user\")\n",
    "\n",
    "    # start at iteration 0\n",
    "    iteration = 0\n",
    "\n",
    "    # set up dict to store all specific comps for this user\n",
    "    users_comp_dict = {}\n",
    "\n",
    "    # populate the comps with the user's baseline items\n",
    "    for item in temp_users_dictionary:\n",
    "        users_comp_dict[item] = [1, 1, item, 0, 0, temp_users_dictionary[item]]\n",
    "        # overall confidence, this item similarity, item, iteration, degrees away, item name\n",
    "\n",
    "    # while the list of items that the user rated is < the number of ratings needed:\n",
    "    while len(users_comp_dict.keys()) < num_ratings_create:\n",
    "\n",
    "        users_rated_items = list(temp_users_dictionary.keys())\n",
    "\n",
    "        iteration += 1  # advance the iteration\n",
    "\n",
    "        new_items = []  # make a list to hold the items for this iteration\n",
    "\n",
    "        # for each rated item:\n",
    "        for rated in users_rated_items:\n",
    "\n",
    "            print(\"\\nCurrent item: \" + str(rated))\n",
    "            # get rating for current item\n",
    "            rated_rating = temp_users_dictionary[rated]\n",
    "            print(rated_rating)\n",
    "\n",
    "            # get current best comp:\n",
    "            current_position = 0\n",
    "            current_comp = game_comps_byid_lookup[rated][0][current_position]\n",
    "\n",
    "            while current_comp in new_items:\n",
    "\n",
    "                # increment position\n",
    "                current_position += 1\n",
    "\n",
    "                if current_position >= 21923:\n",
    "                    # print(current_position)\n",
    "                    break\n",
    "\n",
    "                else:\n",
    "                    # reset current comp to new position new_items\n",
    "                    current_comp = game_comps_byid_lookup[rated][0][current_position]\n",
    "\n",
    "                    # continue back to check\n",
    "                    continue\n",
    "\n",
    "            # any time the current comp is in users_rated_items already:\n",
    "            while current_comp in users_comp_dict.keys():\n",
    "\n",
    "                # increment position\n",
    "                current_position += 1\n",
    "\n",
    "                if current_position >= 21923:\n",
    "                    # print(current_position)\n",
    "                    break\n",
    "\n",
    "                else:\n",
    "\n",
    "                    # reset current comp to new position users_comp_dict\n",
    "                    current_comp = game_comps_byid_lookup[rated][0][current_position]\n",
    "\n",
    "                    # continue back to check\n",
    "                    continue\n",
    "\n",
    "            # The next section activates once the current comp is not already in the user's rated items\n",
    "\n",
    "            if current_position >= 21923:\n",
    "                # print(current_position)\n",
    "                break\n",
    "\n",
    "            else:\n",
    "\n",
    "                # getting similarity of the current comp\n",
    "                comp_similarity = game_comps_byid_lookup[rated][1][current_position]\n",
    "                print(current_position)\n",
    "                print(comp_similarity)\n",
    "\n",
    "                # get the synthetic rating for the item by taking the rating of the base item * similarity\n",
    "                synthetic_rating = rated_rating * comp_similarity\n",
    "                print(synthetic_rating)\n",
    "\n",
    "                # get the overall confidence of this rating\n",
    "                # confidence = confidence of prior item * similarity of current item\n",
    "                confidence = users_comp_dict[rated][0] * comp_similarity\n",
    "                degrees = users_comp_dict[rated][4] + 1\n",
    "\n",
    "                # add this item to the list of new items we are adding to the ratings this round\n",
    "                new_items.append(current_comp)\n",
    "\n",
    "                # make the user's comp dict\n",
    "                users_comp_dict[current_comp] = [\n",
    "                    confidence,\n",
    "                    comp_similarity,\n",
    "                    rated,\n",
    "                    iteration,\n",
    "                    degrees,\n",
    "                    synthetic_rating,\n",
    "                ]\n",
    "\n",
    "                # update the temporary dictionary with the synthetic rating for the item\n",
    "                temp_users_dictionary[current_comp] = synthetic_rating\n",
    "\n",
    "        continue\n",
    "\n",
    "    print(\"End length of rated items is \" + str(len(users_comp_dict)) + \"\\n\")\n",
    "\n",
    "    return users_comp_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10cf1052",
   "metadata": {},
   "outputs": [],
   "source": [
    "# user_matrix = pd.read_pickle('data_store/data_cleaned/ratings_matrix_cleaned_03.pkl')\n",
    "# user_matrix = user_matrix.T\n",
    "# user_matrix.index = user_matrix.index.astype('int32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5554cf0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# run the data synthesizer for each of the 6 ratings matrix files\n",
    "process_to_synthetic(item, num_ratings_create, desired_ratings, game_ids, \"250\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb2bfc33",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_user(user_items, user, game_ids):\n",
    "    \"\"\"\n",
    "    Takes in user's rated items, a the username, and a list of game_ids\n",
    "    Get the mean for the user\n",
    "    Builds a list of user's rated items and subtracts user mean from all ratings\n",
    "    Builds a corresponding list of game ids for the rated games\n",
    "    Gets intersection of user's rated ids with the overall game_ids\n",
    "    Stores user game_id:rating in user ratings dictionary\n",
    "    Returns the user dictionary\n",
    "\n",
    "    Inputs:\n",
    "    user_items: dataframe column of user's rated items\n",
    "    user: user to retrieve\n",
    "    game_ids: the game_ids we are using in our recommender\n",
    "\n",
    "    Outputs:\n",
    "    overall_user: user dictionary with user's ratings\n",
    "    \"\"\"\n",
    "\n",
    "    # get the mean rating for that user\n",
    "    user_mean = user_items.mean()\n",
    "\n",
    "    # normalize the ratings for that user by subtracting their mean from all ratings, store in list\n",
    "    game_ratings_normed = list(user_items - user_mean)\n",
    "\n",
    "    # Get a list of all of the game IDs that the user rated\n",
    "    users_game_ids = list(user_items.index)\n",
    "\n",
    "    # get the set of usable game ids\n",
    "    game_ids_set = set(game_ids).intersection(set(users_game_ids))\n",
    "\n",
    "    # make user storage dictionary\n",
    "    user_ratings = {}\n",
    "\n",
    "    # for the key/value pairs of game_ids and normalized ratings\n",
    "    for key, value in zip(users_game_ids, game_ratings_normed):\n",
    "        user_ratings[key] = value\n",
    "\n",
    "    # make a dictionary to store the intersected ratings\n",
    "    set_dictionary = {}\n",
    "\n",
    "    # for each matching key, value in game_ids and game_ratings for the user\n",
    "    for item in game_ids_set:\n",
    "        set_dictionary[item] = user_ratings[item]\n",
    "\n",
    "    # store the user's ratings\n",
    "    overall_user = set_dictionary\n",
    "\n",
    "    return overall_user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "168fd9a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_matrix_to_synthetic(\n",
    "    path, num_ratings_create, desired_ratings, game_ids, number\n",
    "):\n",
    "    \"\"\"\n",
    "    Process a user matrix and create synthetic data for each user in the matrix\n",
    "\n",
    "    Inputs:\n",
    "    Path: path appendation for file\n",
    "    num_ratings_create: The total number of minimum ratings per user\n",
    "    desired_ratings: the needed number of ratings per user\n",
    "    \"\"\"\n",
    "\n",
    "    # load and transpose data frame\n",
    "    user_matrix = pd.read_pickle(\n",
    "        \"data_store/data_cleaned/ratings_matrix_cleaned_\" + path + \".pkl\"\n",
    "    )\n",
    "    user_matrix.drop_duplicates(keep=\"first\", inplace=True)\n",
    "    user_matrix = user_matrix.T\n",
    "    user_matrix.index = user_matrix.index.astype(\"int32\")\n",
    "\n",
    "    # set up a synthetic ratings dictionary to store the users and ratings\n",
    "    synthetic_users_dictionary = {}\n",
    "\n",
    "    # for each user in the test matrix:\n",
    "    for user in user_matrix.columns:\n",
    "\n",
    "        print(\"Starting user \" + user)\n",
    "\n",
    "        user_items = user_matrix[user].dropna(axis=0)\n",
    "\n",
    "        # copy the current user dictionary to a temp storage dictionary that we can manipulate\n",
    "        synthetic_users_dictionary[user] = get_user(user_items, user, game_ids)\n",
    "        temp_users_dictionary = copy.deepcopy(synthetic_users_dictionary[user])\n",
    "\n",
    "        # get the original number of ratings by this user\n",
    "        original_num_ratings = len(temp_users_dictionary)\n",
    "        print(\"User starts with \" + str(original_num_ratings) + \" ratings\")\n",
    "\n",
    "        # call function to produce synthetic ratings\n",
    "        user_comps_dict = produce_synthetic_ratings(\n",
    "            user, temp_users_dictionary, num_ratings_create\n",
    "        )\n",
    "        # call sort function for top synthetic ratings\n",
    "        sort_synthetic_ratings(\n",
    "            user,\n",
    "            synthetic_users_dictionary,\n",
    "            user_comps_dict,\n",
    "            original_num_ratings,\n",
    "            desired_ratings,\n",
    "        )\n",
    "\n",
    "    # make dataframe from synthetic sort and melt to longform\n",
    "    synthetic_user_ratings = pd.DataFrame.from_dict(synthetic_users_dictionary).T\n",
    "    synthetic_user_ratings.reset_index(inplace=True)\n",
    "    synthetic_user_ratings.rename(columns={\"index\": \"UserID\"}, inplace=True)\n",
    "    synthetic_user_ratings_long = synthetic_user_ratings.melt(\n",
    "        id_vars=\"UserID\", var_name=\"BGGId\", value_name=\"Rating\"\n",
    "    ).dropna()\n",
    "    synthetic_user_ratings_long.sort_values(\"UserID\", inplace=True)\n",
    "    synthetic_user_ratings_long\n",
    "\n",
    "    # save longform\n",
    "    synthetic_user_ratings_long.to_pickle(\n",
    "        \"synthetic_ratings_new_scraper/synthetic_ratings_\"\n",
    "        + path\n",
    "        + \"_\"\n",
    "        + number\n",
    "        + \".pkl\"\n",
    "    )\n",
    "\n",
    "    # save dictionary\n",
    "    with open(\n",
    "        \"synthetic_ratings_new_scraper/users_dump_syntheticratings\"\n",
    "        + path\n",
    "        + \"_\"\n",
    "        + number\n",
    "        + \".json\",\n",
    "        \"w\",\n",
    "    ) as convert_file:\n",
    "        convert_file.write(json.dumps(synthetic_users_dictionary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c86f2ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_synthetic_ratings(\n",
    "    user,\n",
    "    synthetic_users_dictionary,\n",
    "    user_comps_dict,\n",
    "    original_num_ratings,\n",
    "    desired_ratings,\n",
    "):\n",
    "    \"\"\"\n",
    "    Takes the user's synthesized comps dict, the original number of ratings the user made,\n",
    "    and the desired number of ratings the user needs.\n",
    "    Creates a df sorting the synthesized ratings by confidence level,\n",
    "    keeping the highest confidence if an item was recommended more than once.\n",
    "    Evaluates number of ratings needed to reach 500 and keeps only that many ratings with the highest confidence.\n",
    "    For each item kept, logs the synthetic rating to the user;s dictionary\n",
    "\n",
    "    Inputs:\n",
    "    user: specific user to sort\n",
    "    synthetic_users_dictionary: reference to the dictionary of synthesized items\n",
    "    user_comps_dict: dictionary of synthesized ratings specifically for user\n",
    "    original_num_ratings: The number of ratings the user actually rated\n",
    "    desired_ratings: the number of ratings needed by the user\n",
    "\n",
    "    \"\"\"\n",
    "    print(\"Sorting user items\")\n",
    "\n",
    "    # showing synthetic ratings only\n",
    "    user_comps_df = (\n",
    "        pd.DataFrame(\n",
    "            user_comps_dict.values(),\n",
    "            index=user_comps_dict.keys(),\n",
    "            columns=[\n",
    "                \"OverallConfidence\",\n",
    "                \"SimtoLast\",\n",
    "                \"RecFrom\",\n",
    "                \"Iteration\",\n",
    "                \"DegreesAway\",\n",
    "                \"SyntheticRating\",\n",
    "            ],\n",
    "        )\n",
    "        .sort_values(\"OverallConfidence\", ascending=False)\n",
    "        .drop_duplicates(keep=\"first\")\n",
    "    )\n",
    "\n",
    "    # get a list of the ratings to keep (past the real ratings)\n",
    "    keep_items = sorted(list(user_comps_df[:desired_ratings].index))\n",
    "\n",
    "    # for each item that we keep,\n",
    "    for item in keep_items:\n",
    "\n",
    "        # add the rating to the real storage dictionary\n",
    "        synthetic_users_dictionary[user][item] = user_comps_dict[item]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a861416",
   "metadata": {},
   "source": [
    "## Deprecated Matrix Calculations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9500f98",
   "metadata": {},
   "source": [
    "### Using Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73f4413c",
   "metadata": {},
   "outputs": [],
   "source": [
    "larger_matrix = pd.read_pickle(\"synthetic_ratings/users_synthetic_2193_fullmatrix.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf518050",
   "metadata": {},
   "outputs": [],
   "source": [
    "larger_matrix.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecc9d5ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "users = list(larger_matrix.index)\n",
    "users[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c14e553",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_lookup_table = {}\n",
    "\n",
    "user_key = -1\n",
    "\n",
    "for user in users:\n",
    "\n",
    "    user_key += 1\n",
    "\n",
    "    user_lookup_table[int(user_key)] = users[user_key]\n",
    "\n",
    "# save dictionary\n",
    "with open(\"user_lookup_table.json\", \"w\") as convert_file:\n",
    "    convert_file.write(json.dumps(user_lookup_table))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa11f2ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "%time values1 = larger_matrix.loc['Torsten'].values\n",
    "values1[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9748a391",
   "metadata": {},
   "outputs": [],
   "source": [
    "%time values2 = larger_matrix.loc['mitnachtKAUBO-I'].values\n",
    "values2[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b468c7de",
   "metadata": {},
   "outputs": [],
   "source": [
    "%time spatial.distance.cosine(values1,values2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b68c5b74",
   "metadata": {},
   "outputs": [],
   "source": [
    "del similarity_dictionary\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e40f097c",
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_dictionary = {}\n",
    "\n",
    "for user in users:\n",
    "\n",
    "    similarity_dictionary[user] = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e60af8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for user in users[:1]:\n",
    "\n",
    "    start = time.time()\n",
    "    user_values = larger_matrix.loc[user].values\n",
    "\n",
    "    for other_user in users:\n",
    "\n",
    "        if user in similarity_dictionary[other_user]:\n",
    "            continue\n",
    "\n",
    "        else:\n",
    "\n",
    "            other_user_values = larger_matrix.loc[other_user].values\n",
    "            similarity = 1 - spatial.distance.cosine(user_values, other_user_values)\n",
    "            similarity_dictionary[user][other_user] = similarity\n",
    "            similarity_dictionary[other_user][user] = similarity\n",
    "\n",
    "    end = time.time()\n",
    "    print(str(end - start) + \" seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84f603a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "del larger_matrix\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da435220",
   "metadata": {},
   "source": [
    "### Using Numpy Arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eac0ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix_array = larger_matrix.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7e32a13",
   "metadata": {},
   "outputs": [],
   "source": [
    "del larger_matrix\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdf5422a",
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14b3035c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%time values1 = matrix_array[0]\n",
    "values1[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dba2758c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%time values2 = matrix_array[1]\n",
    "values2[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2714ff86",
   "metadata": {},
   "outputs": [],
   "source": [
    "%time spatial.distance.cosine(values1,values2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c0e3ccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "%time similarities = np.matmul(matrix_array[0:10000], matrix_array[0:10000].T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e719c4e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "similarities[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1585ceaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "del similarities\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c98abc44",
   "metadata": {},
   "outputs": [],
   "source": [
    "%time similarities = np.dot(matrix_array[0:10000], matrix_array[0:10000].T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "282e5125",
   "metadata": {},
   "outputs": [],
   "source": [
    "similarities[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15f7a99a",
   "metadata": {},
   "outputs": [],
   "source": [
    "del similarities\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "154e6aa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%time similarities = matrix_array[0:10000]@matrix_array[0:10000].T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "076683a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "similarities[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feaebdef",
   "metadata": {},
   "outputs": [],
   "source": [
    "del similarities\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8042285",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6041eb38",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2abdbb83",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "241c87f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "len_users = len(users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0674b3d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "del similarity_dictionary\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c587d57",
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_dictionary = {}\n",
    "\n",
    "for user in np.arange(0, len_users, 1):\n",
    "\n",
    "    similarity_dictionary[user] = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45805530",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(similarity_dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfce4de6",
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_dictionary[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d997482",
   "metadata": {},
   "outputs": [],
   "source": [
    "for user in np.arange(0, len_users, 1)[:1]:\n",
    "\n",
    "    start = time.time()\n",
    "    user_values = matrix_array[user].reshape(-1, 1)\n",
    "\n",
    "    other_matrix = matrix_array[user + 1 :]\n",
    "\n",
    "    similarities = cosine_similarity(other_matrix, user_values)\n",
    "\n",
    "    end = time.time()\n",
    "    print(str(end - start) + \" seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cca6557",
   "metadata": {},
   "outputs": [],
   "source": [
    "similarities.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8355f899",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ed3a3c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72fbacab",
   "metadata": {},
   "outputs": [],
   "source": [
    "for user in np.arange(0, len_users, 1)[:3]:\n",
    "\n",
    "    start = time.time()\n",
    "    user_values = matrix_array[user]\n",
    "\n",
    "    for other_user in np.arange(0, len_users, 1):\n",
    "\n",
    "        if user in similarity_dictionary[other_user]:\n",
    "            continue\n",
    "\n",
    "        else:\n",
    "\n",
    "            other_user_values = matrix_array[other_user]\n",
    "            similarity = 1 - spatial.distance.cosine(user_values, other_user_values)\n",
    "            similarity_dictionary[user][other_user] = similarity\n",
    "            similarity_dictionary[other_user][user] = similarity\n",
    "\n",
    "    end = time.time()\n",
    "    print(str(end - start) + \" seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b532ab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_dictionary[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a702ca58",
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_dictionary[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "654f4075",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccfb189e",
   "metadata": {},
   "outputs": [],
   "source": [
    "del matrix_array\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0933cd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# larger_matrix_T = pd.read_pickle('synthetic_ratings/users_synthetic_2193_fullmatrixT.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66586640",
   "metadata": {},
   "source": [
    "## Different ways to make calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56b71814",
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix_sparsed = pd.read_pickle(\n",
    "    \"synthetic_ratings/users_synthetic_2193_sparsematrix.pkl\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9557c865",
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix_sparsed.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c2f0a9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix_sparsed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d710272a",
   "metadata": {},
   "outputs": [],
   "source": [
    "users = list(matrix_sparsed.index)\n",
    "users[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32488712",
   "metadata": {},
   "source": [
    "### Chunks, sparse non-normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09f5cddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "%time sparse_matrix = csr_matrix(matrix_sparsed.sparse.to_coo())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "981a033a",
   "metadata": {},
   "outputs": [],
   "source": [
    "del matrix_sparsed\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "302fe813",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(sparse_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f1169c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c392c36",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sparse_matrix.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6fe5aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "%time similarities = cosine_similarity(sparse_matrix[0:10000], sparse_matrix[0:10000], dense_output=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c34df71",
   "metadata": {},
   "outputs": [],
   "source": [
    "similarities[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58e6f1be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change chunk_size to control resource consumption and speed\n",
    "# Higher chunk_size means more memory/RAM needed but also faster\n",
    "chunk_size = 10000\n",
    "matrix_len = sparse_matrix.shape[0]\n",
    "\n",
    "\n",
    "def similarity_cosine_by_chunk(start, end, dense):\n",
    "    if end > matrix_len:\n",
    "        end = matrix_len\n",
    "    return cosine_similarity(\n",
    "        X=sparse_matrix[start:end], Y=sparse_matrix, dense_output=dense\n",
    "    )  # scikit-learn function\n",
    "\n",
    "\n",
    "# for chunk_start in range(0, 10, chunk_size):\n",
    "# cosine_similarity_chunk = similarity_cosine_by_chunk(chunk_start, chunk_start+chunk_size)\n",
    "%time cosine_similarity_chunk = similarity_cosine_by_chunk(0, 10000, dense=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5f3f0dc",
   "metadata": {},
   "source": [
    "- Time for size 1, dense output: 39.4s\n",
    "- Time for size 1000, dense output: 8min 48s\n",
    "- Time for size 1, compact output: 47.8s\n",
    "- Time for size 10000, compact output: 1h 41min 6s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "374fb615",
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_similarity_chunk[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7c88325",
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b94919fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse_matrix[0:10000].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75e2c051",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change chunk_size to control resource consumption and speed\n",
    "# Higher chunk_size means more memory/RAM needed but also faster\n",
    "chunk_size = 10000\n",
    "matrix_len = sparse_matrix.shape[0]\n",
    "\n",
    "\n",
    "def similarity_cosine_by_chunk(start, end, dense):\n",
    "    if end > matrix_len:\n",
    "        end = matrix_len\n",
    "    return np.matmul(sparse_matrix[start:end], sparse_matrix)  # scikit-learn function\n",
    "\n",
    "\n",
    "# for chunk_start in range(0, 10, chunk_size):\n",
    "# cosine_similarity_chunk = similarity_cosine_by_chunk(chunk_start, chunk_start+chunk_size)\n",
    "# %time cosine_similarity_chunk = similarity_cosine_by_chunk(0, 10000, dense=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c3ab31e",
   "metadata": {},
   "outputs": [],
   "source": [
    "similarities[:1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dc71525",
   "metadata": {},
   "source": [
    "### Chunks, normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef18d6cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "normed_matrix = pp.normalize(sparse_matrix.tocsc(), axis=0)\n",
    "del sparse_matrix\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2806570d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change chunk_size to control resource consumption and speed\n",
    "# Higher chunk_size means more memory/RAM needed but also faster\n",
    "chunk_size = 1000\n",
    "matrix_len = normed_matrix.shape[0]\n",
    "\n",
    "\n",
    "def similarity_cosine_by_chunk(start, end, dense=False):\n",
    "    if end > matrix_len:\n",
    "        end = matrix_len\n",
    "    return cosine_similarity(\n",
    "        X=normed_matrix[start:end], Y=normed_matrix, dense_output=dense\n",
    "    )  # scikit-learn function\n",
    "\n",
    "\n",
    "# for chunk_start in range(0, 10, chunk_size):\n",
    "# cosine_similarity_chunk = similarity_cosine_by_chunk(chunk_start, chunk_start+chunk_size)\n",
    "%time cosine_similarity_chunk = similarity_cosine_by_chunk(0, 1, dense=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed842b58",
   "metadata": {},
   "source": [
    "Time for size 1, dense output: 1min 51s\n",
    "Time for size 1000, dense output: 10min 20s\n",
    "Time for size 1, compact output: 1min 51s\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4290b82",
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_similarity_chunk.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5be6f2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_similarity_chunk[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9dc2c78",
   "metadata": {},
   "outputs": [],
   "source": [
    "user = \"Torsten\"\n",
    "\n",
    "%time sparse_user =  csr_matrix(matrix_sparsed.loc[user])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "152cc5b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse_user.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4590056f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sparseuser_AB = sparse_matrix.multiply(sparse_user)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0427749b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sparseuser_AB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ffeef70",
   "metadata": {},
   "source": [
    "### Old function with comparison blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25f24bb4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05b51313",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for user_block in block_indices_lookup:\n",
    "\n",
    "    print(\"Starting block \" + str(user_block))\n",
    "\n",
    "    starting_block_indexes = block_indices_lookup[user_block]\n",
    "    base_start = starting_block_indexes[\"Start\"]\n",
    "    base_end = starting_block_indexes[\"End\"]\n",
    "\n",
    "    array_chunk_a = (matrix_array[base_start:base_end] / 10).astype(\"float32\")\n",
    "\n",
    "    # Opening JSON file\n",
    "    with open(\n",
    "        \"user_similarities/similarity_storage\" + str(user_block) + \".json\"\n",
    "    ) as json_file:\n",
    "        base_users_storage = json.load(json_file)\n",
    "\n",
    "    first_block_of_comparison = user_block\n",
    "    end_range = len(block_indices_lookup) + 1\n",
    "\n",
    "    # TEMPORARY END RANGE FOR TESTINGS\n",
    "    end_range = 2\n",
    "\n",
    "    for comparison_block in np.arange(first_block_of_comparison, end_range, 1):\n",
    "\n",
    "        print(\n",
    "            \"User Block \"\n",
    "            + str(user_block)\n",
    "            + \" vs Comparison Block \"\n",
    "            + str(comparison_block)\n",
    "        )\n",
    "\n",
    "        # Opening JSON file\n",
    "        with open(\n",
    "            \"user_similarities/similarity_storage\" + str(comparison_block) + \".json\"\n",
    "        ) as json_file:\n",
    "            comparison_users_storage = json.load(json_file)\n",
    "\n",
    "        comparison_indexes = block_indices_lookup[comparison_block]\n",
    "        compare_start = comparison_indexes[\"Start\"]\n",
    "        compare_end = comparison_indexes[\"End\"]\n",
    "\n",
    "        print(\"Making matrices\")\n",
    "        start = time.time()\n",
    "        array_chunk_b = ((matrix_array[compare_start:compare_end].T) / 10).astype(\n",
    "            \"float32\"\n",
    "        )\n",
    "\n",
    "        a = tf.constant(array_chunk_a)\n",
    "        b = tf.constant(array_chunk_b)\n",
    "\n",
    "        normalize_a = tf.nn.l2_normalize(a, 1)\n",
    "        del a\n",
    "        gc.collect()\n",
    "\n",
    "        normalize_b = tf.nn.l2_normalize(b, 0)\n",
    "        del b\n",
    "        gc.collect()\n",
    "\n",
    "        print(\"Getting similarity scores\")\n",
    "        similarities = tf.matmul(normalize_a, normalize_b)  # , adjoint_b=True)\n",
    "        del normalize_a\n",
    "        del normalize_b\n",
    "        gc.collect()\n",
    "\n",
    "        # store user info\n",
    "\n",
    "        incrementer_base = 0\n",
    "\n",
    "        print(\"Storing Similarities\")\n",
    "        for base_user in user_blocks_lookup[user_block][:5]:\n",
    "\n",
    "            print(base_user)\n",
    "\n",
    "            user_similarities = similarities[incrementer_base].numpy()\n",
    "            max_spot = np.argmax(user_similarities.max())\n",
    "            mean_spot = np.median(user_similarities)\n",
    "            user_similarities[max_spot] = mean_spot\n",
    "            scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "            user_similarities = scaler.fit_transform(user_similarities.reshape(-1, 1))\n",
    "            user_similarities = list(np.round(user_similarities, 2).ravel())\n",
    "\n",
    "            for key, value in list(\n",
    "                zip(\n",
    "                    user_blocks_lookup[comparison_block][incrementer_base:],\n",
    "                    user_similarities[incrementer_base:],\n",
    "                )\n",
    "            ):\n",
    "                if value >= 0.25 or value <= -0.25:\n",
    "                    base_users_storage[base_user][key] = float(value)\n",
    "                if user_block != comparison_block:\n",
    "                    comparison_users_storage[key][base_user] = float(value)\n",
    "\n",
    "            incrementer_base += 1\n",
    "\n",
    "            # save dictionary\n",
    "            with open(\n",
    "                \"user_similarities/similarity_storage\"\n",
    "                + str(comparison_block)\n",
    "                + \".json\",\n",
    "                \"w\",\n",
    "            ) as convert_file:\n",
    "                convert_file.write(json.dumps(comparison_users_storage))\n",
    "\n",
    "        print(\"Cleaning up memory for this iteration\")\n",
    "        del comparison_users_storage\n",
    "        # del similarities\n",
    "        gc.collect()\n",
    "\n",
    "        end = time.time()\n",
    "        print(str(end - start) + \" seconds elapsed for this comparison section\")\n",
    "\n",
    "    # save dictionary\n",
    "    with open(\n",
    "        \"user_similarities/similarity_storage\" + str(user_block) + \".json\", \"w\"\n",
    "    ) as convert_file:\n",
    "        convert_file.write(json.dumps(base_users_storage))\n",
    "\n",
    "    # del base_users_storage\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e97fba49",
   "metadata": {},
   "source": [
    "## Deprecated Tensorflow time reduction attempts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbe39421",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# the basic file required for this work - the full matrix\n",
    "\n",
    "larger_matrix = pd.read_pickle(\n",
    "    \"synthetic_ratings/users_synthetic_2193_sparsematrix_nogameids.pkl\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e24e0649",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "larger_matrix.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ccd4233",
   "metadata": {},
   "outputs": [],
   "source": [
    "larger_matrix.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bb8e108",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sparse dataframe into numpy array\n",
    "\n",
    "matrix_array = np.array(larger_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19c694b9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b578027a",
   "metadata": {},
   "source": [
    "Turn single user into a column 21921, 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4584f009",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_id = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78f51021",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get single user from matrix_array\n",
    "\n",
    "%time single_user = matrix_array[user_id]\n",
    "single_user.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09000ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get nonzero indices for user\n",
    "%time indices = list(np.nonzero(single_user)[0])\n",
    "indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbe1d0d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make reduced array for user of nonzero indices\n",
    "%time array_chunk_a = (single_user[indices]).astype('float32').reshape(-1,1)\n",
    "array_chunk_a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8455935",
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize user\n",
    "%time normalize_a = normalize(array_chunk_a, axis=0)\n",
    "normalize_a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c857f4cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7dc87dba",
   "metadata": {},
   "source": [
    "Investigate methods of reducing dataframe or array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d78adf7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make reduced on sparse dataframe\n",
    "%time df_chunk_b = larger_matrix[indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ec72e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_chunk_b.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51f90d17",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_chunk_b.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "932946d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make reduced on array\n",
    "%time array_chunk_b = matrix_array[:, indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cb2e9f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "array_chunk_b.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c108ee95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# turn array into sparse matrix\n",
    "sparse_matrix = sparse.csr_matrix(matrix_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae225589",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make reduced on sparse\n",
    "%time array_chunk_b = sparse_matrix[:, indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ee33c2b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5a8a11c7",
   "metadata": {},
   "source": [
    "Convert dataframe to array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0652460",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert reduced dataframe to sparse matrix\n",
    "\n",
    "%time sparse_array = sparse.csr_matrix(df_chunk_b.sparse.to_coo())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40891e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert reduced dataframe to array\n",
    "%time array_b_matrix = df_chunk_b.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95df43dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "array_b_matrix[0][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36972aae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b85f165",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9d6c57fb",
   "metadata": {},
   "source": [
    "Investigate normalization methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78fc26ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sklearn normalize on dataframe\n",
    "%time normalize_b = normalize(df_chunk_b, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44fc7094",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalize_b[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e9f137b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef1a4019",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sklearn normalize on array\n",
    "%time normalize_b = normalize(array_b_matrix, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47703181",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalize_b[0][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb3b4766",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "141e09ef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a6fa5e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make partial dataframe segment\n",
    "%time partial_df = df_chunk_b[:134400]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "932b8228",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make partial array segment\n",
    "%time partial_array = normalize_b[:134400]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ba7ae80",
   "metadata": {},
   "source": [
    "## Deprecated Parallelization Techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c98037e",
   "metadata": {},
   "source": [
    "### Test common indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1626e2a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "\n",
    "# Step one\n",
    "# make the single item matrix for the one item\n",
    "%time single_item = matrix_array[:, item1]\n",
    "# get the indices where the item is nonzero\n",
    "%time indices = np.nonzero(single_item)[0]\n",
    "\n",
    "# Step 2\n",
    "# make the single item matrix for the next item\n",
    "%time next_item = matrix_array[:, item2]\n",
    "# get the indices where the item is nonzero\n",
    "%time indices2 = np.nonzero(next_item)[0]\n",
    "\n",
    "# step 3\n",
    "# get the indices in common between the two\n",
    "%time common_indices = list(set.intersection(set(indices), set(indices2)))\n",
    "\n",
    "# step 4\n",
    "%time reduced_item1 = single_item[common_indices]\n",
    "%time reduced_item2 = next_item[common_indices]\n",
    "\n",
    "# step 5\n",
    "%time a = tf.constant(reduced_item1, dtype=tf.float32)\n",
    "%time b = tf.constant(reduced_item2, dtype=tf.float32)\n",
    "%time a = tf.nn.l2_normalize(a)\n",
    "%time b = tf.nn.l2_normalize(b)\n",
    "\n",
    "# step 6\n",
    "%time item_similarity = 1-cosine_distance(a, b, axis=0).numpy()\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "print(end - start)\n",
    "\n",
    "print(item_similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef153a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "reduced_item1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f94bc4e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "reduced_item2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5677acbb",
   "metadata": {},
   "source": [
    "### Test xor1d indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a8aa400",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "\n",
    "# Step one\n",
    "# make the single item matrix for the one item\n",
    "%time single_item = matrix_array[:, item1].copy()\n",
    "# get the indices where the item is nonzero\n",
    "%time indices = np.nonzero(single_item)[0]\n",
    "\n",
    "# Step 2\n",
    "# make the single item matrix for the next item\n",
    "%time next_item = matrix_array[:, item2].copy()\n",
    "# get the indices where the item is nonzero\n",
    "%time indices2 = np.nonzero(next_item)[0]\n",
    "\n",
    "# step 3\n",
    "# get the indices in common between the two\n",
    "%time indices_diff = np.setxor1d(indices, indices2)\n",
    "\n",
    "%time first_item = single_item.copy()\n",
    "# step 4\n",
    "%time first_item[[indices_diff]]=0\n",
    "%time next_item[[indices_diff]]=0\n",
    "\n",
    "# step 5\n",
    "%time a = tf.constant(first_item, dtype=tf.float32)\n",
    "%time b = tf.constant(next_item, dtype=tf.float32)\n",
    "%time a = tf.nn.l2_normalize(a)\n",
    "%time b = tf.nn.l2_normalize(b)\n",
    "\n",
    "# step 6\n",
    "%time item_similarity = 1-cosine_distance(a, b, axis=0).numpy()\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "print(end - start)\n",
    "\n",
    "print(item_similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaf594fe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f6d4311",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "836d9be0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "76ec3a58",
   "metadata": {},
   "source": [
    "### No Jit and xor1d indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e19ec6b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @jit(nopython=True, parallel=True, fastmath=True)\n",
    "def math_function(game, matrix_array, number_of_games):\n",
    "\n",
    "    # results = []\n",
    "    results_a = []\n",
    "    results_b = []\n",
    "\n",
    "    # make the single user matrix for the one user\n",
    "    single_item = matrix_array[:, game].copy()\n",
    "    # get the indices where the user is nonzero\n",
    "    indices = np.nonzero(single_item)[0]\n",
    "\n",
    "    for game2 in number_of_games:\n",
    "\n",
    "        next_item = matrix_array[:, game2].copy()\n",
    "        indices2 = np.nonzero(next_item)[0]\n",
    "\n",
    "        indices_diff = np.setxor1d(indices, indices2)\n",
    "\n",
    "        # if len(common_indices)<4:\n",
    "        # results.append(0)\n",
    "        # continue\n",
    "\n",
    "        first_item = single_item.copy()\n",
    "        first_item[[indices_diff]] = 0\n",
    "        next_item[[indices_diff]] = 0\n",
    "\n",
    "        # step 5\n",
    "        # a = tf.constant(first_item, dtype=tf.float32)\n",
    "        # b = tf.constant(next_item, dtype=tf.float32)\n",
    "        # a = tf.nn.l2_normalize(a)\n",
    "        # b = tf.nn.l2_normalize(b)\n",
    "\n",
    "        # step 6\n",
    "        # item_similarity = 1-cosine_distance(a, b, axis=0).numpy()\n",
    "\n",
    "        # results.append(item_similarity)\n",
    "        results_a.append(first_item)\n",
    "        results_b.append(next_item)\n",
    "\n",
    "    return results_a, results_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d40afc9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "number_of_games = np.arange(0, matrix_array.shape[1], 1)\n",
    "\n",
    "global_start = time.time()\n",
    "\n",
    "# Load the storage dictionary for this block\n",
    "with open(\"item_similarities/similarity_storage_real_scaled_temp.json\") as json_file:\n",
    "    base_items_storage = json.load(json_file)\n",
    "\n",
    "# for each user block in the block_indices_lookup. The user blocks are integers from 1-20\n",
    "for game in number_of_games[0:100]:\n",
    "\n",
    "    print(\"\\nStarting game: \" + str(game))\n",
    "    start = time.time()\n",
    "\n",
    "    gameid_1 = gameids_columnorder[game]\n",
    "\n",
    "    results_a, results_b = math_function(game, matrix_array, number_of_games)\n",
    "\n",
    "    # base_items_storage[gameid_1]['Sims'] = results\n",
    "    base_items_storage[gameid_1][\"Left\"] = results_a\n",
    "    base_items_storage[gameid_1][\"Right\"] = results_b\n",
    "\n",
    "    end = time.time()\n",
    "\n",
    "    print(end - start)\n",
    "\n",
    "print(time.time() - global_start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc26d366",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b5b7f8f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29b3f863",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a025daaa",
   "metadata": {},
   "source": [
    "No GPU. No filtering: 2572sec for 97 entries\n",
    "No GPU. Filtering < 4: 2453sec for 97 entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e282cae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54313e88",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4509b69",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a91138fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3b43ce3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b7e2197",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "365a298f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "370ec231",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a5f3251",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "222.83px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
