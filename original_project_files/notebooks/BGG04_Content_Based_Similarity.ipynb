{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "979811e9",
   "metadata": {},
   "source": [
    "# Notebook Objective and Setup\n",
    "\n",
    "BGG05 is the building of a content-based item filter. Using category weights, I use my domain expertise to tune an item similarity matrix for all game IDs in the games file.\n",
    "\n",
    "This content-based filter could be used as-is to find similar games to a user's catalog and predict ratings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4e48291",
   "metadata": {},
   "source": [
    "## Notebook Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e76df09b",
   "metadata": {},
   "source": [
    "### Package Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57e943a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import regex as re\n",
    "import time\n",
    "import os\n",
    "import gc\n",
    "import copy\n",
    "import json\n",
    "\n",
    "# ignore warnings (gets rid of Pandas copy warnings)\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "pd.options.display.max_columns = None\n",
    "\n",
    "pd.set_option(\"display.max_columns\", 30)\n",
    "pd.set_option(\"display.max_rows\", 30)\n",
    "\n",
    "# from sklearn.metrics.pairwise import cosine_similarity\n",
    "# from sklearn.preprocessing import MinMaxScaler, OneHotEncoder\n",
    "# from missingpy import MissForest\n",
    "# from sklearn.pipeline import Pipeline\n",
    "# from sklearn.compose import ColumnTransformer\n",
    "# from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "# # NLP tools\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "# import re\n",
    "# import nltk\n",
    "# import fasttext\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "# from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "# from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "# from tensorflow.python.keras.preprocessing import sequence, text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "275f08a9",
   "metadata": {},
   "source": [
    "### Notebook Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e95d7216",
   "metadata": {},
   "source": [
    "##### Processing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a78e8abb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    # remove numbers\n",
    "    text = re.sub(r\"\\d+\", \"\", text)\n",
    "    # remove punctuation except periods\n",
    "    text = re.sub(r\"[^\\w\\s\\.]\", \"\", text)\n",
    "    text = text.lower()\n",
    "    text = text.strip()\n",
    "    return text\n",
    "\n",
    "def filter_stopwords(text):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    word_tokens = word_tokenize(text)\n",
    "    filtered_sentence = [w for w in word_tokens if not w.lower() in stop_words]\n",
    "    return \" \".join(filtered_sentence)\n",
    "\n",
    "def evaluate_quality_words_over_thresh(text):\n",
    "    word_tokens = word_tokenize(text)\n",
    "    return len(word_tokens) > 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64427375",
   "metadata": {},
   "outputs": [],
   "source": [
    "def processing_pipeline(weight_groups, df):\n",
    "    \"\"\"\n",
    "    !!!Hard-coded processor!!!\n",
    "    Takes in weight tuples and a dataframe\n",
    "    Scales specific dataframe columns to tuples\n",
    "\n",
    "    Inputs:\n",
    "    weight_groups: list of weight tuples (x, y)\n",
    "    df: df to be scaled\n",
    "\n",
    "    Returns:\n",
    "    Processed Dataframe\"\"\"\n",
    "\n",
    "    # continuous pipeline\n",
    "    family_encoder = Pipeline(\n",
    "        [\n",
    "            (\"encoder\", OneHotEncoder()),\n",
    "            (\"scaler\", MinMaxScaler(feature_range=weight_groups[6])),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Whole pipeline with continuous then categorical transformers\n",
    "    total_pipeline = ColumnTransformer(\n",
    "        [\n",
    "            (\n",
    "                \"games_weight_weight\",\n",
    "                MinMaxScaler(feature_range=weight_groups[0]),\n",
    "                [\"GameWeight\"],\n",
    "            ),\n",
    "            (\"avgrating\", MinMaxScaler(feature_range=weight_groups[1]), [\"AvgRating\"]),\n",
    "            (\n",
    "                \"bayes_weight\",\n",
    "                MinMaxScaler(feature_range=weight_groups[2]),\n",
    "                [\"BayesAvgRating\"],\n",
    "            ),\n",
    "            (\n",
    "                \"players_weight\",\n",
    "                MinMaxScaler(feature_range=weight_groups[3]),\n",
    "                [\"BestPlayers\"],\n",
    "            ),\n",
    "            (\n",
    "                \"playtime_weight\",\n",
    "                MinMaxScaler(feature_range=weight_groups[4]),\n",
    "                [\"Playtime\"],\n",
    "            ),\n",
    "            (\n",
    "                \"remainder_weight\",\n",
    "                MinMaxScaler(feature_range=weight_groups[5]),\n",
    "                [\"Cat:War\", \"Cat:CGS\", \"Cat:Abstract\", \"Cat:Party\", \"Cat:Childrens\"],\n",
    "            ),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Fit and tranform the pipeline on x_train, then transform x_test\n",
    "    processed = total_pipeline.fit_transform(df)\n",
    "\n",
    "    return processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d0e6662",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_dataset(dataset, weights, tfidf=False):\n",
    "    \"\"\"\n",
    "    Set each item either according to Term Frequency or TF-IDF\n",
    "    Then scale dataset according to provided weights\n",
    "\n",
    "    tfidf flag:\n",
    "    Use TRUE when RARE entries are more important than FREQUENT entries\n",
    "    Use FALSE when COMMON entries are more important than RARE entries\n",
    "\n",
    "    Inputs:\n",
    "    dataset: dataset to scale\n",
    "    weights: tuple (x, y) to weight dataset\n",
    "    tfidf: set flag to True for TF-IDF\n",
    "\n",
    "    Outputs:\n",
    "    Scaled data\n",
    "    \"\"\"\n",
    "\n",
    "    # drop BGG Id\n",
    "    try:\n",
    "        dataset_pared = dataset.drop(\"BGGId\", axis=1)\n",
    "    except:\n",
    "        dataset_pared = dataset\n",
    "\n",
    "    # get list of titles to reapply to DF after transformation\n",
    "    titles = list(dataset_pared.columns)\n",
    "\n",
    "    # set up weighted scaler\n",
    "    scaler = MinMaxScaler(feature_range=weights)\n",
    "\n",
    "    # get total number of entries\n",
    "    total_entries = sum(dataset.sum())\n",
    "\n",
    "    if tfidf:\n",
    "\n",
    "        # instantiate tfidf transformer\n",
    "        tfidf = TfidfTransformer()\n",
    "\n",
    "        # convert matrix to tfidf\n",
    "        tfidf_dataset = pd.DataFrame(\n",
    "            tfidf.fit_transform(dataset_pared).toarray(), columns=titles\n",
    "        )\n",
    "\n",
    "        # run scaler on transpose (scale by row not column)\n",
    "        transpose_scaled = scaler.fit_transform(tfidf_dataset.T)\n",
    "\n",
    "    else:\n",
    "        # for each column,\n",
    "        for item in list(dataset_pared.columns):\n",
    "            # set item as its term frequency\n",
    "            dataset_pared.loc[dataset_pared[item] > 0, item] = (\n",
    "                dataset_pared[item].sum() / total_entries\n",
    "            )\n",
    "\n",
    "        # run scaler on transpose (scale by row not column)\n",
    "        transpose_scaled = scaler.fit_transform(dataset_pared.T)\n",
    "\n",
    "    # rebuild dataframe\n",
    "    scaled_dataset = pd.DataFrame(transpose_scaled.T, columns=titles)\n",
    "\n",
    "    return scaled_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ee8e79f",
   "metadata": {},
   "source": [
    "# Content Based Filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbfc4e58",
   "metadata": {},
   "source": [
    "## Set Weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81252846",
   "metadata": {},
   "source": [
    "These are the scales for each of these categories. All entries in the category will be scaled to this tuple range by the MinMaxScaler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab1cf563",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### DO NOT TOUCH THESE ARE THE PRODUCTION WEIGHTS!!!!!\n",
    "\n",
    "games_weight_weight = (-1.5, 1.5)  # game weight. Is a range, so (-, )\n",
    "rating_weight = (-0.3, 0.3)\n",
    "bayes_weight = (-0.5, 0.5)  # game weighted rating. Is a range, so (-, )\n",
    "players_weight = (0, 1)  # best players. Is a problematic range due to outliers\n",
    "playtime_weight = (0, 2)  # playtime. Is a range so (-, ). Has high outliers\n",
    "designers_weight = (0, 0.5)  # designers, binary\n",
    "mechanics_weight = (0, 0.75)  # mechanics, binary\n",
    "subcategories_weight = (0, 0.75)  # other mechanics like card game, print&play. binary\n",
    "family_weights = (0, 0.5)  # game families like pandemic, century. binary\n",
    "categories_weight = (0, 1)  # the five large overarching categories, binary\n",
    "themes_weight = (-0.15, 0.15)  # themes like space, western. binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c122b8b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"games_weight_weight = (-1, 1) # game weight. Is a range, so (-, )\n",
    "rating_weight = (-.5, .5)\n",
    "bayes_weight = (-.001, .001) # game weighted rating. Is a range, so (-, )\n",
    "players_weight = (0, 1) # best players. Is a problematic range due to outliers\n",
    "playtime_weight = (0, 2) # playtime. Is a range so (-, ). Has high outliers\n",
    "designers_weight = (0, 0.5) # designers, binary\n",
    "mechanics_weight = (0, .75) # mechanics, binary\n",
    "subcategories_weight = (0, .75) # other mechanics like card game, print&play. binary\n",
    "family_weights = (0, 0.5) # game families like pandemic, century. binary\n",
    "categories_weight = (0, 1) # the five large overarching categories, binary\n",
    "themes_weight = (-.15, 0.15) # themes like space, western. binary\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c29560c",
   "metadata": {},
   "source": [
    "## Load and Prep Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5875ed4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "game_data_dir = \"../../data/prod/games/game_dfs_clean\"\n",
    "\n",
    "# Load games\n",
    "games = pd.read_pickle(f\"{game_data_dir}/games_clean.pkl\")\n",
    "\n",
    "# determine playtime for each game according to community\n",
    "games[\"Playtime\"] = 0\n",
    "games[\"Playtime\"] = games.apply(\n",
    "    lambda x: np.mean(x[\"ComMinPlaytime\"] + x[\"ComMaxPlaytime\"]), axis=1\n",
    ")\n",
    "\n",
    "# set upper cap on playtime\n",
    "over_6_hours = list(games.loc[games[\"Playtime\"] > 360].index)\n",
    "games.loc[over_6_hours, \"Playtime\"] = 360\n",
    "\n",
    "# load other files to use\n",
    "\n",
    "designers = pd.read_pickle(f\"{game_data_dir}/designers_clean.pkl\")\n",
    "subcategories = pd.read_pickle(f\"{game_data_dir}/subcategories_clean.pkl\")\n",
    "themes = pd.read_pickle(f\"{game_data_dir}/themes_clean.pkl\")\n",
    "\n",
    "# games.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d8d376f",
   "metadata": {},
   "outputs": [],
   "source": [
    "themes.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41ee09e0",
   "metadata": {},
   "source": [
    "### TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dec8bc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load spaCy with English language processor\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f67086e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "descriptions = games[[\"Description\"]].astype(str)\n",
    "descriptions['Description'] = descriptions['Description'].apply(lambda x: clean_text(x))\n",
    "descriptions['Description'] = descriptions['Description'].apply(lambda x: filter_stopwords(x))\n",
    "descriptions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c155f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare the vectorizer with the chosen parameters\n",
    "tfid_proc = TfidfVectorizer(\n",
    "    sublinear_tf=True,\n",
    "    use_idf=True,\n",
    "    max_df=0.8,\n",
    "    min_df=0.005,\n",
    "    ngram_range=(1, 3),\n",
    "    max_features=5000,\n",
    ")\n",
    "\n",
    "# fit the vectorizer to the descriptions\n",
    "word_vectors = tfid_proc.fit_transform(descriptions[\"Description\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceb27e11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cast the vector array to a data frame with columns named by the features selected by the vectorizer\n",
    "word_vectors_df = pd.DataFrame(\n",
    "    word_vectors.toarray(), columns=tfid_proc.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecc316d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vectors_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71b7bc34",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vectors_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edd11941",
   "metadata": {},
   "source": [
    "Load in the description vectors (produced and saved in the appendix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "534e4ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "description_vectors = pd.read_pickle(\"{game_data_dir}/description_vectors.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbfc64bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "description_vectors.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dc912fa",
   "metadata": {},
   "source": [
    "### Weight Scale Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78ce3cd1",
   "metadata": {},
   "source": [
    "##### TF-IDF Mechanics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19153722",
   "metadata": {},
   "outputs": [],
   "source": [
    "mechanics = pd.read_pickle(f\"{game_data_dir}/mechanics_clean.pkl\")\n",
    "\n",
    "mechanics = pd.get_dummies(mechanics, columns=[\"mechanic\"], prefix=\"\", prefix_sep=\"\").groupby(\"BGGId\").sum()\n",
    "\n",
    "# get floor of mechanics presence in catalog (.05% of games)\n",
    "mechanics_floor = round(mechanics.shape[0] * 0.005)\n",
    "\n",
    "# make a list of mechanics more than the floor\n",
    "sums = pd.DataFrame(mechanics.sum() >= mechanics_floor)\n",
    "\n",
    "# get indices for the mechanics keeping\n",
    "keep_mechanics = sums.loc[sums[0] == True].index\n",
    "\n",
    "mechanics = mechanics[keep_mechanics]\n",
    "\n",
    "mechanics.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80d0d975",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scaled mechanics\n",
    "scaled_mechanics = scale_dataset(mechanics, mechanics_weight, tfidf=True)\n",
    "\n",
    "# make new column for games without any mechanics information\n",
    "no_mechanics_index = list(scaled_mechanics.loc[scaled_mechanics.sum(axis=1) == 0].index)\n",
    "scaled_mechanics[\"No Mechanics\"] = 0\n",
    "scaled_mechanics.loc[no_mechanics_index, \"No Mechanics\"] = mechanics_weight[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdbdd98e",
   "metadata": {},
   "source": [
    "##### TF-Scale Only Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aadf82bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale themes\n",
    "scaler = MinMaxScaler(feature_range=themes_weight)\n",
    "scaled_themes = scaler.fit_transform(themes)\n",
    "\n",
    "# get list of titles to reapply to DF after transformation\n",
    "titles = list(themes.columns)\n",
    "\n",
    "scaled_themes = pd.DataFrame(scaled_themes, columns=titles)\n",
    "scaled_themes.drop(\"BGGId\", axis=1, inplace=True)\n",
    "\n",
    "# scaled designers\n",
    "scaled_designers = scale_dataset(designers, designers_weight)\n",
    "\n",
    "# scaled game families\n",
    "game_families = pd.get_dummies(games[\"Family\"])\n",
    "scaled_families = scale_dataset(game_families, family_weights)\n",
    "\n",
    "# scaled subcategories\n",
    "scaled_subcategories = scale_dataset(subcategories, subcategories_weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8984f9f7",
   "metadata": {},
   "source": [
    "### Master CBF Frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d60ca69a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# include these columns for comparison\n",
    "games_included_columns = [\n",
    "    \"GameWeight\",\n",
    "    \"AvgRating\",\n",
    "    \"BayesAvgRating\",\n",
    "    \"BestPlayers\",\n",
    "    \"Playtime\",\n",
    "    \"Cat:War\",\n",
    "    \"Cat:CGS\",\n",
    "    \"Cat:Abstract\",\n",
    "    \"Cat:Party\",\n",
    "    \"Cat:Childrens\",\n",
    "]\n",
    "\n",
    "# make smaller df of the included columns\n",
    "scaled_games = games[games_included_columns]\n",
    "\n",
    "# get list of game names\n",
    "game_names = list(games[\"Name\"])\n",
    "# get list of game ids\n",
    "game_ids = list(games[\"BGGId\"])\n",
    "\n",
    "# create game lookup table\n",
    "game_lookup = {}\n",
    "for key, value in zip(game_ids, game_names):\n",
    "    game_lookup[key] = value\n",
    "\n",
    "# instantiate MissForest imputer and fill all nans in scaled_games\n",
    "imputer = MissForest()\n",
    "scaled_games = pd.DataFrame(\n",
    "    imputer.fit_transform(scaled_games), columns=games_included_columns\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70adaad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up weight groups for hard coded pipeline\n",
    "weight_groups = [\n",
    "    games_weight_weight,\n",
    "    rating_weight,\n",
    "    bayes_weight,\n",
    "    players_weight,\n",
    "    playtime_weight,\n",
    "    categories_weight,\n",
    "    family_weights,\n",
    "]\n",
    "\n",
    "# process scaled_games with pipeline\n",
    "scaled_games = pd.DataFrame(\n",
    "    processing_pipeline(weight_groups, scaled_games), columns=games_included_columns\n",
    ")\n",
    "\n",
    "# make list of games and ids (is this used anywhere?)\n",
    "game_and_id = list(zip(game_names, game_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be35687a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# put together master dataframe with other already processed dataframes\n",
    "master_games = pd.concat(\n",
    "    (\n",
    "        scaled_games,\n",
    "        scaled_themes,\n",
    "        scaled_mechanics,\n",
    "        scaled_families,\n",
    "        scaled_designers,\n",
    "        scaled_subcategories,\n",
    "        description_vectors,\n",
    "        word_vectors,\n",
    "    ),\n",
    "    axis=1,\n",
    ")  # , description_vectors, word_vectors\n",
    "\n",
    "# put game id on master_games DF\n",
    "master_games[\"BGGId\"] = game_ids\n",
    "\n",
    "# set index to id\n",
    "master_games.set_index(\"BGGId\", inplace=True)\n",
    "\n",
    "# master_games.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4da929b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "master_games.to_pickle(\"{game_data_dir}/master_games_scaled.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ba19e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"# clean up\n",
    "\n",
    "del mechanics\n",
    "del designers\n",
    "del subcategories\n",
    "del themes\n",
    "del scaled_mechanics\n",
    "del scaled_families\n",
    "del scaled_designers\n",
    "del scaled_games\n",
    "del scaled_subcategories\n",
    "del scaled_themes\n",
    "del description_vectors\n",
    "\n",
    "gc.collect()\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd859967",
   "metadata": {},
   "source": [
    "## Item Similarity via Cosine Distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "784c375c",
   "metadata": {},
   "outputs": [],
   "source": [
    "master_games = pd.read_pickle(\"{game_data_dir}/master_games_scaled.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28e7611c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load games\n",
    "games = pd.read_pickle(\"{game_data_dir}/games.pkl\")\n",
    "\n",
    "# get list of game ids\n",
    "game_ids = list(games[\"BGGId\"])\n",
    "\n",
    "game_names = list(games[\"Name\"].str.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6f9b1ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get cosine similarities!\n",
    "cosine_sims = cosine_similarity(master_games)\n",
    "\n",
    "# do similarities by game id\n",
    "sims_byid = pd.DataFrame(cosine_sims, columns=game_ids)\n",
    "sims_byid[\"Game_Id\"] = game_ids\n",
    "sims_byid.set_index(\"Game_Id\", inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1e2910f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SCALE IF NEEDED\n",
    "\n",
    "threshold = 0.95\n",
    "\n",
    "sims_byid = sims_byid.round(6)\n",
    "\n",
    "for column in list(sims_byid.columns):\n",
    "    lower = sims_byid[column].min()\n",
    "    sims_byid[column].replace(1.0, lower, inplace=True)\n",
    "    if sims_byid[column].max() > threshold:\n",
    "        continue\n",
    "    else:\n",
    "        scaler = MinMaxScaler(feature_range=(lower, threshold))\n",
    "        scaled_values = scaler.fit_transform(np.array(sims_byid[column]).reshape(-1, 1))\n",
    "        sims_byid[column] = scaled_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfad17ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "sims_byid = sims_byid.round(2)\n",
    "sims_byid.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26825089",
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in np.arange(0, len(game_names), 1):\n",
    "    game_names[item] = re.sub(\"[^A-Za-z0-9\\s]+\", \"\", game_names[item])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd666ec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "sims_byname = sims_byid.copy()\n",
    "sims_byname.set_axis(game_names, axis=1, inplace=True)\n",
    "sims_byname.set_axis(game_names, axis=0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a52ca28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save to pickles, we really only need the id one\n",
    "sims_byid.to_pickle(\"{game_data_dir}/game_cosine_similarity_byid.pkl\")\n",
    "sims_byname.to_pickle(\"{game_data_dir}/game_cosine_similarity_byname.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdda5478",
   "metadata": {},
   "source": [
    "### CHECK GAME HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4edae9cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save to pickles, we really only need the id one\n",
    "sims_byid = pd.read_pickle(\"{game_data_dir}/game_cosine_similarity_byid.pkl\")\n",
    "sims_byname = pd.read_pickle(\n",
    "    \"{game_data_dir}/game_cosine_similarity_byname.pkl\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c0bb401",
   "metadata": {},
   "source": [
    "This is why we made the name one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7669caea",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# test my specific game set here\n",
    "\n",
    "test_dict = {\n",
    "    \"Dominion\": list(sims_byname[\"dominion\"].sort_values(ascending=False)[:15].index),\n",
    "    \"D_Sim\": list(sims_byname[\"dominion\"].sort_values(ascending=False)[:15]),\n",
    "    \"Gloomhaven\": list(\n",
    "        sims_byname[\"gloomhaven\"].sort_values(ascending=False)[:15].index\n",
    "    ),\n",
    "    \"G_Sim\": list(sims_byname[\"gloomhaven\"].sort_values(ascending=False)[:15]),\n",
    "    \"Pandemic\": list(sims_byname[\"pandemic\"].sort_values(ascending=False)[:15].index),\n",
    "    \"Pa_Sim\": list(sims_byname[\"pandemic\"].sort_values(ascending=False)[:15]),\n",
    "    \"Splendor\": list(sims_byname[\"splendor\"].sort_values(ascending=False)[:15].index),\n",
    "    \"Sp_Sim\": list(sims_byname[\"splendor\"].sort_values(ascending=False)[:15]),\n",
    "    \"Viticulture Essential Edition\": list(\n",
    "        sims_byname[\"viticulture essential edition\"]\n",
    "        .sort_values(ascending=False)[:15]\n",
    "        .index\n",
    "    ),\n",
    "    \"V_Sim\": list(\n",
    "        sims_byname[\"viticulture essential edition\"].sort_values(ascending=False)[:15]\n",
    "    ),\n",
    "    \"Agricola\": list(sims_byname[\"agricola\"].sort_values(ascending=False)[:15].index),\n",
    "    \"Ag_Sim\": list(sims_byname[\"agricola\"].sort_values(ascending=False)[:15]),\n",
    "    \"Space Base\": list(\n",
    "        sims_byname[\"space base\"].sort_values(ascending=False)[:15].index\n",
    "    ),\n",
    "    \"Spa_Sim\": list(sims_byname[\"space base\"].sort_values(ascending=False)[:15]),\n",
    "    \"Terraforming Mars\": list(\n",
    "        sims_byname[\"terraforming mars\"].sort_values(ascending=False)[:15].index\n",
    "    ),\n",
    "    \"Te_Sim\": list(sims_byname[\"terraforming mars\"].sort_values(ascending=False)[:15]),\n",
    "    #'Puerto Rico':list(sims_byname['Puerto Rico'].sort_values(ascending=False)[:15].index), 'Pu_Sim':list(sims_byname['Puerto Rico'].sort_values(ascending=False)[:15]),\n",
    "    \"Chess\": list(sims_byname[\"chess\"].sort_values(ascending=False)[:15].index),\n",
    "    \"Ch_Sim\": list(sims_byname[\"chess\"].sort_values(ascending=False)[:15]),\n",
    "    #'Backgammon':list(sims_byname['Backgammon'].sort_values(ascending=False)[:15].index), 'B_Sim':list(sims_byname['Backgammon'].sort_values(ascending=False)[:15]),\n",
    "    #'Sagrada':list(sims_byname['Sagrada'].sort_values(ascending=False)[:15].index), 'Sa_Sim':list(sims_byname['Sagrada'].sort_values(ascending=False)[:15]),\n",
    "    \"Azul\": list(sims_byname[\"azul\"].sort_values(ascending=False)[:15].index),\n",
    "    \"Az_Sim\": list(sims_byname[\"azul\"].sort_values(ascending=False)[:15]),\n",
    "    \"Codenames\": list(sims_byname[\"codenames\"].sort_values(ascending=False)[:15].index),\n",
    "    \"Co_Sim\": list(sims_byname[\"codenames\"].sort_values(ascending=False)[:15]),\n",
    "    \"Secret Hitler\": list(\n",
    "        sims_byname[\"secret hitler\"].sort_values(ascending=False)[:15].index\n",
    "    ),\n",
    "    \"Se_Sim\": list(sims_byname[\"secret hitler\"].sort_values(ascending=False)[:15]),\n",
    "    \"Monopoly\": list(sims_byname[\"monopoly\"].sort_values(ascending=False)[:15].index),\n",
    "    \"M_Sim\": list(sims_byname[\"monopoly\"].sort_values(ascending=False)[:15]),\n",
    "    \"Lords of Waterdeep\": list(\n",
    "        sims_byname[\"lords of waterdeep\"].sort_values(ascending=False)[:15].index\n",
    "    ),\n",
    "    \"L_Sim\": list(sims_byname[\"lords of waterdeep\"].sort_values(ascending=False)[:15]),\n",
    "    \"Stone Age\": list(sims_byname[\"stone age\"].sort_values(ascending=False)[:15].index),\n",
    "    \"St_Sim\": list(sims_byname[\"stone age\"].sort_values(ascending=False)[:15]),\n",
    "    \"Century: Spice Road\": list(\n",
    "        sims_byname[\"century spice road\"].sort_values(ascending=False)[:15].index\n",
    "    ),\n",
    "    \"Ce_Sim\": list(sims_byname[\"century spice road\"].sort_values(ascending=False)[:15]),\n",
    "    #'Scrabble':list(sims_byname['Scrabble'].sort_values(ascending=False)[:15].index), 'Sc_Sim':list(sims_byname['Scrabble'].sort_values(ascending=False)[:15]),\n",
    "    \"18MS: The Railroads Come to Mississippi\": list(\n",
    "        sims_byname[\"18ms the railroads come to mississippi\"]\n",
    "        .sort_values(ascending=False)[:15]\n",
    "        .index\n",
    "    ),\n",
    "    \"18xx\": list(\n",
    "        sims_byname[\"18ms the railroads come to mississippi\"].sort_values(\n",
    "            ascending=False\n",
    "        )[:15]\n",
    "    ),\n",
    "    #'Roads to Gettysburg II: Lee Strikes North':list(sims_byname['Roads to Gettysburg II: Lee Strikes North'].sort_values(ascending=False)[:15].index), 'War3':list(sims_byname['Roads to Gettysburg II: Lee Strikes North'].sort_values(ascending=False)[:15]),\n",
    "    \"Power Grid\": list(\n",
    "        sims_byname[\"power grid\"].sort_values(ascending=False)[:15].index\n",
    "    ),\n",
    "    \"P_Grid\": list(sims_byname[\"power grid\"].sort_values(ascending=False)[:15]),\n",
    "}\n",
    "\n",
    "pd.DataFrame(test_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a77c126c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# test specific games here\n",
    "game = \"mariposas\"\n",
    "game = game.lower()\n",
    "\n",
    "results = pd.DataFrame(\n",
    "    data={\"Similarity\": sims_byname[game].sort_values(ascending=False)[0:21]}\n",
    ")\n",
    "results.index = results.index.str.title()\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3551ad4f",
   "metadata": {},
   "source": [
    "# Appendix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a026443b",
   "metadata": {},
   "source": [
    "## Glove Word Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bb088ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# amount of vocabulary to use, will pick the top 10000 words seen in the corpus\n",
    "features = 5000\n",
    "\n",
    "# max text sequence length, must match tokens in transfer file, we are using glove 300d so it is 300\n",
    "max_words = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c496a814",
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate our word tokenizer\n",
    "tokenizer = Tokenizer(num_words=features)\n",
    "\n",
    "# Create vocabulary with training texts\n",
    "tokenizer.fit_on_texts(\n",
    "    games[\"Description\"]\n",
    ")  # nltk method which creates a vocab index based on the word frequency, every word gets own integer value\n",
    "\n",
    "# pad the train text to 300, or cut off if over\n",
    "tokenized_train = tokenizer.texts_to_sequences(\n",
    "    games[\"Description\"]\n",
    ")  # transforms each text to a sequence of integers\n",
    "tokenized_train = sequence.pad_sequences(\n",
    "    tokenized_train, maxlen=max_words, truncating=\"post\", padding=\"post\"\n",
    ")  # truncates or pads the vector to the max_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "367c5e24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# identify the embedding filename; we are using the Glove 42B 300d embeddings\n",
    "glove_file = \"glove.42B.300d.txt\"\n",
    "\n",
    "# create the embeddings index dictionary\n",
    "embeddings_index = {}  # create a lookup dictionary to store words and their vectors\n",
    "f = open(glove_file, errors=\"ignore\")  # open our embedding file\n",
    "for line in f:  # for each line in the file\n",
    "    values = line.split(\n",
    "        \" \"\n",
    "    )  # split the line on spaces between the word and its vectors\n",
    "    word = values[0]  # the word is the first entry\n",
    "    if (\n",
    "        word in tokenizer.word_index.keys()\n",
    "    ):  # we check if the word is in our tokenizer word index\n",
    "        coefs = np.asarray(values[1:], dtype=\"float32\")  # if so, get the word's vectors\n",
    "        embeddings_index[word] = (\n",
    "            coefs  # add the word and its vectors to the embeddings_index dictionary\n",
    "        )\n",
    "f.close()\n",
    "\n",
    "print(\n",
    "    \"Found %s word vectors.\" % len(embeddings_index)\n",
    ")  # report how many words in our corpus were found in the GloVe words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb5a091c",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_tokens = (\n",
    "    len(tokenizer.word_index) + 1\n",
    ")  # for num tokens we always do the length of our word index +1 for a pad token\n",
    "hits = 0\n",
    "misses = 0\n",
    "\n",
    "embedding_matrix = np.zeros(\n",
    "    (num_tokens, max_words)\n",
    ")  # setting up an array for our tokens with a row per token and 300 columns\n",
    "for (\n",
    "    word,\n",
    "    i,\n",
    ") in tokenizer.word_index.items():  # for each word in the tokenizer word index\n",
    "    embedding_vector = embeddings_index.get(\n",
    "        word\n",
    "    )  # get the vector from the embeddings index dictionary\n",
    "    if embedding_vector is not None:  # if the vector isn't None,\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = (\n",
    "            embedding_vector  # store the embedding vector in the matrix at that index\n",
    "        )\n",
    "        hits += 1\n",
    "    else:\n",
    "        misses += 1\n",
    "\n",
    "print(\"Converted %d words (%d misses)\" % (hits, misses))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d5aa2ec",
   "metadata": {},
   "source": [
    "### Single Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5c77cc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = games[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e33d6f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_storage = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a5b5650",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp.apply(lambda x: vectorize(x), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14112607",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(vector_storage).T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c21d61bf",
   "metadata": {},
   "source": [
    "### Real Word Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fe55dfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize(x):\n",
    "\n",
    "    description = x[\"Description\"]\n",
    "\n",
    "    tokens = nlp(description)\n",
    "\n",
    "    vector_storage[x[\"BGGId\"]] = {}\n",
    "\n",
    "    for token in tokens:\n",
    "\n",
    "        word = str(token)\n",
    "\n",
    "        try:\n",
    "            if np.all(embedding_matrix[tokenizer.word_index[word]]):\n",
    "                vector_storage[x[\"BGGId\"]][word] = np.mean(\n",
    "                    embedding_matrix[tokenizer.word_index[word]]\n",
    "                )\n",
    "            else:\n",
    "                continue\n",
    "        except:\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c1c3e06",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_storage = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f81370fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "descriptions = games[[\"BGGId\", \"Description\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60bdde95",
   "metadata": {},
   "outputs": [],
   "source": [
    "descriptions.apply(lambda x: vectorize(x), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe35f66f",
   "metadata": {},
   "outputs": [],
   "source": [
    "description_vectors = pd.DataFrame(vector_storage).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a16d6cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "description_vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb3383c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get only vectors that show up more than once!\n",
    "refined_vectors = description_vectors.loc[\n",
    "    :, (description_vectors.isnull().sum(axis=0) <= 21923)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0933469f",
   "metadata": {},
   "outputs": [],
   "source": [
    "refined_vectors.fillna(0, inplace=True)\n",
    "refined_vectors.reset_index(inplace=True)\n",
    "refined_vectors.drop(\"level_0\", axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b6f47e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "refined_vectors.to_pickle(\"{game_data_dir}/description_vectors.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fc6cc19",
   "metadata": {},
   "outputs": [],
   "source": [
    "del vector_storage\n",
    "del refined_vectors\n",
    "del embedding_matrix\n",
    "del tokenizer\n",
    "del descriptions\n",
    "del embedding_vector\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55b29d63",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "boardgamegeek-ZH0FNRKg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "292.865px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
